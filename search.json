[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data7 Exploratory Data Analysis In Python Book",
    "section": "",
    "text": "Overview\nHere you will find a collection of materials prepared by the staff of the Data Science Institute"
  },
  {
    "objectID": "index.html#diagnosing-like-a-data-doctor",
    "href": "index.html#diagnosing-like-a-data-doctor",
    "title": "Data7 Exploratory Data Analysis In Python Book",
    "section": "Diagnosing like a data doctor",
    "text": "Diagnosing like a data doctor\n\nChapter summary\nExploring a novel data set and produce an HTML interactive reports"
  },
  {
    "objectID": "index.html#exploring-like-a-data-adventurer",
    "href": "index.html#exploring-like-a-data-adventurer",
    "title": "Data7 Exploratory Data Analysis In Python Book",
    "section": "Exploring like a data adventurer",
    "text": "Exploring like a data adventurer\n\nChapter summary\nExploring the normality of numerical columns in a novel data set"
  },
  {
    "objectID": "index.html#transforming-like-a-data-transformer",
    "href": "index.html#transforming-like-a-data-transformer",
    "title": "Data7 Exploratory Data Analysis In Python Book",
    "section": "Transforming like a Data… Transformer",
    "text": "Transforming like a Data… Transformer\n\nChapter summary\nUsing data transformation to correct non-normality in numerical data"
  },
  {
    "objectID": "index.html#imputating-like-a-data-scientist",
    "href": "index.html#imputating-like-a-data-scientist",
    "title": "Data7 Exploratory Data Analysis In Python Book",
    "section": "Imputating like a Data Scientist",
    "text": "Imputating like a Data Scientist\n\nChapter summary\nExploring, visualizing, and imputing outliers and missing values (NAs) in a novel data set"
  },
  {
    "objectID": "index.html#correlating-like-a-data-master",
    "href": "index.html#correlating-like-a-data-master",
    "title": "Data7 Exploratory Data Analysis In Python Book",
    "section": "Correlating Like a Data Master",
    "text": "Correlating Like a Data Master\n\nChapter summary\nAssess relationships within a novel data set\n\nVisit our available Digital Learning Resources Library!\n\nCreated: 09/14/2022 (G. Chism); Last update: 11/17/2022\n CC BY-NC-SA"
  },
  {
    "objectID": "intro.html#what-is-exploratory-data-analysis",
    "href": "intro.html#what-is-exploratory-data-analysis",
    "title": "Introduction",
    "section": "What is Exploratory Data Analysis?",
    "text": "What is Exploratory Data Analysis?\nExploratory data analysis is a statistical, approach towards analyzing data sets to investigate and summarize their main characteristics, often through statistical graphics and other data visualization methods."
  },
  {
    "objectID": "intro.html#what-are-some-important-data-set-characteristics",
    "href": "intro.html#what-are-some-important-data-set-characteristics",
    "title": "Introduction",
    "section": "What are Some Important Data Set Characteristics?",
    "text": "What are Some Important Data Set Characteristics?\nThere are several characteristics that are arguably important, but we will only consider those covered in this workshop series. Let’s start with the fundamentals that will help guide us."
  },
  {
    "objectID": "intro.html#diagnostics",
    "href": "intro.html#diagnostics",
    "title": "Introduction",
    "section": "Diagnostics",
    "text": "Diagnostics\nWhen importing data sets, it is important to consider characteristics about the data columns, rows, and individual cells.\n\n\nVariables\nName of each variable\n\n\n   Pregnancies  Glucose  BloodPressure  ...  Age  Outcome  Age_group\n0            6      148             72  ...   50        1     Middle\n1            1       85             66  ...   31        0     Middle\n2            8      183             64  ...   32        1     Middle\n3            1       89             66  ...   21        0      Young\n4            0      137             40  ...   33        1     Middle\n\n[5 rows x 10 columns]\n\n\n\n\nTypes\nData type of each variable\n\n\nPregnancies                   int64\nGlucose                       int64\nBloodPressure                 int64\nSkinThickness                 int64\nInsulin                       int64\nBMI                         float64\nDiabetesPedigreeFunction    float64\nAge                           int64\nOutcome                       int64\nAge_group                    object\ndtype: object\n\n\n\nNumerical: Continuous\nMeasurable numbers that are fractional or decimal and cannot be counted (e.g., time, height, weight)\n\n\n\n\n\n\n\nNumerical: Discrete\nCountable whole numbers or integers (e.g., number of successes or failures)\n\n\n\n\n\n\n\n\nCategorical: Nominal\nLabeling variables without any order or quantitative value (e.g., hair color, nationality)\n\n\n\n\n\n\n\nCategorical: Ordinal\nWhere there is a hierarchical order along a scale (e.g., ranks, letter grades, age groups)\n\n\n\n\n\n\n\n\nMissing Values (NAs)\nCells, rows, or columns without data\n\nMissing percent: percentage of missing values * Unique count: number of unique values.\nUnique rate: rate of unique value - unique count / total number of observations.\n\n\n\n   Pregnancies  Glucose  BloodPressure  ...  Outcome  Age_group  Outcome1\n0          NaN    148.0           72.0  ...      1.0     Middle       Yes\n1          1.0     85.0           66.0  ...      0.0     Middle        No\n2          8.0    183.0           64.0  ...      1.0     Middle       Yes\n3          1.0     89.0           66.0  ...      0.0      Young        No\n4          0.0    137.0           40.0  ...      1.0     Middle       Yes\n\n[5 rows x 11 columns]"
  },
  {
    "objectID": "intro.html#summary-statistics",
    "href": "intro.html#summary-statistics",
    "title": "Introduction",
    "section": "Summary Statistics",
    "text": "Summary Statistics\nAbove we described some properties of data. However, you will need to know some descriptive characteristics of your data before you can move forward. Enter, summary statistics.\nSummary statistics allow you to summarize large amounts of information about your data as quickly as possible.\n\nCentral Tendency\nMeasuring a central property of your data. Some examples you’ve probably heard of are:\n\nMean: Average value\nMedian: Middle value\nMode: Most common value\n\n\n\n\n\n\nNotice that all values of central tendency can be pretty similar in this figure.\n\n\n\n\n\nHowever, in this figure, all measures are different. This will be important when we discuss statistical dispersion in chapter 3.\n\n\nStatistical Dispersion\nMeasure of data variability, scatter, or spread. Some examples you may have heard of:\n\nStandard deviation (SD): The amount of variation that occurs in a set of values.\nInterquartile range (IQR): The difference between the 75th and 25th percentiles\nOutliers: A value outside of \\(1.5 * IQR\\)\n\n\n\n\n\n\n\n\nDistribution Shape\nMeasures of describing the shape of a distribution, usually compared to a normal distribution (bell-curve)\n\nSkewness: The symmetry of the distribution\nKurtosis: The tailedness of the distribution\n\n\n\n\n\n\n\n\nStatistical Dependence (Correlation)\nMeasure of causality between two random variables (statistically). Notably, we approximate causality with correlations (see correlation \\(\\neq\\) causation)\n\nNumerical values, but you can compare numericals across categories (see the first plot above).\n\n\n\n\n\n\n\n\n\n\nTeam, The Pandas Development. 2022. Pandas-Dev/Pandas: Pandas. Zenodo. https://doi.org/10.5281/ZENODO.7093122."
  },
  {
    "objectID": "DiagnosingLikeDataDoctor.html#purpose-of-this-chapter",
    "href": "DiagnosingLikeDataDoctor.html#purpose-of-this-chapter",
    "title": "Diagnosing like a Data Doctor",
    "section": "Purpose of this chapter",
    "text": "Purpose of this chapter\nExploring a novel data set and produce an HTML interactive reports"
  },
  {
    "objectID": "DiagnosingLikeDataDoctor.html#take-aways",
    "href": "DiagnosingLikeDataDoctor.html#take-aways",
    "title": "Diagnosing like a Data Doctor",
    "section": "Take-aways",
    "text": "Take-aways\n\nLoad and explore a data set with publication quality tables\nDiagnose outliers and missing values in a data set\nPrepare an HTML summary report showcasing properties of a data set"
  },
  {
    "objectID": "DiagnosingLikeDataDoctor.html#required-setup",
    "href": "DiagnosingLikeDataDoctor.html#required-setup",
    "title": "Diagnosing like a Data Doctor",
    "section": "Required Setup",
    "text": "Required Setup\nWe first need to prepare our environment with the necessary libraries and set a global theme for publishable plots in seaborn.\n\n# Import all required libraries\n# Data analysis and manipulation\nimport pandas as pd\n# Working with arrays\nimport numpy as np\n# Statistical visualization\nimport seaborn as sns\n# Matlab plotting for Python\nimport matplotlib.pyplot as plt\n# Data analysis\nimport statistics as stat\n# Predictive data analysis: process data \nfrom sklearn import preprocessing as pproc\nimport scipy.stats as stats\n# Visualizing missing values\nimport missingno as msno\n# Interactive HTML EDA report\nfrom ydata_profiling import ProfileReport\n\n# Increase font size of all Seaborn plot elements\nsns.set(font_scale = 1.25)"
  },
  {
    "objectID": "DiagnosingLikeDataDoctor.html#load-and-examine-a-data-set",
    "href": "DiagnosingLikeDataDoctor.html#load-and-examine-a-data-set",
    "title": "Diagnosing like a Data Doctor",
    "section": "Load and Examine a Data Set",
    "text": "Load and Examine a Data Set\n\nLoad data and view\nExamine columns and data types\nDefine box plots\nDescribe meta data\n\nWe will be using open source data from UArizona researchers for Test, Trace, Treat (T3) efforts offers two clinical diagnostic tests (Antigen, RT-PCR) to determine whether an individual is currently infected with the COVID-19 virus. (Merchant et al. 2022)\n\n# Read csv \ndata = pd.read_csv(\"data/daily_summary.csv\")\n\n# Convert 'result_date' column to datetime\ndata['result_date'] = pd.to_datetime(data['result_date'])\n\n# What does the data look like\ndata.head()\n\n  result_date      affil_category  ... test_count          test_source\n0  2020-08-04            Employee  ...          5        Campus Health\n1  2020-08-04            Employee  ...          0        Campus Health\n2  2020-08-04            Employee  ...          1  Test All Test Smart\n3  2020-08-04            Employee  ...          0  Test All Test Smart\n4  2020-08-04  Off-Campus Student  ...          9        Campus Health\n\n[5 rows x 6 columns]"
  },
  {
    "objectID": "DiagnosingLikeDataDoctor.html#diagnose-your-data",
    "href": "DiagnosingLikeDataDoctor.html#diagnose-your-data",
    "title": "Diagnosing like a Data Doctor",
    "section": "Diagnose your Data",
    "text": "Diagnose your Data\n\n# What are the properties of the data\ndiagnose = data.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 9180 entries, 0 to 9179\nData columns (total 6 columns):\n #   Column          Non-Null Count  Dtype         \n---  ------          --------------  -----         \n 0   result_date     9180 non-null   datetime64[ns]\n 1   affil_category  9180 non-null   object        \n 2   test_type       9180 non-null   object        \n 3   test_result     9180 non-null   object        \n 4   test_count      9180 non-null   int64         \n 5   test_source     9180 non-null   object        \ndtypes: datetime64[ns](1), int64(1), object(4)\nmemory usage: 430.4+ KB\n\n\n\nColumn: name of each variable\nNon-Null Count: number of missing values\nDType: data type of each variable"
  },
  {
    "objectID": "DiagnosingLikeDataDoctor.html#summary-statistics-of-your-data",
    "href": "DiagnosingLikeDataDoctor.html#summary-statistics-of-your-data",
    "title": "Diagnosing like a Data Doctor",
    "section": "Summary Statistics of your Data",
    "text": "Summary Statistics of your Data\n\nNumerical Variables\n\n# Summary statistics of our numerical columns\ndata.describe()\n\n                         result_date   test_count\ncount                           9180  9180.000000\nmean   2021-06-23 02:28:32.941176576    46.771024\nmin              2020-08-04 00:00:00     0.000000\n25%              2021-01-11 00:00:00     0.000000\n50%              2021-06-09 00:00:00     2.000000\n75%              2021-12-01 00:00:00    16.000000\nmax              2022-06-30 00:00:00  1472.000000\nstd                              NaN   129.475844\n\n\n\ncount: number of observations\nmean: arithmetic mean (average value)\nstd: standard deviation\nmin: minimum value\n25%: 1/4 quartile, 25th percentile\n50%: median, 50th percentile\n75%: 3/4 quartile, 75th percentile\nmax: maximum value\n\n\n\n\nOutliers\nValues outside of \\(1.5 * IQR\\)\n\n\n\nImage Credit: CÉDRIC SCHERER\n\n\n\nThere are several numerical variables that have outliers above, let’s see what the data look like with and without them\n\nCreate a table with columns containing outliers\nPlot outliers in a box plot and histogram\n\n\n# Make a copy of the data \ndataCopy = data.copy()\n\n# Select only numerical columns\ndataRed = dataCopy.select_dtypes(include = np.number)\n\n# List of numerical columns\ndataRedColsList = dataRed.columns[...]\n\n# For all values in the numerical column list from above\nfor i_col in dataRedColsList:\n  # List of the values in i_col\n  dataRed_i = dataRed.loc[:,i_col]\n  \n  # Define the 25th and 75th percentiles\n  q25, q75 = round((dataRed_i.quantile(q=0.25)), 3), round((dataRed_i.quantile(q=0.75)), 3)\n  \n  # Define the interquartile range from the 25th and 75th percentiles defined above\n  IQR = round((q75 - q25), 3)\n  \n  # Calculate the outlier cutoff \n  cut_off = IQR * 1.5\n  \n  # Define lower and upper cut-offs\n  lower, upper = round((q25 - cut_off), 3), round((q75 + cut_off), 3)\n  \n  # Print the values\n  print(' ')\n  \n  # For each value of i_col, print the 25th and 75th percentiles and IQR\n  print(i_col, 'q25=', q25, 'q75=', q75, 'IQR=', IQR)\n  \n  # Print the lower and upper cut-offs\n  print('lower, upper:', lower, upper)\n\n  # Count the number of outliers outside the (lower, upper) limits, print that value\n  print('Number of Outliers: ', dataRed_i[(dataRed_i &lt; lower) | (dataRed_i &gt; upper)].count())\n\n \ntest_count q25= 0.0 q75= 16.0 IQR= 16.0\nlower, upper: -24.0 40.0\nNumber of Outliers:  1721\n\n\n\nq25: 1/4 quartile, 25th percentile\nq75: 3/4 quartile, 75th percentile\nIQR: interquartile range (q75-q25)\nlower: lower limit of \\(1.5*IQR\\) used to calculate outliers\nupper: upper limit of \\(1.5*IQR\\) used to calculate outliers\n\n\n# Change theme to \"white\"\nsns.set_style(\"white\")\n\n# Select only numerical columns\ndataRedColsList = data.select_dtypes(include = np.number)\n\n# Melt data from wide-to-long format\ndata_melted = pd.melt(dataRedColsList)\n\n# Boxplot of all numerical variables\nsns.boxplot(data = data_melted, x = 'variable', y = 'value', hue = 'variable' , width = 0.20)\n\n\n\n\nNote the extreme number of outliers represented in the boxplot\n\n# Find Q1, Q3, and interquartile range (IQR) for each column\nQ1 = dataRedColsList.quantile(q = .25)\nQ3 = dataRedColsList.quantile(q = .75)\nIQR = dataRedColsList.apply(stats.iqr)\n\n# Only keep rows in dataframe that have values within 1.5*IQR of Q1 and Q3\ndata_clean = dataRedColsList[~((dataRedColsList &lt; (Q1 - 1.5 * IQR)) | (dataRedColsList &gt; (Q3 + 1.5 * IQR))).any(axis = 1)]\n\n# Melt data from wide-to-long format\ndata_clean_melted =  pd.melt(data_clean)\n\n# Boxplot of all numerical variables, with outliers removed via the IQR cutoff criteria\nsns.boxplot(data = data_clean_melted, x = 'variable', y = 'value', hue = 'variable' , width = 0.20)\n\n\n\n\nBut the distribution changes dramatically when we remove outliers with the IQR method (see above). Interestingly, there are a new set of “outliers” which results from a new IQR being calculated.\n\n\nMissing Values (NAs)\n\nTable showing the extent of NAs in columns containing them\n\n\n# Copy of the data\ndataNA = data\n\n# Randomly add NAs to all columns replacing 10% of values\nfor col in dataNA.columns:\n    dataNA.loc[dataNA.sample(frac = 0.1).index, col] = np.nan\n\n# Sum of NAs in each column (should be the same, 10% of all)   \ndataNA.isnull().sum()\n\nresult_date       918\naffil_category    918\ntest_type         918\ntest_result       918\ntest_count        918\ntest_source       918\ndtype: int64\n\n\nBar plot showing all NA values in each column. Since we randomly produced a set amount above the numbers will all be the same.\n\n# Bar plot showing the number of NAs in each column\nmsno.bar(dataNA, figsize = (8, 8), fontsize = 10)\nplt.tight_layout()\n\n\n\n\n\n\n\nCategorical Variables\n\n# Select only categorical columns (objects) and describe\ndata.describe(exclude = [np.number]) \n\n                          result_date  ...          test_source\ncount                            8262  ...                 8262\nunique                            NaN  ...                    2\ntop                               NaN  ...  Test All Test Smart\nfreq                              NaN  ...                 4559\nmean    2021-06-23 16:53:20.000000256  ...                  NaN\nmin               2020-08-04 00:00:00  ...                  NaN\n25%               2021-01-12 00:00:00  ...                  NaN\n50%               2021-06-10 00:00:00  ...                  NaN\n75%               2021-12-01 00:00:00  ...                  NaN\nmax               2022-06-30 00:00:00  ...                  NaN\n\n[10 rows x 5 columns]\n\n\n\ncount: number of values in the column\nunique: the number of unique categories\ntop: category with the most observations\nfreq: number of observations in the top category"
  },
  {
    "objectID": "DiagnosingLikeDataDoctor.html#produce-an-html-summary-of-a-data-set",
    "href": "DiagnosingLikeDataDoctor.html#produce-an-html-summary-of-a-data-set",
    "title": "Diagnosing like a Data Doctor",
    "section": "Produce an HTML Summary of a Data Set",
    "text": "Produce an HTML Summary of a Data Set\n\n# Producing a pandas-profiling report \nprofile = ProfileReport(data, title = \"Pandas Profiling Report\")\n\n# HTML output\nprofile.to_widgets()\n\n\n\n\n\nMerchant, Nirav C, Jim Davis, George H Franks, Chun Ly, Fernando Rios, Todd Wickizer, Gary D Windham, and Michelle Yung. 2022. “University of Arizona Test-Trace-Treat COVID-19 Testing Results.” University of Arizona Research Data Repository. https://doi.org/10.25422/AZU.DATA.14869740.V3."
  },
  {
    "objectID": "ExploringLikeDataAdventurer.html#purpose-of-this-chapter",
    "href": "ExploringLikeDataAdventurer.html#purpose-of-this-chapter",
    "title": "Exploring like a Data Adventurer",
    "section": "Purpose of this chapter",
    "text": "Purpose of this chapter\nExploring the normality of numerical columns in a novel data set"
  },
  {
    "objectID": "ExploringLikeDataAdventurer.html#take-aways",
    "href": "ExploringLikeDataAdventurer.html#take-aways",
    "title": "Exploring like a Data Adventurer",
    "section": "Take-aways",
    "text": "Take-aways\n\nUsing summary statistics to better understand individual columns in a data set.\nAssessing data normality in numerical columns.\nAssessing data normality within groups."
  },
  {
    "objectID": "ExploringLikeDataAdventurer.html#required-setup",
    "href": "ExploringLikeDataAdventurer.html#required-setup",
    "title": "Exploring like a Data Adventurer",
    "section": "Required Setup",
    "text": "Required Setup\nWe first need to prepare our environment with the necessary libraries and set a global theme for publishable plots in seaborn.\n\n# Import all required libraries\n# Data analysis and manipulation\nimport pandas as pd\n# Working with arrays\nimport numpy as np\n# Statistical visualization\nimport seaborn as sns\n# Matlab plotting for Python\nimport matplotlib.pyplot as plt\n# Data analysis\nimport statistics as stat\n# Predictive data analysis: process data \nfrom sklearn import preprocessing as pproc\nimport scipy.stats as stats\n# Visualizing missing values\nimport missingno as msno\n# Statistical modeling\nimport statsmodels.api as sm\n\n# increase font size of all seaborn plot elements\nsns.set(font_scale = 1.25)"
  },
  {
    "objectID": "ExploringLikeDataAdventurer.html#load-and-examine-a-data-set",
    "href": "ExploringLikeDataAdventurer.html#load-and-examine-a-data-set",
    "title": "Exploring like a Data Adventurer",
    "section": "Load and Examine a Data Set",
    "text": "Load and Examine a Data Set\nWe will be using open source data from UArizona researchers that investigates the effects of climate change on canopy trees. (Meredith, Ladd, and Werner 2021)\n\n# Read csv \ndata = pd.read_csv(\"data/Data_Fig2_Repo.csv\")\n\n# Convert 'Date' column to datetime\ndata['Date'] = pd.to_datetime(data['Date'])\n\n# What does the data look like\ndata.head()\n\n        Date                Group    Sap_Flow  TWaterFlux      pLWP      mLWP\n0 2019-10-04  Drought-sens-canopy  184.040975   82.243292 -0.263378 -0.679769\n1 2019-10-04   Drought-sens-under    2.475989    1.258050 -0.299669 -0.761326\n2 2019-10-04   Drought-tol-canopy   10.598949    4.405479 -0.437556 -0.722557\n3 2019-10-04    Drought-tol-under    4.399854    2.055276 -0.205224 -0.702858\n4 2019-10-05  Drought-sens-canopy  182.905444   95.865255 -0.276928 -0.708261"
  },
  {
    "objectID": "ExploringLikeDataAdventurer.html#diagnose-your-data",
    "href": "ExploringLikeDataAdventurer.html#diagnose-your-data",
    "title": "Exploring like a Data Adventurer",
    "section": "Diagnose your Data",
    "text": "Diagnose your Data\n\n# What are the properties of the data\ndiagnose = data.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 588 entries, 0 to 587\nData columns (total 6 columns):\n #   Column      Non-Null Count  Dtype         \n---  ------      --------------  -----         \n 0   Date        588 non-null    datetime64[ns]\n 1   Group       588 non-null    object        \n 2   Sap_Flow    480 non-null    float64       \n 3   TWaterFlux  588 non-null    float64       \n 4   pLWP        276 non-null    float64       \n 5   mLWP        308 non-null    float64       \ndtypes: datetime64[ns](1), float64(4), object(1)\nmemory usage: 27.7+ KB\n\n\n\nColumn: name of each variable\nNon-Null Count: number of missing values\nDType: data type of each variable\n\n\n\nBox Plot\n\n\n\nImage Credit: CÉDRIC SCHERER\n\n\n\n\n\nSkewness\n\n\n\n(c) Andrey Akinshin\n\n\n\n\nNOTE\n\n“Skewness” has multiple definitions. Several underlying equations mey be at play\nSkewness is “designed” for distributions with one peak (unimodal); it’s meaningless for distributions with multiple peaks (multimodal).\nMost default skewness definitions are not robust: a single outlier could completely distort the skewness value.\nWe can’t make conclusions about the locations of the mean and the median based on the skewness sign.\n\n\n\n\n\nKurtosis\n\n\n\n(c) Andrey Akinshin\n\n\n\nNOTE\n\nThere are multiple definitions of kurtosis - i.e., “kurtosis” and “excess kurtosis,” but there are other definitions of this measure.\nKurtosis may work fine for distributions with one peak (unimodal); it’s meaningless for distributions with multiple peaks (multimodal).\nThe classic definition of kurtosis is not robust: it could be easily spoiled by extreme outliers."
  },
  {
    "objectID": "ExploringLikeDataAdventurer.html#describe-your-continuous-data",
    "href": "ExploringLikeDataAdventurer.html#describe-your-continuous-data",
    "title": "Exploring like a Data Adventurer",
    "section": "Describe your Continuous Data",
    "text": "Describe your Continuous Data\n\n# Summary statistics of our numerical columns\ndata.describe()\n\n                      Date    Sap_Flow  TWaterFlux        pLWP        mLWP\ncount                  588  480.000000  588.000000  276.000000  308.000000\nmean   2019-12-16 00:00:00   25.091576   11.925722   -0.609055   -1.029703\nmin    2019-10-04 00:00:00    0.172630    0.101381   -1.433333   -1.812151\n25%    2019-11-09 00:00:00    2.454843    1.293764   -0.714008   -1.227326\n50%    2019-12-16 00:00:00    5.815661    2.995357   -0.586201   -0.946656\n75%    2020-01-22 00:00:00   16.371703    7.577102   -0.450000   -0.808571\nmax    2020-02-27 00:00:00  184.040975   96.012719   -0.205224   -0.545165\nstd                    NaN   40.520386   19.048809    0.227151    0.295834\n\n\n\ncount: number of observations\nmean: arithmetic mean (average value)\nstd: standard deviation\nmin: minimum value\n25%: 1/4 quartile, 25th percentile\n50%: median, 50th percentile\n75%: 3/4 quartile, 75th percentile\nmax: maximum value\n\n\n# Make a copy of the data \ndataCopy = data.copy()\n\n# Select only numerical columns\ndataRed = dataCopy.select_dtypes(include = np.number)\n\n# List of numerical columns\ndataRedColsList = dataRed.columns[...]\n\n# For all values in the numerical column list from above\nfor i_col in dataRedColsList:\n  # List of the values in i_col\n  dataRed_i = dataRed.loc[:,i_col]\n  \n  # Define the 25th and 75th percentiles\n  q25, q75 = round((dataRed_i.quantile(q = 0.25)), 3), round((dataRed_i.quantile(q = 0.75)), 3)\n  \n  # Define the interquartile range from the 25th and 75th percentiles defined above\n  IQR = round((q75 - q25), 3)\n  \n  # Calculate the outlier cutoff \n  cut_off = IQR * 1.5\n  \n  # Define lower and upper cut-offs\n  lower, upper = round((q25 - cut_off), 3), round((q75 + cut_off), 3)\n  \n  # Skewness\n  skewness = round((dataRed_i.skew()), 3) \n  \n  # Kurtosis\n  kurtosis = round((dataRed_i.kurt()), 3)\n  \n  # Number of outliers\n  outliers = dataRed_i[(dataRed_i &lt; lower) | (dataRed_i &gt; upper)].count()\n  \n  # Print a blank row\n  print('')\n  \n  # Print the column name\n  print(i_col)\n  \n  # For each value of i_col, print the 25th and 75th percentiles and IQR\n  print('q25 =', q25, 'q75 =', q75, 'IQR =', IQR)\n  \n  # Print the lower and upper cut-offs\n  print('lower, upper:', lower, upper)\n  \n  # Print skewness and kurtosis\n  print('skewness =', skewness, 'kurtosis =', kurtosis)\n  \n  # Count the number of outliers outside the (lower, upper) limits, print that value\n  print('Number of Outliers: ', outliers)\n\n\nSap_Flow\nq25 = 2.455 q75 = 16.372 IQR = 13.917\nlower, upper: -18.42 37.248\nskewness = 2.153 kurtosis = 4.197\nNumber of Outliers:  116\n\nTWaterFlux\nq25 = 1.294 q75 = 7.577 IQR = 6.283\nlower, upper: -8.13 17.002\nskewness = 2.081 kurtosis = 3.884\nNumber of Outliers:  139\n\npLWP\nq25 = -0.714 q75 = -0.45 IQR = 0.264\nlower, upper: -1.11 -0.054\nskewness = -1.105 kurtosis = 1.767\nNumber of Outliers:  12\n\nmLWP\nq25 = -1.227 q75 = -0.809 IQR = 0.418\nlower, upper: -1.854 -0.182\nskewness = -0.797 kurtosis = -0.181\nNumber of Outliers:  0\n\n\n\nq25: 1/4 quartile, 25th percentile\nq75: 3/4 quartile, 75th percentile\nIQR: interquartile range (q75-q25)\nlower: lower limit of \\(1.5*IQR\\) used to calculate outliers\nupper: upper limit of \\(1.5*IQR\\) used to calculate outliers\nskewness: skewness\nkurtosis: kurtosis"
  },
  {
    "objectID": "ExploringLikeDataAdventurer.html#describe-categorical-variables",
    "href": "ExploringLikeDataAdventurer.html#describe-categorical-variables",
    "title": "Exploring like a Data Adventurer",
    "section": "Describe Categorical Variables",
    "text": "Describe Categorical Variables\n\n# Select only categorical columns (objects) \ndata.describe(exclude=[np.number]) \n\n                       Date                Group\ncount                   588                  588\nunique                  NaN                    4\ntop                     NaN  Drought-sens-canopy\nfreq                    NaN                  147\nmean    2019-12-16 00:00:00                  NaN\nmin     2019-10-04 00:00:00                  NaN\n25%     2019-11-09 00:00:00                  NaN\n50%     2019-12-16 00:00:00                  NaN\n75%     2020-01-22 00:00:00                  NaN\nmax     2020-02-27 00:00:00                  NaN\n\n\n\n\nGroup Descriptive Statistics\n\n# Grouped describe by one column, stacked \nGroups = data.groupby('Group').describe().unstack(1)\n\n# Print all rows\nprint(Groups.to_string())\n\n                   Group              \nDate        count  Drought-sens-canopy                    147\n                   Drought-sens-under                     147\n                   Drought-tol-canopy                     147\n                   Drought-tol-under                      147\n            mean   Drought-sens-canopy    2019-12-16 00:00:00\n                   Drought-sens-under     2019-12-16 00:00:00\n                   Drought-tol-canopy     2019-12-16 00:00:00\n                   Drought-tol-under      2019-12-16 00:00:00\n            min    Drought-sens-canopy    2019-10-04 00:00:00\n                   Drought-sens-under     2019-10-04 00:00:00\n                   Drought-tol-canopy     2019-10-04 00:00:00\n                   Drought-tol-under      2019-10-04 00:00:00\n            25%    Drought-sens-canopy    2019-11-09 12:00:00\n                   Drought-sens-under     2019-11-09 12:00:00\n                   Drought-tol-canopy     2019-11-09 12:00:00\n                   Drought-tol-under      2019-11-09 12:00:00\n            50%    Drought-sens-canopy    2019-12-16 00:00:00\n                   Drought-sens-under     2019-12-16 00:00:00\n                   Drought-tol-canopy     2019-12-16 00:00:00\n                   Drought-tol-under      2019-12-16 00:00:00\n            75%    Drought-sens-canopy    2020-01-21 12:00:00\n                   Drought-sens-under     2020-01-21 12:00:00\n                   Drought-tol-canopy     2020-01-21 12:00:00\n                   Drought-tol-under      2020-01-21 12:00:00\n            max    Drought-sens-canopy    2020-02-27 00:00:00\n                   Drought-sens-under     2020-02-27 00:00:00\n                   Drought-tol-canopy     2020-02-27 00:00:00\n                   Drought-tol-under      2020-02-27 00:00:00\n            std    Drought-sens-canopy                    NaN\n                   Drought-sens-under                     NaN\n                   Drought-tol-canopy                     NaN\n                   Drought-tol-under                      NaN\nSap_Flow    count  Drought-sens-canopy                  120.0\n                   Drought-sens-under                   120.0\n                   Drought-tol-canopy                   120.0\n                   Drought-tol-under                    120.0\n            mean   Drought-sens-canopy              85.269653\n                   Drought-sens-under                1.448825\n                   Drought-tol-canopy                9.074309\n                   Drought-tol-under                 4.573516\n            min    Drought-sens-canopy               33.37045\n                   Drought-sens-under                 0.17263\n                   Drought-tol-canopy                 5.90461\n                   Drought-tol-under                  2.17178\n            25%    Drought-sens-canopy              53.975162\n                   Drought-sens-under                0.534165\n                   Drought-tol-canopy                 8.11941\n                   Drought-tol-under                 4.053346\n            50%    Drought-sens-canopy              76.717782\n                   Drought-sens-under                1.665492\n                   Drought-tol-canopy                9.286552\n                   Drought-tol-under                 4.944842\n            75%    Drought-sens-canopy              94.068107\n                   Drought-sens-under                2.194299\n                   Drought-tol-canopy               10.404117\n                   Drought-tol-under                 5.139685\n            max    Drought-sens-canopy             184.040975\n                   Drought-sens-under                2.475989\n                   Drought-tol-canopy               10.705455\n                   Drought-tol-under                 5.726712\n            std    Drought-sens-canopy              41.313962\n                   Drought-sens-under                0.803858\n                   Drought-tol-canopy                 1.39567\n                   Drought-tol-under                  0.90243\nTWaterFlux  count  Drought-sens-canopy                  147.0\n                   Drought-sens-under                   147.0\n                   Drought-tol-canopy                   147.0\n                   Drought-tol-under                    147.0\n            mean   Drought-sens-canopy              40.404061\n                   Drought-sens-under                 0.75177\n                   Drought-tol-canopy                4.357234\n                   Drought-tol-under                 2.189824\n            min    Drought-sens-canopy              12.377738\n                   Drought-sens-under                0.101381\n                   Drought-tol-canopy                2.036843\n                   Drought-tol-under                 0.953906\n            25%    Drought-sens-canopy              25.220908\n                   Drought-sens-under                 0.27419\n                   Drought-tol-canopy                3.601341\n                   Drought-tol-under                 1.735003\n            50%    Drought-sens-canopy              38.630891\n                   Drought-sens-under                0.824875\n                   Drought-tol-canopy                4.460778\n                   Drought-tol-under                 2.198131\n            75%    Drought-sens-canopy              50.096197\n                   Drought-sens-under                 1.11289\n                   Drought-tol-canopy                5.112844\n                   Drought-tol-under                 2.686605\n            max    Drought-sens-canopy              96.012719\n                   Drought-sens-under                1.801823\n                   Drought-tol-canopy                 5.97689\n                   Drought-tol-under                 3.654336\n            std    Drought-sens-canopy              19.027997\n                   Drought-sens-under                0.429073\n                   Drought-tol-canopy                0.940353\n                   Drought-tol-under                 0.597511\npLWP        count  Drought-sens-canopy                   69.0\n                   Drought-sens-under                    69.0\n                   Drought-tol-canopy                    69.0\n                   Drought-tol-under                     69.0\n            mean   Drought-sens-canopy              -0.669932\n                   Drought-sens-under               -0.696138\n                   Drought-tol-canopy               -0.629909\n                   Drought-tol-under                -0.440243\n            min    Drought-sens-canopy              -1.299263\n                   Drought-sens-under               -1.433333\n                   Drought-tol-canopy               -0.863656\n                   Drought-tol-under                -0.746667\n            25%    Drought-sens-canopy              -0.790573\n                   Drought-sens-under                    -0.8\n                   Drought-tol-canopy               -0.706479\n                   Drought-tol-under                -0.520487\n            50%    Drought-sens-canopy              -0.705942\n                   Drought-sens-under               -0.592118\n                   Drought-tol-canopy               -0.602841\n                   Drought-tol-under                -0.406439\n            75%    Drought-sens-canopy               -0.47329\n                   Drought-sens-under               -0.521217\n                   Drought-tol-canopy               -0.571356\n                   Drought-tol-under                -0.360789\n            max    Drought-sens-canopy              -0.263378\n                   Drought-sens-under               -0.299669\n                   Drought-tol-canopy               -0.437556\n                   Drought-tol-under                -0.205224\n            std    Drought-sens-canopy                0.24639\n                   Drought-sens-under                0.283935\n                   Drought-tol-canopy                0.095571\n                   Drought-tol-under                 0.131879\nmLWP        count  Drought-sens-canopy                   77.0\n                   Drought-sens-under                    77.0\n                   Drought-tol-canopy                    77.0\n                   Drought-tol-under                     77.0\n            mean   Drought-sens-canopy              -1.319148\n                   Drought-sens-under               -1.097537\n                   Drought-tol-canopy               -0.892554\n                   Drought-tol-under                -0.809572\n            min    Drought-sens-canopy              -1.812151\n                   Drought-sens-under               -1.808333\n                   Drought-tol-canopy               -1.073619\n                   Drought-tol-under                -1.168716\n            25%    Drought-sens-canopy              -1.525563\n                   Drought-sens-under               -1.335521\n                   Drought-tol-canopy               -0.945841\n                   Drought-tol-under                -0.907041\n            50%    Drought-sens-canopy              -1.354771\n                   Drought-sens-under               -1.054159\n                   Drought-tol-canopy               -0.890061\n                   Drought-tol-under                -0.735647\n            75%    Drought-sens-canopy              -1.111942\n                   Drought-sens-under               -0.907564\n                   Drought-tol-canopy               -0.828777\n                   Drought-tol-under                -0.699087\n            max    Drought-sens-canopy              -0.679769\n                   Drought-sens-under               -0.546152\n                   Drought-tol-canopy               -0.707789\n                   Drought-tol-under                -0.545165\n            std    Drought-sens-canopy               0.298107\n                   Drought-sens-under                0.263522\n                   Drought-tol-canopy                0.091729\n                   Drought-tol-under                 0.170603"
  },
  {
    "objectID": "ExploringLikeDataAdventurer.html#sec-testing-normality",
    "href": "ExploringLikeDataAdventurer.html#sec-testing-normality",
    "title": "Exploring like a Data Adventurer",
    "section": "Testing Normality",
    "text": "Testing Normality\n\nShapiro-Wilk test & Q-Q plots\nTesting overall normality of two columns\nTesting normality of groups\n\n\n\nNormality of Columns\n\n\nShapiro-Wilk Test\nShapiro-Wilk test looks at whether a target distribution is sample form a normal distribution\n\n# Make a copy of the data \ndataCopy = data.copy()\n\n# Remove NAs\ndataCopyFin = dataCopy.dropna()\n\n# Specify desired column\ni_col = dataCopyFin.Sap_Flow\n\n# Normality test\nstat, p = stats.shapiro(i_col)\n\nprint('\\nShapiro-Wilk Test for Normality\\n\\nSap_Flow\\nStatistic = %.3f, p = %.3f' % (stat, p))\n\n\nShapiro-Wilk Test for Normality\n\nSap_Flow\nStatistic = 0.603, p = 0.000\n\n# Interpret\nalpha = 0.05\n  \nif p &gt; alpha:\n  print('Sample looks Gaussian (fail to reject H0)')\nelse:\n  print('Sample does not look Gaussian (reject H0)')\n\nSample does not look Gaussian (reject H0)\n\n\nYou can also run the Shapiro-Wilk test on all numerical columns with a for-loop\n\n# Make a copy of the data \ndataCopy = data.copy()\n\n# Remove NAs\ndataCopyFin = dataCopy.dropna()\n\n# Select only numerical columns\ndataRed = dataCopyFin.select_dtypes(include = np.number)\n\n# List of numerical columns\ndataRedColsList = dataRed.columns[...]\n\n# For all values in the numerical column list from above\nfor i_col in dataRedColsList:\n  # List of the values in i_col\n  dataRed_i = dataRed.loc[:,i_col]\n  \n  # Normality test\n  stat, p = stats.shapiro(dataRed_i)\n  \n  # Print a blank, the column name, the statistic and p-value\n  print('')\n  print(i_col)\n  print('Statistic = %.3f, p = %.3f' % (stat, p))\n  \n  # Interpret\n  alpha = 0.05\n  \n  # Print the interpretation\n  if p &gt; alpha:\n    print('Sample looks Gaussian (fail to reject H0)')\n  else:\n      print('Sample does not look Gaussian (reject H0)')\n\n\nSap_Flow\nStatistic = 0.603, p = 0.000\nSample does not look Gaussian (reject H0)\n\nTWaterFlux\nStatistic = 0.600, p = 0.000\nSample does not look Gaussian (reject H0)\n\npLWP\nStatistic = 0.929, p = 0.000\nSample does not look Gaussian (reject H0)\n\nmLWP\nStatistic = 0.940, p = 0.000\nSample does not look Gaussian (reject H0)\n\n\n\n\n\nQ-Q Plots\nPlots of the quartiles of a target data set and plot it against predicted quartiles from a normal distribution.\n\n# Change theme to \"white\"\nsns.set_style(\"white\")\n\n# Make a copy of the data \ndataCopy = data.copy()\n\n# Remove NAs\ndataCopyFin = dataCopy.dropna()\n\n# Specify desired column\ni_col = dataCopyFin.Sap_Flow\n\n# Subplots\nfig, (ax1, ax2) = plt.subplots(ncols = 2, nrows = 1)\n\n# Density plot\nsns.kdeplot(i_col, linewidth = 5, ax = ax1)\nax1.set_title('Sap_Flow Density plot')\n\n# Q-Q plot\nsm.qqplot(i_col, line='s', ax = ax2)\nax2.set_title('Sap_Flow Q-Q plot')\nplt.tight_layout()\nplt.show()\n\n\n\n\nYou can also produce these plots for all numerical columns with a for-loop (output not shown).\n\n# Change theme to \"white\"\nsns.set_style(\"white\")\n\n# Make a copy of the data \ndataCopy = data.copy()\n\n# Remove NAs\ndataCopyFin = dataCopy.dropna()\n\n# Select only numerical columns\ndataRed = dataCopyFin.select_dtypes(include = np.number)\n\n# Combine multiple plots, the number of columns and rows is derived from the number of numerical columns from above. \n\n# Overall figure that subplots fill\nfig, axes = plt.subplots(ncols = 2, nrows = 4, sharex = True, figsize = (4, 4))\n\n# Fill the subplots\nfor k, ax in zip(dataRed.columns, np.ravel(axes)):\n    # Subplots\n    fig, (ax1, ax2) = plt.subplots(ncols = 2, nrows = 1)\n    \n    # Density plot\n    sns.kdeplot(dataRed[k], linewidth = 5, ax = ax1)\n    ax1.set_title(f'{k} Density Plot')\n    \n    # Q-Q plot\n    sm.qqplot(dataRed[k], line='s', ax = ax2)\n    ax2.set_title(f'{k} QQ Plot')\n    plt.tight_layout()\n    plt.show()\n\n\n\n\n\nNormality within Groups\nLooking within Age_group at the subgroup normality.\n\nShapiro-Wilk Test\n\n# Make a copy of the data \ndataCopy = data.copy()\n\n# Remove NAs\ndataCopyFin = dataCopy.dropna()\n\n# Pivot the data from long-to-wide with pivot, using Date as the index, so that a column is created for each Group and numerical column subset\ndataPivot = dataCopyFin.pivot(index = 'Date', columns = 'Group', values = ['Sap_Flow', 'TWaterFlux', 'pLWP', 'mLWP'])\n\n# Select only numerical columns\ndataRed = dataPivot.select_dtypes(include = np.number)\n\n# List of numerical columns\ndataRedColsList = dataRed.columns[...]\n\n# For all values in the numerical column list from above\nfor i_col in dataRedColsList:\n  # List of the values in i_col\n  dataRed_i = dataRed.loc[:,i_col]\n  \n  # normality test\n  stat, p = stats.shapiro(dataRed_i)\n  \n  print('')\n  print(i_col)\n  print('Statistics = %.3f, p = %.3f' % (stat, p))\n  \n  # interpret\n  alpha = 0.05\n  \n  if p &gt; alpha:\n    print('Sample looks Gaussian (fail to reject H0)')\n  else:\n      print('Sample does not look Gaussian (reject H0)')\n\n\n('Sap_Flow', 'Drought-sens-canopy')\nStatistics = 0.869, p = 0.000\nSample does not look Gaussian (reject H0)\n\n('Sap_Flow', 'Drought-sens-under')\nStatistics = 0.889, p = 0.000\nSample does not look Gaussian (reject H0)\n\n('Sap_Flow', 'Drought-tol-canopy')\nStatistics = 0.950, p = 0.008\nSample does not look Gaussian (reject H0)\n\n('Sap_Flow', 'Drought-tol-under')\nStatistics = 0.908, p = 0.000\nSample does not look Gaussian (reject H0)\n\n('TWaterFlux', 'Drought-sens-canopy')\nStatistics = 0.885, p = 0.000\nSample does not look Gaussian (reject H0)\n\n('TWaterFlux', 'Drought-sens-under')\nStatistics = 0.856, p = 0.000\nSample does not look Gaussian (reject H0)\n\n('TWaterFlux', 'Drought-tol-canopy')\nStatistics = 0.973, p = 0.147\nSample looks Gaussian (fail to reject H0)\n\n('TWaterFlux', 'Drought-tol-under')\nStatistics = 0.977, p = 0.233\nSample looks Gaussian (fail to reject H0)\n\n('pLWP', 'Drought-sens-canopy')\nStatistics = 0.969, p = 0.086\nSample looks Gaussian (fail to reject H0)\n\n('pLWP', 'Drought-sens-under')\nStatistics = 0.867, p = 0.000\nSample does not look Gaussian (reject H0)\n\n('pLWP', 'Drought-tol-canopy')\nStatistics = 0.952, p = 0.010\nSample does not look Gaussian (reject H0)\n\n('pLWP', 'Drought-tol-under')\nStatistics = 0.964, p = 0.044\nSample does not look Gaussian (reject H0)\n\n('mLWP', 'Drought-sens-canopy')\nStatistics = 0.962, p = 0.034\nSample does not look Gaussian (reject H0)\n\n('mLWP', 'Drought-sens-under')\nStatistics = 0.956, p = 0.016\nSample does not look Gaussian (reject H0)\n\n('mLWP', 'Drought-tol-canopy')\nStatistics = 0.962, p = 0.034\nSample does not look Gaussian (reject H0)\n\n('mLWP', 'Drought-tol-under')\nStatistics = 0.852, p = 0.000\nSample does not look Gaussian (reject H0)\n\n\n\n\n\nQ-Q Plots\n\n# Make a copy of the data \ndataCopy = data.copy()\n\n# Remove NAs\ndataCopyFin = dataCopy.dropna()\n\n# Pivot the data from long-to-wide with pivot, using Date as the index, so that a column is created for each Group and numerical column subset\ndataPivot = dataCopyFin.pivot(index = 'Date', columns = 'Group', values = ['Sap_Flow', 'TWaterFlux', 'pLWP', 'mLWP'])\n\n# Select only numerical columns\ndataRed = dataPivot.select_dtypes(include = np.number)\n\n# Combine multiple plots, the number of columns and rows is derived from the number of numerical columns from above. \nfig, axes = plt.subplots(ncols = 2, nrows = 8, sharex = True, figsize = (2 * 4, 8 * 4))\n\n# Generate figures for all numerical grouped data subsets\nfor k, ax in zip(dataRed.columns, np.ravel(axes)):\n    sm.qqplot(dataRed[k], line = 's', ax = ax)\n    ax.set_title(f'{k}\\n QQ Plot')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nMeredith, Laura, S. Nemiah Ladd, and Christiane Werner. 2021. “Data for \"Ecosystem Fluxes During Drought and Recovery in an Experimental Forest\".” University of Arizona Research Data Repository. https://doi.org/10.25422/AZU.DATA.14632593.V1."
  },
  {
    "objectID": "TransformingLikeDataTrans.html#purpose-of-this-chapter",
    "href": "TransformingLikeDataTrans.html#purpose-of-this-chapter",
    "title": "Transforming like a Data… Transformer",
    "section": "Purpose of this chapter",
    "text": "Purpose of this chapter\nUsing data transformation to correct non-normality in numerical data"
  },
  {
    "objectID": "TransformingLikeDataTrans.html#take-aways",
    "href": "TransformingLikeDataTrans.html#take-aways",
    "title": "Transforming like a Data… Transformer",
    "section": "Take-aways",
    "text": "Take-aways\n\nLoad and explore a data set with publication quality tables\nQuickly diagnose non-normality in data\nData transformation"
  },
  {
    "objectID": "TransformingLikeDataTrans.html#required-setup",
    "href": "TransformingLikeDataTrans.html#required-setup",
    "title": "Transforming like a Data… Transformer",
    "section": "Required Setup",
    "text": "Required Setup\nWe first need to prepare our environment with the necessary libraries and set a global theme for publishable plots in seaborn.\n\n# Import all required libraries\n# Data analysis and manipulation\nimport pandas as pd\n# Working with arrays\nimport numpy as np\n# Statistical visualization\nimport seaborn as sns\n# Matlab plotting for Python\nimport matplotlib.pyplot as plt\n# Data analysis\nimport statistics as stat\n# Predictive data analysis: process data \nfrom sklearn import preprocessing as pproc\nimport scipy.stats as stats\n# Visualizing missing values\nimport missingno as msno\n# Statistical modeling\nimport statsmodels.api as sm\n\n# Increase font size of all seaborn plot elements\nsns.set(font_scale = 1.5, rc = {'figure.figsize':(8, 8)})\n\n# Change theme to \"white\"\nsns.set_style(\"white\")"
  },
  {
    "objectID": "TransformingLikeDataTrans.html#load-and-examine-a-data-set",
    "href": "TransformingLikeDataTrans.html#load-and-examine-a-data-set",
    "title": "Transforming like a Data… Transformer",
    "section": "Load and Examine a Data Set",
    "text": "Load and Examine a Data Set\n\nLoad data and view\nExamine columns and data types\nExamine data normality\nDescribe properties of data\n\n\n# Read csv \ndata = pd.read_csv(\"data/diabetes.csv\")\n\n# Create Age_group from the age column\ndef Age_group_data(data): \n  if data.Age &gt;= 21 and data.Age &lt;= 30: return \"Young\"\n  elif data.Age &gt; 30 and data.Age &lt;= 50: return \"Middle\" \n  else: return \"Elderly\"\n\n# Apply the function to data\ndata['Age_group'] = data.apply(Age_group_data, axis = 1)\n\n# What does the data look like\ndata.head()\n\n   Pregnancies  Glucose  BloodPressure  ...  Age  Outcome  Age_group\n0            6      148             72  ...   50        1     Middle\n1            1       85             66  ...   31        0     Middle\n2            8      183             64  ...   32        1     Middle\n3            1       89             66  ...   21        0      Young\n4            0      137             40  ...   33        1     Middle\n\n[5 rows x 10 columns]\n\n\n\n\nData Normality\nNormal distributions (bell curves) are a common data assumptions for many hypothesis testing statistics, in particular parametric statistics. Deviations from normality can either strongly skew the results or reduce the power to detect a significant statistical difference.\nHere are the distribution properties to know and consider:\n\nThe mean, median, and mode are the same value.\nDistribution symmetry at the mean.\nNormal distributions can be described by the mean and standard deviation.\n\nHere’s an example using the Glucose column in our dataset\n\n\n\n\n\n\n\nDescribing Properties of our Data (Refined)\n\nSkewness\nThe symmetry of the distribution\nSee Section 4.3 for more information about these values\n\n# Make a copy of the data \ndataCopy = data.copy()\n\n# Select only numerical columns\ndataRed = dataCopy.select_dtypes(include = np.number)\n\n# List of numerical columns\ndataRedColsList = dataRed.columns[...]\n\n# For all values in the numerical column list from above\nfor i_col in dataRedColsList:\n  # List of the values in i_col\n  dataRed_i = dataRed.loc[:,i_col]\n  \n  # Skewness\n  skewness = round((dataRed_i.skew()), 3) \n  \n  # Kurtosis\n  kurtosis = round((dataRed_i.kurt()), 3)\n  \n  # Print a blank row\n  print('')\n  \n  # Print the column name\n  print(i_col)\n  \n  # Print skewness and kurtosis\n  print('skewness =', skewness, 'kurtosis =', kurtosis)\n\n\nPregnancies\nskewness = 0.902 kurtosis = 0.159\n\nGlucose\nskewness = 0.174 kurtosis = 0.641\n\nBloodPressure\nskewness = -1.844 kurtosis = 5.18\n\nSkinThickness\nskewness = 0.109 kurtosis = -0.52\n\nInsulin\nskewness = 2.272 kurtosis = 7.214\n\nBMI\nskewness = -0.429 kurtosis = 3.29\n\nDiabetesPedigreeFunction\nskewness = 1.92 kurtosis = 5.595\n\nAge\nskewness = 1.13 kurtosis = 0.643\n\nOutcome\nskewness = 0.635 kurtosis = -1.601\n\n\n\nskewness: skewness\nkurtosis: kurtosis"
  },
  {
    "objectID": "TransformingLikeDataTrans.html#testing-normality-accelerated",
    "href": "TransformingLikeDataTrans.html#testing-normality-accelerated",
    "title": "Transforming like a Data… Transformer",
    "section": "Testing Normality (Accelerated)",
    "text": "Testing Normality (Accelerated)\n\nQ-Q plots\nTesting overall normality of numerical columns\nTesting normality of groups\n\nNote that you can also run Shapiro-Wilk tests (see -Section 8), but since this test is not viable at N &lt; 20, I recommend just skipping to Q-Q plots.\n\n\nQ-Q Plots\nPlots of the quartiles of a target data set and plot it against predicted quartiles from a normal distribution (see -Section 8.2 for density and Q-Q plots)\n\n# Make a copy of the data \ndataCopy = data.copy()\n\n# Remove NAs\ndataCopyFin = dataCopy.dropna()\n\n# Drop Outcome, binary columns are never normally distributed \ndataCopyFin1 = dataCopyFin.drop('Outcome', axis = \"columns\")\n\n# Select only numerical columns\ndataRed = dataCopyFin1.select_dtypes(include = np.number)\n\n# Combine multiple plots, the number of columns and rows is derived from the number of numerical columns from above. \nfig, axes = plt.subplots(ncols = 2, nrows = 4, sharex = True, figsize = (2 * 4, 4 * 4))\n\n# Generate figures for all numerical grouped data subsets\nfor k, ax in zip(dataRed.columns, np.ravel(axes)):\n    sm.qqplot(dataRed[k], line = 's', ax = ax)\n    ax.set_title(f'{k}\\n QQ Plot')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "TransformingLikeDataTrans.html#normality-within-groups",
    "href": "TransformingLikeDataTrans.html#normality-within-groups",
    "title": "Transforming like a Data… Transformer",
    "section": "Normality within Groups",
    "text": "Normality within Groups\nLooking within Age_group at the subgroup normality\n\nQ-Q Plots\n\n# Make a copy of the data \ndataCopy = data.copy()\n\n# Remove NAs\ndataCopyFin = dataCopy.dropna()\n\n# Create a new column named in x, which is filled with the dataset rownames\ndataCopyFin.index.name = 'Index'\n\n# Reset the rownames index (not a column)\ndataCopyFin.reset_index(inplace = True)\n\n# Pivot the data from long-to-wide with pivot, using Date as the index, so that a column is created for each Group and numerical column subset\ndataPivot = dataCopyFin.pivot(index = 'Index', columns = 'Age_group', values = ['Insulin', 'Glucose', 'SkinThickness', 'BloodPressure'])\n\n# Select only numerical columns\ndataRed = dataPivot.select_dtypes(include = np.number)\n\n# Combine multiple plots, the number of columns and rows is derived from the number of numerical columns from above. \nfig, axes = plt.subplots(ncols = 2, nrows = 6, sharex = True, figsize = (2 * 4, 6 * 4))\n\n# Generate figures for all numerical grouped data subsets\nfor k, ax in zip(dataRed.columns, np.ravel(axes)):\n    sm.qqplot(dataRed[k], line = 's', ax = ax)\n    ax.set_title(f'{k}\\n QQ Plot')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "TransformingLikeDataTrans.html#transforming-data",
    "href": "TransformingLikeDataTrans.html#transforming-data",
    "title": "Transforming like a Data… Transformer",
    "section": "Transforming Data",
    "text": "Transforming Data\nYour data could be more easily interpreted with a transformation, since not all relationships in nature follow a linear relationship - i.e., many biological phenomena follow a power law (or logarithmic curve), where they do not scale linearly.\nWe will try to transform the Insulin column with through several approaches and discuss the pros and cons of each. First however, we will remove 0 values, because Insulin values are impossible…\n\n# Filter insulin greater than 0\nIns = data[data.Insulin &gt; 0]\n\n# Select only Insulin\nInsMod = Ins.filter([\"Insulin\"], axis = \"columns\")\n\n\n\nSquare-root, Cube-root, and Logarithmic Transformations\nResolving Skewness using the following data transformations:\nSquare-root transformation. \\(\\sqrt x\\) (moderate skew)\nLog transformation. \\(log(x)\\) (greater skew)\nLog + constant transformation. \\(log(x + 1)\\). Used for values that contain 0.\nInverse transformation. \\(1/x\\) (severe skew)\nSquared transformation. \\(x^2\\)\nCubed transformation. \\(x^3\\)\nWe will compare sqrt, log+1, and 1/x (inverse) transformations. Note that you would have to add a constant to use the log transformation, so it is easier to use the log+1 instead. You however need to add a constant to both the sqrt and 1/x transformations because they don’t include zeros and will otherwise skew the results.\n\n\nSquare-root Transformation\n\n# Square-root transform the data in a new column\nInsMod['Ins_Sqrt'] = np.sqrt(InsMod['Insulin'])\n\n# Specify desired column\ncol = InsMod.Insulin\n\n# Specify desired column\ni_col = InsMod.Ins_Sqrt\n\n# ORIGINAL\n# Subplots\nfig, (ax1, ax2) = plt.subplots(ncols = 2, nrows = 1)\n\n# Density plot\nsns.kdeplot(col, linewidth = 5, ax = ax1)\nax1.set_title('Insulin Density plot')    \n\n# Q-Q plot\nsm.qqplot(col, line='s', ax = ax2)\nax2.set_title('Insulin Q-Q plot')    \nplt.tight_layout()\nplt.show()\n\n\n\n# TRANSFORMED\n# Subplots\nfig, (ax1, ax2) = plt.subplots(ncols = 2, nrows = 1)\n\n# Density plot\nsns.kdeplot(i_col, linewidth = 5, ax = ax1)\nax1.set_title('Ins_Sqrt Density plot')   \n\n# Q-Q plot\nsm.qqplot(i_col, line='s', ax = ax2)\nax2.set_title('Ins_Sqrt Q-Q plot') \nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nLogarithmic (+1) Transformation\n\n# Logarithmic transform the data in a new column\nInsMod['Ins_Log'] = np.log(InsMod['Insulin'] + 1)\n\n# Specify desired column\ncol = InsMod.Insulin\n\n# Specify desired column\ni_col = InsMod.Ins_Log\n\n# ORIGINAL\n# Subplots\nfig, (ax1, ax2) = plt.subplots(ncols = 2, nrows = 1)\n\n# Density plot\nsns.kdeplot(col, linewidth = 5, ax = ax1)\nax1.set_title('Insulin Density plot')    \n\n# Q-Q plot\nsm.qqplot(col, line='s', ax = ax2)\nax2.set_title('Insulin Q-Q plot')    \nplt.tight_layout()\nplt.show()\n\n\n\n# TRANSFORMED\n# Subplots\nfig, (ax1, ax2) = plt.subplots(ncols = 2, nrows = 1)\n\n# Density plot\nsns.kdeplot(i_col, linewidth = 5, ax = ax1)\nax1.set_title('Ins_Log Density plot')   \n\n# Q-Q plot\nsm.qqplot(i_col, line='s', ax = ax2)\nax2.set_title('Ins_Log Q-Q plot') \nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nInverse Transformation\n\n# Inverse transform the data in a new column\nInsMod['Ins_Inv'] = 1/InsMod.Insulin\n\n# Specify desired column\ncol = InsMod.Insulin\n\n# Specify desired column\ni_col = InsMod.Ins_Inv\n\n# ORIGINAL\n# Subplots\nfig, (ax1, ax2) = plt.subplots(ncols = 2, nrows = 1)\n\n# Density plot\nsns.kdeplot(col, linewidth = 5, ax = ax1)\nax1.set_title('Insulin Density plot')    \n\n# Q-Q plot\nsm.qqplot(col, line='s', ax = ax2)\nax2.set_title('Insulin Q-Q plot')    \nplt.tight_layout()\nplt.show()\n\n\n\n# TRANSFORMED\n# Subplots\nfig, (ax1, ax2) = plt.subplots(ncols = 2, nrows = 1)\n\n# Density plot\nsns.kdeplot(i_col, linewidth = 5, ax = ax1)\nax1.set_title('Ins_Inv Density plot')   \n\n# Q-Q plot\nsm.qqplot(i_col, line='s', ax = ax2)\nax2.set_title('Ins_Inv Q-Q plot') \nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nBox-cox Transformation\nThere are several transformations, each with it’s own “criteria”, and they don’t always fix extremely skewed data. Instead, you can just choose the Box-Cox transformation which searches for the the best lambda value that maximizes the log-likelihood (basically, what power transformation is best). The benefit is that you should have normally distributed data after, but the power relationship might be pretty abstract (i.e., what would a transformation of x^0.12 be interpreted as in your system?..)\n\n# Box-cox transform the data in a new column\nInsMod['Ins_Boxcox'], parameters = stats.boxcox(InsMod['Insulin'])\n\n# Specify desired column\ncol = InsMod.Insulin\n\n# Specify desired column\ni_col = InsMod.Ins_Boxcox\n\n# ORIGINAL\n# Subplots\nfig, (ax1, ax2) = plt.subplots(ncols = 2, nrows = 1)\n\n# Density plot\nsns.kdeplot(col, linewidth = 5, ax = ax1)\nax1.set_title('Insulin Density plot')    \n\n# Q-Q plot\nsm.qqplot(col, line='s', ax = ax2)\nax2.set_title('Insulin Q-Q plot')    \nplt.tight_layout()\nplt.show()\n\n\n\n# TRANSFORMED\n# Subplots\nfig, (ax1, ax2) = plt.subplots(ncols = 2, nrows = 1)\n\n# Density plot\nsns.kdeplot(i_col, linewidth = 5, ax = ax1)\nax1.set_title('Ins_Boxcox Density plot')   \n\n# Q-Q plot\nsm.qqplot(i_col, line='s', ax = ax2)\nax2.set_title('Ins_Boxcox Q-Q plot') \nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "ImputingLikeDataScientist.html#purpose-of-this-chapter",
    "href": "ImputingLikeDataScientist.html#purpose-of-this-chapter",
    "title": "Imputing like a Data Scientist",
    "section": "Purpose of this chapter",
    "text": "Purpose of this chapter\nExploring, visualizing, and imputing outliers and missing values (NAs) in a novel data set\nIMPORTANT NOTE: imputation should only be used when missing data is unavoidable and probably limited to 10% of your data being outliers / missing data (though some argue imputation is necessary between 30-60%). Ask what the cause is for the outlier and missing data."
  },
  {
    "objectID": "ImputingLikeDataScientist.html#take-aways",
    "href": "ImputingLikeDataScientist.html#take-aways",
    "title": "Imputing like a Data Scientist",
    "section": "Take-aways",
    "text": "Take-aways\n\nLoad and explore a data set with publication quality tables\nThoroughly diagnose outliers and missing values\nImpute outliers and missing values"
  },
  {
    "objectID": "ImputingLikeDataScientist.html#required-setup",
    "href": "ImputingLikeDataScientist.html#required-setup",
    "title": "Imputing like a Data Scientist",
    "section": "Required Setup",
    "text": "Required Setup\nWe first need to prepare our environment with the necessary libraries and set a global theme for publishable plots in seaborn.\n\n# Import all required libraries\n# Data analysis and manipulation\nimport pandas as pd\n# Working with arrays\nimport numpy as np\n# Statistical visualization\nimport seaborn as sns\n# Matlab plotting for Python\nimport matplotlib.pyplot as plt\n# Data analysis\nimport statistics as stat\nimport scipy.stats as stats\n# Visualizing missing values\nimport missingno as msno\n# Statistical modeling\nimport statsmodels.api as smx\n# Predictive data analysis: process data \nfrom sklearn import preprocessing as pproc\n# Predictive data analysis: outlier imputation\nfrom sklearn.impute import SimpleImputer\n# Predictive data analysis: KNN NA imputation\nfrom sklearn.impute import KNNImputer\n# Predictive data analysis: experimental iterative NA imputer (MICE)\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n# Predictive data analysis: linear models\nfrom sklearn.linear_model import LinearRegression\n# Predictive data analysis: Classifying nearest neighbors\nfrom sklearn import neighbors\n# Predictive data analysis: Plotting decision regions\nfrom mlxtend.plotting import plot_decision_regions\n\n# Increase font size of all seaborn plot elements\nsns.set(font_scale = 1.5, rc = {'figure.figsize':(8, 8)})\n\n# Change theme to \"white\"\nsns.set_style(\"white\")"
  },
  {
    "objectID": "ImputingLikeDataScientist.html#load-and-examine-a-data-set",
    "href": "ImputingLikeDataScientist.html#load-and-examine-a-data-set",
    "title": "Imputing like a Data Scientist",
    "section": "Load and Examine a Data Set",
    "text": "Load and Examine a Data Set\n\n# Read csv \ndata = pd.read_csv(\"data/diabetes.csv\")\n\n# Create Age_group from the age column\ndef Age_group_data(data): \n  if data.Age &gt;= 21 and data.Age &lt;= 30: return \"Young\"\n  elif data.Age &gt; 30 and data.Age &lt;= 50: return \"Middle\" \n  else: return \"Elderly\"\n\n# Apply the function to data\ndata['Age_group'] = data.apply(Age_group_data, axis = 1)\n\n# What does the data look like\ndata.head()\n\n   Pregnancies  Glucose  BloodPressure  ...  Age  Outcome  Age_group\n0            6      148             72  ...   50        1     Middle\n1            1       85             66  ...   31        0     Middle\n2            8      183             64  ...   32        1     Middle\n3            1       89             66  ...   21        0      Young\n4            0      137             40  ...   33        1     Middle\n\n[5 rows x 10 columns]"
  },
  {
    "objectID": "ImputingLikeDataScientist.html#diagnose-your-data",
    "href": "ImputingLikeDataScientist.html#diagnose-your-data",
    "title": "Imputing like a Data Scientist",
    "section": "Diagnose your Data",
    "text": "Diagnose your Data\n\n# What are the properties of the data\ndiagnose = data.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 768 entries, 0 to 767\nData columns (total 10 columns):\n #   Column                    Non-Null Count  Dtype  \n---  ------                    --------------  -----  \n 0   Pregnancies               768 non-null    int64  \n 1   Glucose                   768 non-null    int64  \n 2   BloodPressure             768 non-null    int64  \n 3   SkinThickness             768 non-null    int64  \n 4   Insulin                   768 non-null    int64  \n 5   BMI                       768 non-null    float64\n 6   DiabetesPedigreeFunction  768 non-null    float64\n 7   Age                       768 non-null    int64  \n 8   Outcome                   768 non-null    int64  \n 9   Age_group                 768 non-null    object \ndtypes: float64(2), int64(7), object(1)\nmemory usage: 60.1+ KB\n\n\n\nColumn: name of each variable\nNon-Null Count: number of missing values\nDType: data type of each variable"
  },
  {
    "objectID": "ImputingLikeDataScientist.html#diagnose-outliers",
    "href": "ImputingLikeDataScientist.html#diagnose-outliers",
    "title": "Imputing like a Data Scientist",
    "section": "Diagnose Outliers",
    "text": "Diagnose Outliers\nThere are several numerical variables that have outliers above, let’s see what the data look like with and without them\n\nCreate a table with columns containing outliers\nPlot outliers in a box plot and histogram\n\n\n# Make a copy of the data \ndataCopy = data.copy()\n\n# Select only numerical columns\ndataRed = dataCopy.select_dtypes(include = np.number)\n\n# List of numerical columns\ndataRedColsList = dataRed.columns[...]\n\n# For all values in the numerical column list from above\nfor i_col in dataRedColsList:\n  # List of the values in i_col\n  dataRed_i = dataRed.loc[:,i_col]\n  \n  # Define the 25th and 75th percentiles\n  q25, q75 = round((dataRed_i.quantile(q = 0.25)), 3), round((dataRed_i.quantile(q = 0.75)), 3)\n  \n  # Define the interquartile range from the 25th and 75th percentiles defined above\n  IQR = round((q75 - q25), 3)\n  \n  # Calculate the outlier cutoff \n  cut_off = IQR * 1.5\n  \n  # Define lower and upper cut-offs\n  lower, upper = round((q25 - cut_off), 3), round((q75 + cut_off), 3)\n  \n  # Print the values\n  print(' ')\n  \n  # For each value of i_col, print the 25th and 75th percentiles and IQR\n  print(i_col, 'q25=', q25, 'q75=', q75, 'IQR=', IQR)\n  \n  # Print the lower and upper cut-offs\n  print('lower, upper:', lower, upper)\n\n  # Count the number of outliers outside the (lower, upper) limits, print that value\n  print('Number of Outliers: ', dataRed_i[(dataRed_i &lt; lower) | (dataRed_i &gt; upper)].count())\n\n \nPregnancies q25= 1.0 q75= 6.0 IQR= 5.0\nlower, upper: -6.5 13.5\nNumber of Outliers:  4\n \nGlucose q25= 99.0 q75= 140.25 IQR= 41.25\nlower, upper: 37.125 202.125\nNumber of Outliers:  5\n \nBloodPressure q25= 62.0 q75= 80.0 IQR= 18.0\nlower, upper: 35.0 107.0\nNumber of Outliers:  45\n \nSkinThickness q25= 0.0 q75= 32.0 IQR= 32.0\nlower, upper: -48.0 80.0\nNumber of Outliers:  1\n \nInsulin q25= 0.0 q75= 127.25 IQR= 127.25\nlower, upper: -190.875 318.125\nNumber of Outliers:  34\n \nBMI q25= 27.3 q75= 36.6 IQR= 9.3\nlower, upper: 13.35 50.55\nNumber of Outliers:  19\n \nDiabetesPedigreeFunction q25= 0.244 q75= 0.626 IQR= 0.382\nlower, upper: -0.329 1.199\nNumber of Outliers:  29\n \nAge q25= 24.0 q75= 41.0 IQR= 17.0\nlower, upper: -1.5 66.5\nNumber of Outliers:  9\n \nOutcome q25= 0.0 q75= 1.0 IQR= 1.0\nlower, upper: -1.5 2.5\nNumber of Outliers:  0\n\n\n\nq25: 1/4 quartile, 25th percentile\nq75: 3/4 quartile, 75th percentile\nIQR: interquartile range (q75-q25)\nlower: lower limit of \\(1.5*IQR\\) used to calculate outliers\nupper: upper limit of \\(1.5*IQR\\) used to calculate outliers"
  },
  {
    "objectID": "ImputingLikeDataScientist.html#basic-exploration-of-missing-values-nas",
    "href": "ImputingLikeDataScientist.html#basic-exploration-of-missing-values-nas",
    "title": "Imputing like a Data Scientist",
    "section": "Basic Exploration of Missing Values (NAs)",
    "text": "Basic Exploration of Missing Values (NAs)\n\nTable showing the extent of NAs in columns containing them\n\n\ndataNA = data\n\nfor col in dataNA.columns:\n    dataNA.loc[dataNA.sample(frac = 0.1).index, col] = np.nan\n    \ndataNA.isnull().sum()\n\nPregnancies                 77\nGlucose                     77\nBloodPressure               77\nSkinThickness               77\nInsulin                     77\nBMI                         77\nDiabetesPedigreeFunction    77\nAge                         77\nOutcome                     77\nAge_group                   77\ndtype: int64\n\n\nBar plot showing all NA values in each column. Since we randomly produced a set amount above the numbers will all be the same.\n\nmsno.bar(dataNA, figsize = (8, 8), fontsize = 10)\nplt.tight_layout()"
  },
  {
    "objectID": "ImputingLikeDataScientist.html#advanced-exploration-of-missing-values-nas",
    "href": "ImputingLikeDataScientist.html#advanced-exploration-of-missing-values-nas",
    "title": "Imputing like a Data Scientist",
    "section": "Advanced Exploration of Missing Values (NAs)",
    "text": "Advanced Exploration of Missing Values (NAs)\nThis matrix shows the number of missing values throughout each column.\n\nX-axis is the column names\nLeft Y-axis is the row number\nRight Y-axis is a line plot that shows each row’s completeness, e.g., if there are 11 columns, 4-10 valid values means that there are 1-7 missing values in a row.\n\n\ndataNA1 = dataNA.drop('DiabetesPedigreeFunction', axis = \"columns\")\n\n# NA matric\nmsno.matrix(dataNA1, figsize = (8, 8), fontsize = 10)"
  },
  {
    "objectID": "ImputingLikeDataScientist.html#impute-outliers",
    "href": "ImputingLikeDataScientist.html#impute-outliers",
    "title": "Imputing like a Data Scientist",
    "section": "Impute Outliers",
    "text": "Impute Outliers\nRemoving outliers and NAs can be tricky, but there are methods to do so. I will go over several, and discuss benefits and costs to each.\nThe principle goal for all imputation is to find the method that does not change the distribution too much (or oddly).\n\n\nClassifying Outliers\nBefore imputing outliers, you will want to diagnose whether it’s they are natural outliers or not. We will be looking at “Insulin” for example across Age_group, because there are several outliers and NAs, which we will impute below.\n\n# Increase font size of all seaborn plot elements\nsns.set(font_scale = 1.25, rc = {'figure.figsize':(6, 8)})\n\n# Change theme to \"white\"\nsns.set_style(\"white\")\n\n# Box plot\nAge_Box = sns.boxplot(data = data, x = \"Insulin\", y = \"Age_group\", width = 0.3)\n\n# Tweak the visual presentation\nAge_Box.set(ylabel = \"Age group\")\n\n\n\n\n\n\n\n\nNow let’s say that we want to impute extreme values and remove outliers that don’t make sense, such as Insulin levels &gt; 600 mg/dL: values greater than this induce a diabetic coma.\nWe remove outliers using SimpleImputer from sklearn and replace them with values that are estimates based on the existing data\n\nMean: arithmetic mean\nMedian: median\nMode: mode\nCapping: Impute the upper outliers with 95 percentile, and impute the bottom outliers with 5 percentile - aka Winsorizing\n\n\n# Select only Insulin\nInsMod = data.filter([\"Insulin\"], axis = \"columns\")\n\n\n\n\nMean Imputation\nThe mean of the observed values for each variable is computed and the outliers for that variable are imputed by this mean\n\n# Python can't impute outliers easily, so we will convert them to NAs and imputate them\nInsMod.loc[InsMod.Insulin &gt; 600, 'Insulin'] = np.nan\n\n# Set mean imputation algorithm\nMean_Impute = SimpleImputer(missing_values = np.nan, strategy = 'mean')\n\n# Fit imputation\nMean_Impute = Mean_Impute.fit(InsMod[['Insulin']])\n\n# Transform NAs with the mean imputation\nInsMod['Ins_Mean'] = Mean_Impute.transform(InsMod[['Insulin']])\n\n\n# Visualization of the mean imputation\n# Original data\nmean_plot = sns.kdeplot(data = InsMod, x = 'Insulin', linewidth = 2, label = \"Original\")\n\n# Mean imputation\nmean_plot = sns.kdeplot(data = InsMod, x = 'Ins_Mean', linewidth = 2, label = \"Mean Imputated\")\n\n# Show legend\nplt.legend()\n\n# Show plot\nplt.show()\n\n\n\n\n\n\n\nMedian Imputation\nThe median of the observed values for each variable is computed and the outliers for that variable are imputed by this median\n\n# Python can't impute outliers easily, so we will convert them to NAs and imputate them\nInsMod.loc[InsMod.Insulin &gt; 600, 'Insulin'] = np.nan\n\n# Set median imputation algorithm\nMedian_Impute = SimpleImputer(missing_values = np.nan, strategy = 'median')\n\n# Fit imputation\nMedian_Impute = Median_Impute.fit(InsMod[['Insulin']])\n\n# Transform NAs with the median imputation\nInsMod['Ins_Median'] = Median_Impute.transform(InsMod[['Insulin']])\n\n\n# Visualization of the median imputation\n# Original data\nmedian_plot = sns.kdeplot(data = InsMod, x = 'Insulin', linewidth = 2, label = \"Original\")\n\n# Median imputation\nmedian_plot = sns.kdeplot(data = InsMod, x = 'Ins_Median', linewidth = 2, label = \"Median Imputated\")\n\n# Show legend\nplt.legend()\n\n# Show plot\nplt.show()\n\n\n\n\n\n\nPros & Cons of Using the Mean or Median Imputation\nPros:\n\nEasy and fast.\nWorks well with small numerical datasets.\n\nCons:\n\nDoesn’t factor the correlations between features. It only works on the column level.\nWill give poor results on encoded categorical features (do NOT use it on categorical features).\nNot very accurate.\nDoesn’t account for the uncertainty in the imputations.\n\n\n\n\n\nMode Imputation\nThe mode of the observed values for each variable is computed and the outliers for that variable are imputed by this mode\n\n# Python can't impute outliers easily, so we will convert them to NAs and imputate them\nInsMod.loc[InsMod.Insulin &gt; 600, 'Insulin'] = np.nan\n\n# Set mode imputation algorithm\nMode_Impute = SimpleImputer(missing_values = np.nan, strategy = 'most_frequent')\n\n# Fit imputation\nMode_Impute = Mode_Impute.fit(InsMod[['Insulin']])\n\n# Transform NAs with the mode imputation\nInsMod['Ins_Mode'] = Mode_Impute.transform(InsMod[['Insulin']])\n\n\n# Visualization of the mode imputation\n# Original data\nmode_plot = sns.kdeplot(data = InsMod, x = 'Insulin', linewidth = 2, label = \"Original\")\n\n# Mode imputation\nmode_plot = sns.kdeplot(data = InsMod, x = 'Ins_Mode', linewidth = 2, label = \"Mode Imputated\")\n\n# Show legend\nplt.legend()\n\n# Show plot\nplt.show()\n\n\n\n\n\n\nPros & Cons of Using the Mode Imputation\nPros:\n\nWorks well with categorical features.\n\nCons:\n\nIt also doesn’t factor the correlations between features.\nIt can introduce bias in the data.\n\n\n\n\n\nCapping Imputation (aka Winsorizing)\nThe Percentile Capping is a method of Imputing the outlier values by replacing those observations outside the lower limit with the value of 5th percentile and those that lie above the upper limit, with the value of 95th percentile of the same dataset.\n\n# Winsorizing deals specifically with outliers, so we don't have to worry about changing outliers to NAs\n\n# New column for capping imputated data at the lowest and highest 10% of values\nInsMod['Ins_Cap'] = pd.DataFrame(stats.mstats.winsorize(InsMod['Insulin'], limits = [0.05, 0.05]))\n\n\n# Visualization of the capping imputation\n# Original data\ncap_plot = sns.kdeplot(data = InsMod, x = 'Insulin', linewidth = 2, label = \"Original\")\n\n# Capping imputation\ncap_plot = sns.kdeplot(data = InsMod, x = 'Ins_Cap', linewidth = 2, label = \"Capping Imputated\")\n\n# Show legend\nplt.legend()\n\n# Show plot\nplt.show()\n\n\n\n\n\n\nPros and Cons of Capping\nPros:\n\nNot influenced by extreme values\n\nCons:\n\nCapping only modifies the smallest and largest values slightly. This is generally not a good idea since it means we’re just modifying data values for the sake of modifications.\nIf no extreme outliers are present, Winsorization may be unnecessary."
  },
  {
    "objectID": "ImputingLikeDataScientist.html#imputing-nas",
    "href": "ImputingLikeDataScientist.html#imputing-nas",
    "title": "Imputing like a Data Scientist",
    "section": "Imputing NAs",
    "text": "Imputing NAs\nI will only be addressing a subset of methods for NA imputation, but you can use the mean, median, and mode methods from above as well:\n\nKNN: K-nearest neighbors\nMICE: Multivariate Imputation by Chained Equations\n\nSince our normal data has no NA values, we will add the Insulin column from the dataNA we created earlier and replace the original with it.\n\n# Make a copy of the data \ndataCopy = data.copy()\n\n# Select the Insulin \nInsNA = dataNA.filter([\"Insulin\"], axis = \"columns\")\n\n# Add Insulin with NAs to copy of original data\ndataCopy['Insulin'] = InsNA\n\n\n\nK-Nearest Neighbor (KNN) Imputation\nKNN is a machine learning algorithm that classifies data by similarity. This in effect clusters data into similar groups. The algorithm predicts values of new data to replace NA values based on how closely they resembles training data points, such as by comparing across other columns.\nHere’s a visual example using the plot_decision_regions function from mlxtend.plotting library to run a KNN algorithm on our dataset, where three clusters are created by the algorithm.\n\n# KNN plot function\ndef knn_comparision(data, k):\n  # Define x and y values (your data will need to have these)\n    X = data[['x1','x2']].values\n    y = data['y'].astype(int).values\n    # Knn function, defining the number of neighbors\n    clf = neighbors.KNeighborsClassifier(n_neighbors = k)\n    # Fit knn algorithm to data\n    clf.fit(X, y)\n\n    # Plotting decision regions\n    plot_decision_regions(X, y, clf = clf, legend = 2)\n\n    # Adding axes annotations\n    plt.xlabel('X1')\n    plt.ylabel('X2')\n    plt.title('Knn with K='+ str(k))\n    plt.legend(loc = 'upper right')\n    plt.tight_layout()\n    plt.show()\n\n\n# Prepare data for the KNN plotting function\ndata1 = data.loc[:, ['Insulin', 'Glucose', 'Outcome']]\n\n# Drop NAs\ndata1 = data1.dropna()\n\n# Set the two target x variables and the binary y variable we are clustering the data from\ndata1 = data1.rename(columns = {'Insulin': 'x1', 'Glucose': 'x2', 'Outcome': 'y'})\n\n# Create KNN plot for 3 nearest neighbors\nknn_comparision(data1, 3)\n\n\n\n\nYou can also loop the KNN plots for i nearest neighbors:\n\n# Loop to create KNN plots for i number of nearest neighbors\nfor i in [1, 5, 15]:\n  knn_comparision(data1, i)\n\nNote, we have to change a three things to make KNNImputer work correctly:\n\nWe need to change any characters, into dummy variables that are numericals, because scalars and imputers do not recognize characters. In this case, Age_group is an ordinal category, so we will use OrdinalEncoder from Scikit-learn, specifically in preprocessing which we imported as pproc.\n\n\n# Numeric dummy variable from our Age_group ordinal column\n# Define the orginal encoder \nenc = pproc.OrdinalEncoder()\n\n# Ordinal variable from Age_group column \ndataCopy[['Age_group']] = enc.fit_transform(dataCopy[['Age_group']])\n\n\nWe need to reorder our target column with NAs to the end of the dataframe so that the rest of the dataframe can be called as training data more easily.\n\n\n# Reorder columns\ndataCopy = dataCopy[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', \"BMI\", \"DiabetesPedigreeFunction\", \"Age\", \"Outcome\", \"Age_group\", \"Insulin\"]]\n\n\nKNNImputater is distance-based so we need to normalize our data. Otherwise KNNImputer will create biased replacement. We will use the pproc.MinMaxScaler from Scikit-learn, which scales our values from 0-1.\n\n\n# Min-max schaler\nscaler = pproc.MinMaxScaler()\n\n# Scale columns\ndataCopy_Scale = pd.DataFrame(scaler.fit_transform(dataCopy), columns = dataCopy.columns)\n\nWe are finally ready to for KNN Imputation!\n\n# Set KNN imputation function parameters\nimputer = KNNImputer(n_neighbors = 3)\n\n# Fit imputation\nDataKnn = pd.DataFrame(imputer.fit_transform(dataCopy_Scale),columns = dataCopy_Scale.columns)\n\n\n# Add KNN imputated column to original dataCopy\ndataCopy_Scale[['InsKnn']] = DataKnn[['Insulin']]\n\n# Visualization of the KNN imputation\n# Original data\nknn_plot = sns.kdeplot(data = dataCopy_Scale, x = 'Insulin', linewidth = 2, label = \"Original\")\n\n# KNN imputation\nknn_plot = sns.kdeplot(data = dataCopy_Scale, x = 'InsKnn', linewidth = 2, label = \"KNN Imputated\")\n\n# Show legend\nplt.legend()\n\n# Show plot\nplt.show()\n\n\n\n\n\n\nPros & Cons of Using KNN Imputation\nPro:\n\nPossibly much more accurate than mean, median, or mode imputation for some data sets.\n\nCons:\n\nKNN is computationally expensive because it stores the entire training dataset into computer memory.\nKNN is very sensitive to outliers, so you would have to imputate these first.\n\n\n\n\n\nMultivariate Imputation by Chained Equations (MICE)\nMICE is an algorithm that fills missing values multiple times, hence dealing with uncertainty better than other methods. This approach creates multiple copies of the data that can then be analyzed and then pooled into a single dataset.\n\n\n\nImage Credit: Will Badr\n\n\n\n# Assign a regression model\nlm = LinearRegression()\n\n# Set MICE imputation function parameters\nimputer = IterativeImputer(estimator = lm, missing_values = np.nan, max_iter = 10, verbose = 2, imputation_order = 'roman', random_state = 0)\n\n# Fit imputation\ndataMice = pd.DataFrame(imputer.fit_transform(dataCopy),columns = dataCopy.columns)\n\n\n# Add MICE imputated column to original dataCopy\ndataCopy[['InsMice']] = dataMice[['Insulin']]\n\n# Visualization of the MICE imputation\n# Original data\nmice_plot = sns.kdeplot(data = dataCopy, x = 'Insulin', linewidth = 2, label = \"Original\")\n\n# MICE imputation\nmice_plot = sns.kdeplot(data = dataCopy, x = 'InsMice', linewidth = 2, label = \"MICE Imputated\")\n\n# Show legend\nplt.legend()\n\n# Show plot\nplt.show()\n\n\n\n\n\n\nPros & Cons of MICE Imputation\nPros:\n\nMultiple imputations are more accurate than a single imputation.\nThe chained equations are very flexible to data types, such as categorical and ordinal.\n\nCons:\n\nYou have to round the results for ordinal data because resulting data points are too great or too small (floating-points)."
  },
  {
    "objectID": "CorrelateLikeDataMaster.html#purpose-of-this-chapter",
    "href": "CorrelateLikeDataMaster.html#purpose-of-this-chapter",
    "title": "Correlating Like a Data Master",
    "section": "Purpose of this chapter",
    "text": "Purpose of this chapter\nAssess relationships within a novel data set"
  },
  {
    "objectID": "CorrelateLikeDataMaster.html#take-aways",
    "href": "CorrelateLikeDataMaster.html#take-aways",
    "title": "Correlating Like a Data Master",
    "section": "Take-aways",
    "text": "Take-aways\n\nDescribe and visualize correlations between numerical variables\nVisualize correlations of all numerical variables within groups\nDescribe and visualize relationships based on target variables\n\n\n\nRequired setup\nWe first need to prepare our environment with the necessary libraries and set a global theme for publishable plots in seaborn.\n\n# Import all required libraries\n# Data analysis and manipulation\nimport pandas as pd\n# Working with arrays\nimport numpy as np\n# Statistical visualization\nimport seaborn as sns\n# Matlab plotting for Python\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\n# Data analysis\nimport statistics as stat\nimport scipy.stats as stats\n# Two-sample Chi-Square test\nfrom scipy.stats import chi2_contingency\n# Predictive data analysis: process data \nfrom sklearn import preprocessing as pproc\n# Predictive data analysis: linear models\nfrom sklearn.model_selection import cross_val_predict\n# Predictive data analysis: linear models\nfrom sklearn.linear_model import LinearRegression\n# Visualizing missing values\nimport missingno as msno\n# Statistical modeling\nimport statsmodels.api as sm\n# Statistical modeling: ANOVA\nfrom statsmodels.formula.api import ols\n# Mosaic plot\nfrom statsmodels.graphics.mosaicplot import mosaic\nfrom itertools import product\n\n# Increase font and figure size of all seaborn plot elements\nsns.set(font_scale = 1.5, rc = {'figure.figsize':(8, 8)})\n\n# Change theme to \"white\"\nsns.set_style(\"white\")"
  },
  {
    "objectID": "CorrelateLikeDataMaster.html#load-the-examine-a-data-set",
    "href": "CorrelateLikeDataMaster.html#load-the-examine-a-data-set",
    "title": "Correlating Like a Data Master",
    "section": "Load the Examine a Data Set",
    "text": "Load the Examine a Data Set\nWe will be using open source data from UArizona researchers that investigates the effects of climate change on canopy trees. (Meredith, Ladd, and Werner 2021)\n\n# Read csv \ndata = pd.read_csv(\"data/Data_Fig2_Repo.csv\")\n\n# Convert 'Date' column to datetime\ndata['Date'] = pd.to_datetime(data['Date'])\n\n# What does the data look like\ndata.head()\n\n        Date                Group    Sap_Flow  TWaterFlux      pLWP      mLWP\n0 2019-10-04  Drought-sens-canopy  184.040975   82.243292 -0.263378 -0.679769\n1 2019-10-04   Drought-sens-under    2.475989    1.258050 -0.299669 -0.761326\n2 2019-10-04   Drought-tol-canopy   10.598949    4.405479 -0.437556 -0.722557\n3 2019-10-04    Drought-tol-under    4.399854    2.055276 -0.205224 -0.702858\n4 2019-10-05  Drought-sens-canopy  182.905444   95.865255 -0.276928 -0.708261"
  },
  {
    "objectID": "CorrelateLikeDataMaster.html#describe-and-visualize-correlations",
    "href": "CorrelateLikeDataMaster.html#describe-and-visualize-correlations",
    "title": "Correlating Like a Data Master",
    "section": "Describe and Visualize Correlations",
    "text": "Describe and Visualize Correlations\nCorrelations are a statistical relationship between two numerical variables, may or may not be causal. Exploring correlations in your data allows you determine data independence, a major assumption of parametric statistics, which means your variables are both randomly collected.\n\nIf you’re interested in some underlying statistics…\nNote that the we will use the Pearson’s \\(r\\) coefficient in corr() function from the pandas library, but you can specify any method you would like: corr(method = \"\"), where the method can be \"pearson\" for Pearson’s \\(r\\), \"spearman\" for Spearman’s \\(\\rho\\), or \"kendall\" for Kendall’s \\(\\tau\\). The main differences are that Pearson’s \\(r\\) assumes a normal distribution for ALL numerical variables, whereas Spearman’s \\(\\rho\\) and Kendall’s \\(\\tau\\) do not, but Spearman’s \\(\\rho\\) requires \\(N &gt; 10\\), and Kendall’s \\(\\tau\\) does not. Notably, Kendall’s \\(\\tau\\) performs as well as Spearman’s \\(\\rho\\) when \\(N &gt; 10\\), so its best to just use Kendall’s \\(\\tau\\) when data are not normally distributed.\n\n# subset dataframe to include only numeric columns\nnumData = data.select_dtypes(include='number')\n\n# Table of correlations between numerical variables (we are sticking to the default Pearson's r coefficient)\nnumData.corr()\n\n            Sap_Flow  TWaterFlux      pLWP      mLWP\nSap_Flow    1.000000    0.988137  0.120281 -0.201195\nTWaterFlux  0.988137    1.000000  0.125645 -0.189330\npLWP        0.120281    0.125645  1.000000  0.677651\nmLWP       -0.201195   -0.189330  0.677651  1.000000\n\n\n\n# Heatmap correlation matrix of numerical variables\n# Correlation matrix\ncorr = numData.corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype = bool))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(230, 20, as_cmap = True)\n\n# Heatmap of the correlation matrix\nsns.heatmap(corr, cmap = cmap, mask = mask, vmax = 0.3, center = 0,\n            square = True, linewidths = 0.5, cbar_kws = {\"shrink\": .5})\n            \n# Tight margins for plot\nplt.tight_layout()\n\n# Show plot\nplt.show()"
  },
  {
    "objectID": "CorrelateLikeDataMaster.html#visualize-correlations-within-groups",
    "href": "CorrelateLikeDataMaster.html#visualize-correlations-within-groups",
    "title": "Correlating Like a Data Master",
    "section": "Visualize Correlations within Groups",
    "text": "Visualize Correlations within Groups\nIf we have groups that we will compare later on, it is a good idea to see how each numerical variable correlates within these groups.\n\n# Increase font and figure size of all seaborn plot elements\nsns.set(font_scale = 1.5, rc = {'figure.figsize':(10, 10)})\n\n# Change theme to \"white\"\nsns.set_style(\"white\")\n\n# Heatmap correlation matrix of numerical variables\n# Correlation matrix\ncorr = data.groupby('Group').corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype = bool))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(230, 20, as_cmap = True)\n\n# Heatmap of the correlation matrix\nax = sns.heatmap(corr, cmap = cmap, mask = mask, vmax = 0.3, center = 0,\n            square = True, linewidths = 0.5, cbar_kws = {\"shrink\": .5})\n            \n# Change y-axis label\nax.set(ylabel = 'Group')\n\n# Tight margins for plot\nplt.tight_layout()\n\n# Show plot\nplt.show()\n\n\n\n\nThis is great, we have our correlations within groups! However, the correlation matrices aren’t always the most intuitive, so let’s plot!\nSpecifically, we are looking at the correlations between predawn leaf water potential pLWP and midday leaf water potential mLWP. Leaf water potential is a key indicator for how stressed plants are in droughts.\n\ndataplot = data[[\"Group\", \"pLWP\", \"mLWP\"]]\n\n# Increase font and figure size of all seaborn plot elements\nsns.set(font_scale = 2.5, rc = {'figure.figsize':(10, 10)})\n\n# Change seaborn plot theme to white\nsns.set_style(\"white\")\n\n# Empty subplot grid for pairwise relationships\ng = sns.PairGrid(dataplot, hue = \"Group\", height = 5)\n\n# Add scatterplots to the upper portion of the grid\ng1 = g.map_upper(sns.scatterplot, alpha = 0.5, s = 100)\n\n# Add a kernal density plot to the diagonal of the grid\ng2 = g1.map_diag(sns.kdeplot, fill = True, linewidth = 3)\n\n# Add a kernal density plot to the lower portion of the grid\ng3 = g2.map_lower(sns.kdeplot, levels = 5, alpha = 0.75)\n\n# Remove legend title\ng4 = g3.add_legend(title = \"\", adjust_subtitles = True)\n\n# Show plot\nplt.show()"
  },
  {
    "objectID": "CorrelateLikeDataMaster.html#describe-and-visualize-relationships-based-on-target-variables",
    "href": "CorrelateLikeDataMaster.html#describe-and-visualize-relationships-based-on-target-variables",
    "title": "Correlating Like a Data Master",
    "section": "Describe and Visualize Relationships Based on Target Variables",
    "text": "Describe and Visualize Relationships Based on Target Variables\n\nTarget Variables\nTarget variables are essentially numerical or categorical variables that you want to relate others to in a data frame.\nThe relationships below will have the formula relationship target ~ predictor.\n\n\n\nNumerical Target Variables: Numerical Variable of Interest\nFormula: pLWP (numerical)  ~ mLWP (numerical)\n\n# The numerical predictor variable \nX = data[[\"mLWP\"]]\n\n# The numerical target variable\nY = data[[\"pLWP\"]]\n\n# Define the linear model, drop NAs\nmodel = sm.OLS(Y, X, missing = 'drop')\n\n# Fit the model\nmodel_result = model.fit()\n\n# Summary of the linear model\nmodel_result.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\npLWP\nR-squared (uncentered):\n0.934\n\n\nModel:\nOLS\nAdj. R-squared (uncentered):\n0.934\n\n\nMethod:\nLeast Squares\nF-statistic:\n3882.\n\n\nDate:\nFri, 15 Sep 2023\nProb (F-statistic):\n3.29e-164\n\n\nTime:\n00:32:12\nLog-Likelihood:\n102.11\n\n\nNo. Observations:\n276\nAIC:\n-202.2\n\n\nDf Residuals:\n275\nBIC:\n-198.6\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nmLWP\n0.5995\n0.010\n62.309\n0.000\n0.581\n0.618\n\n\n\n\n\n\nOmnibus:\n6.825\nDurbin-Watson:\n1.348\n\n\nProb(Omnibus):\n0.033\nJarque-Bera (JB):\n8.247\n\n\nSkew:\n-0.219\nProb(JB):\n0.0162\n\n\nKurtosis:\n3.725\nCond. No.\n1.00\n\n\n\nNotes:[1] R² is computed without centering (uncentered) since the model does not contain a constant.[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n# Plotting the linear relationship\n\n# Increase font and figure size of all seaborn plot elements\nsns.set(font_scale = 1.25, rc = {'figure.figsize':(8, 8)})\n\n# Change seaborn plot theme to white\nsns.set_style(\"white\")\n\n# Subplots\nfig, (ax1, ax2) = plt.subplots(ncols = 2, nrows = 1)\n\n# Regression plot between mLWP and pLWP\nsns.regplot(data = data, x = \"mLWP\", y = \"pLWP\", ax = ax1)\n\n# Set regression plot title\nax1.set_title(\"Linear regression\")\n\n# Regression plot between mLWP and pLWP\nsns.residplot(data = data, x = \"mLWP\",\n              y = \"pLWP\", ax = ax2)\n              \n# Set residual plot title\nax2.set_title(\"Residuals\")\n\n# Tight margins\nplt.tight_layout()\n\n# Show plot\nplt.show()\n\n\n\n\n\n\n\nNumerical Target Variables: Categorical Variable of Interest\nFormula: pLWP (numerical) ~ Group (categorical)\n\nmodel = ols('pLWP ~ C(Group)', data = data).fit()\n\nsm.stats.anova_lm(model, typ = 2)\n\n             sum_sq     df          F        PR(&gt;F)\nC(Group)   2.775293    3.0  22.045408  8.267447e-13\nResidual  11.414011  272.0        NaN           NaN\n\n\n\n# Increase font and figure size of all seaborn plot elements\nsns.set(font_scale = 1.25, rc = {'figure.figsize':(4, 4)})\n\n# Change seaborn plot theme to white\nsns.set_style(\"white\")\n\n# Box plot\nGroup_Box = sns.boxplot(data = data, x = \"pLWP\", y = \"Group\", width = 0.3)\n\n# Tweak the visual presentation\nGroup_Box.set(ylabel = \"Group\")\n\n# Tight margins\nplt.tight_layout()\n\n# Show plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCategorical Target Variables: Numerical Variable of Interest\nFormula: Group (categorical) ~ pLWP (numerical)\n\n# Grouped describe by one column, stacked \nGroups = data.groupby('Group').describe().unstack(1)\n\n# Print all rows\nprint(Groups.to_string())\n\n                   Group              \nDate        count  Drought-sens-canopy                    147\n                   Drought-sens-under                     147\n                   Drought-tol-canopy                     147\n                   Drought-tol-under                      147\n            mean   Drought-sens-canopy    2019-12-16 00:00:00\n                   Drought-sens-under     2019-12-16 00:00:00\n                   Drought-tol-canopy     2019-12-16 00:00:00\n                   Drought-tol-under      2019-12-16 00:00:00\n            min    Drought-sens-canopy    2019-10-04 00:00:00\n                   Drought-sens-under     2019-10-04 00:00:00\n                   Drought-tol-canopy     2019-10-04 00:00:00\n                   Drought-tol-under      2019-10-04 00:00:00\n            25%    Drought-sens-canopy    2019-11-09 12:00:00\n                   Drought-sens-under     2019-11-09 12:00:00\n                   Drought-tol-canopy     2019-11-09 12:00:00\n                   Drought-tol-under      2019-11-09 12:00:00\n            50%    Drought-sens-canopy    2019-12-16 00:00:00\n                   Drought-sens-under     2019-12-16 00:00:00\n                   Drought-tol-canopy     2019-12-16 00:00:00\n                   Drought-tol-under      2019-12-16 00:00:00\n            75%    Drought-sens-canopy    2020-01-21 12:00:00\n                   Drought-sens-under     2020-01-21 12:00:00\n                   Drought-tol-canopy     2020-01-21 12:00:00\n                   Drought-tol-under      2020-01-21 12:00:00\n            max    Drought-sens-canopy    2020-02-27 00:00:00\n                   Drought-sens-under     2020-02-27 00:00:00\n                   Drought-tol-canopy     2020-02-27 00:00:00\n                   Drought-tol-under      2020-02-27 00:00:00\n            std    Drought-sens-canopy                    NaN\n                   Drought-sens-under                     NaN\n                   Drought-tol-canopy                     NaN\n                   Drought-tol-under                      NaN\nSap_Flow    count  Drought-sens-canopy                  120.0\n                   Drought-sens-under                   120.0\n                   Drought-tol-canopy                   120.0\n                   Drought-tol-under                    120.0\n            mean   Drought-sens-canopy              85.269653\n                   Drought-sens-under                1.448825\n                   Drought-tol-canopy                9.074309\n                   Drought-tol-under                 4.573516\n            min    Drought-sens-canopy               33.37045\n                   Drought-sens-under                 0.17263\n                   Drought-tol-canopy                 5.90461\n                   Drought-tol-under                  2.17178\n            25%    Drought-sens-canopy              53.975162\n                   Drought-sens-under                0.534165\n                   Drought-tol-canopy                 8.11941\n                   Drought-tol-under                 4.053346\n            50%    Drought-sens-canopy              76.717782\n                   Drought-sens-under                1.665492\n                   Drought-tol-canopy                9.286552\n                   Drought-tol-under                 4.944842\n            75%    Drought-sens-canopy              94.068107\n                   Drought-sens-under                2.194299\n                   Drought-tol-canopy               10.404117\n                   Drought-tol-under                 5.139685\n            max    Drought-sens-canopy             184.040975\n                   Drought-sens-under                2.475989\n                   Drought-tol-canopy               10.705455\n                   Drought-tol-under                 5.726712\n            std    Drought-sens-canopy              41.313962\n                   Drought-sens-under                0.803858\n                   Drought-tol-canopy                 1.39567\n                   Drought-tol-under                  0.90243\nTWaterFlux  count  Drought-sens-canopy                  147.0\n                   Drought-sens-under                   147.0\n                   Drought-tol-canopy                   147.0\n                   Drought-tol-under                    147.0\n            mean   Drought-sens-canopy              40.404061\n                   Drought-sens-under                 0.75177\n                   Drought-tol-canopy                4.357234\n                   Drought-tol-under                 2.189824\n            min    Drought-sens-canopy              12.377738\n                   Drought-sens-under                0.101381\n                   Drought-tol-canopy                2.036843\n                   Drought-tol-under                 0.953906\n            25%    Drought-sens-canopy              25.220908\n                   Drought-sens-under                 0.27419\n                   Drought-tol-canopy                3.601341\n                   Drought-tol-under                 1.735003\n            50%    Drought-sens-canopy              38.630891\n                   Drought-sens-under                0.824875\n                   Drought-tol-canopy                4.460778\n                   Drought-tol-under                 2.198131\n            75%    Drought-sens-canopy              50.096197\n                   Drought-sens-under                 1.11289\n                   Drought-tol-canopy                5.112844\n                   Drought-tol-under                 2.686605\n            max    Drought-sens-canopy              96.012719\n                   Drought-sens-under                1.801823\n                   Drought-tol-canopy                 5.97689\n                   Drought-tol-under                 3.654336\n            std    Drought-sens-canopy              19.027997\n                   Drought-sens-under                0.429073\n                   Drought-tol-canopy                0.940353\n                   Drought-tol-under                 0.597511\npLWP        count  Drought-sens-canopy                   69.0\n                   Drought-sens-under                    69.0\n                   Drought-tol-canopy                    69.0\n                   Drought-tol-under                     69.0\n            mean   Drought-sens-canopy              -0.669932\n                   Drought-sens-under               -0.696138\n                   Drought-tol-canopy               -0.629909\n                   Drought-tol-under                -0.440243\n            min    Drought-sens-canopy              -1.299263\n                   Drought-sens-under               -1.433333\n                   Drought-tol-canopy               -0.863656\n                   Drought-tol-under                -0.746667\n            25%    Drought-sens-canopy              -0.790573\n                   Drought-sens-under                    -0.8\n                   Drought-tol-canopy               -0.706479\n                   Drought-tol-under                -0.520487\n            50%    Drought-sens-canopy              -0.705942\n                   Drought-sens-under               -0.592118\n                   Drought-tol-canopy               -0.602841\n                   Drought-tol-under                -0.406439\n            75%    Drought-sens-canopy               -0.47329\n                   Drought-sens-under               -0.521217\n                   Drought-tol-canopy               -0.571356\n                   Drought-tol-under                -0.360789\n            max    Drought-sens-canopy              -0.263378\n                   Drought-sens-under               -0.299669\n                   Drought-tol-canopy               -0.437556\n                   Drought-tol-under                -0.205224\n            std    Drought-sens-canopy                0.24639\n                   Drought-sens-under                0.283935\n                   Drought-tol-canopy                0.095571\n                   Drought-tol-under                 0.131879\nmLWP        count  Drought-sens-canopy                   77.0\n                   Drought-sens-under                    77.0\n                   Drought-tol-canopy                    77.0\n                   Drought-tol-under                     77.0\n            mean   Drought-sens-canopy              -1.319148\n                   Drought-sens-under               -1.097537\n                   Drought-tol-canopy               -0.892554\n                   Drought-tol-under                -0.809572\n            min    Drought-sens-canopy              -1.812151\n                   Drought-sens-under               -1.808333\n                   Drought-tol-canopy               -1.073619\n                   Drought-tol-under                -1.168716\n            25%    Drought-sens-canopy              -1.525563\n                   Drought-sens-under               -1.335521\n                   Drought-tol-canopy               -0.945841\n                   Drought-tol-under                -0.907041\n            50%    Drought-sens-canopy              -1.354771\n                   Drought-sens-under               -1.054159\n                   Drought-tol-canopy               -0.890061\n                   Drought-tol-under                -0.735647\n            75%    Drought-sens-canopy              -1.111942\n                   Drought-sens-under               -0.907564\n                   Drought-tol-canopy               -0.828777\n                   Drought-tol-under                -0.699087\n            max    Drought-sens-canopy              -0.679769\n                   Drought-sens-under               -0.546152\n                   Drought-tol-canopy               -0.707789\n                   Drought-tol-under                -0.545165\n            std    Drought-sens-canopy               0.298107\n                   Drought-sens-under                0.263522\n                   Drought-tol-canopy                0.091729\n                   Drought-tol-under                 0.170603\n\n\n\n\n\nCategorical Target Variables: Categorical Variable of Interest\nNotably, there is only one categorical variable… Let’s make another:\nIf \\(mLWP &gt; mean(mLWP) + sd(mLWP)\\) then Yes, else No.\n\ndata1 = data.dropna()\nQual = stat.mean(data1.pLWP + stat.stdev(data1.pLWP))\n\n# Create HighLWP from the age column\ndef HighLWP_data(data): \n  if data.pLWP &gt;= Qual: return \"Yes\"\n  else: return \"No\"\n\n# Apply the function to data and create a dataframe\nHighLWP = pd.DataFrame(data1.apply(HighLWP_data, axis = 1))\n\n# Name new column\nHighLWP.columns = ['HighLWP']\n\n# Concatenate the two dataframes\ndata1 = pd.concat([data1, HighLWP], axis = 1)\n\n# First six rows of new dataset\ndata1.head()\n\n        Date                Group    Sap_Flow  ...      pLWP      mLWP  HighLWP\n0 2019-10-04  Drought-sens-canopy  184.040975  ... -0.263378 -0.679769      Yes\n1 2019-10-04   Drought-sens-under    2.475989  ... -0.299669 -0.761326      Yes\n2 2019-10-04   Drought-tol-canopy   10.598949  ... -0.437556 -0.722557       No\n3 2019-10-04    Drought-tol-under    4.399854  ... -0.205224 -0.702858      Yes\n4 2019-10-05  Drought-sens-canopy  182.905444  ... -0.276928 -0.708261      Yes\n\n[5 rows x 7 columns]\n\n\nNow we have two categories!\nFormula = Group (categorical) ~ HighLWP (categorical)\n\nobs = pd.crosstab(data1.Group, data1.HighLWP)\nprint(obs)\n\nHighLWP              No  Yes\nGroup                       \nDrought-sens-canopy  58   11\nDrought-sens-under   65    4\nDrought-tol-canopy   69    0\nDrought-tol-under    49   20\n\n# Chi-square test\nchi2, p, dof, ex = chi2_contingency(obs, correction = False)\n  \n# Interpret\nalpha = 0.05\n  \n# Print the interpretation\nprint('Statistic = %.3f, p = %.3f' % (chi2, p))\n\nStatistic = 30.201, p = 0.000\n\nif p &gt; alpha:\n  print('Chi-square value is not greater than critical value (fail to reject H0)')\nelse:\n    print('Chi-square value is greater than critical value (reject H0)')\n\nChi-square value is greater than critical value (reject H0)\n\n\n\n# Increase font and figure size of all seaborn plot elements\nsns.set(font_scale = 1.25)\n\n# Change seaborn plot theme to white\nsns.set_style(\"white\")\n\n# Count plot of HighLWP grouped by Group \ncounts = sns.countplot(data = data1, x = \"HighLWP\", hue = \"Group\")\n\n# Tweak the visual presentation\ncounts.set(ylabel = \"Count\")\n\n# Tight margins\nplt.tight_layout()\n\n# Show plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nMeredith, Laura, S. Nemiah Ladd, and Christiane Werner. 2021. “Data for \"Ecosystem Fluxes During Drought and Recovery in an Experimental Forest\".” University of Arizona Research Data Repository. https://doi.org/10.25422/AZU.DATA.14632593.V1."
  }
]
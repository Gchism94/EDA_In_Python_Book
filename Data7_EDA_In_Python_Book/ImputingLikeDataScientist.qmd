# Exploratory Data Analysis in Python - Imputing like a Data Scientist {.unnumbered}

```{r setup, include = FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 

# Sets the repository to download packages from
options(repos = list(CRAN = "http://cran.rstudio.com/"))

install.packages("reticulate")

library(reticulate)

py_install("pandas")

py_install("seaborn")

py_install("scikit-learn")

py_install("missingno")

py_install("statsmodels")
```

## Purpose of Workshop

**Exploring, visualizing, and imputing outliers and missing values (NAs) in a novel data set**

------------------------------------------------------------------------

## Objectives

1.  Load and explore a data set with publication quality tables
2.  Thoroughly diagnose outliers and missing values
3.  Impute outliers and missing values

------------------------------------------------------------------------

## Required Setup

We first need to prepare our environment with the necessary libraries and set a global theme for publishable plots in `seaborn`.

```{python, echo = TRUE}
# Import all required libraries
# Data analysis and manipulation
import pandas as pd
# Working with arrays
import numpy as np
# Statistical visualization
import seaborn as sns
# Matlab plotting for Python
import matplotlib.pyplot as plt
# Data analysis
import statistics as stat
import scipy.stats as stats
# Visualizing missing values
import missingno as msno
# Statistical modeling
import statsmodels.api as smx
# Predictive data analysis: process data 
from sklearn import preprocessing as pproc
# Predictive data analysis: outlier imputation
from sklearn.impute import SimpleImputer
# Predictive data analysis: KNN NA imputation
from sklearn.impute import KNNImputer
# Predictive data analysis: experimental iterative NA imputer (MICE)
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
# Predictive data analysis: linear models
from sklearn.linear_model import LinearRegression
# Predictive data analysis: Classifying nearest neighbors
from sklearn import neighbors
# Predictive data analysis: Plotting decision regions
from mlxtend.plotting import plot_decision_regions


# Increase font size of all seaborn plot elements
sns.set(font_scale = 1.5, rc = {'figure.figsize':(8, 8)})

# Change theme to "white"
sns.set_style("white")
```

------------------------------------------------------------------------

## Load and Examine a Data Set

```{python}
# Read csv 
data = pd.read_csv("data/diabetes.csv")

# Create Age_group from the age column
def Age_group_data(data): 
  if data.Age >= 21 and data.Age <= 30: return "Young"
  elif data.Age > 30 and data.Age <= 50: return "Middle" 
  else: return "Elderly"

# Apply the function to data
data['Age_group'] = data.apply(Age_group_data, axis = 1)

# What does the data look like
data.head()
```

------------------------------------------------------------------------

## Diagnose your Data

```{python}
# What are the properties of the data
diagnose = data.info()
```

-   `Column`: name of each variable
-   `Non-Null Count`: number of missing values
-   `DType`: data type of each variable

------------------------------------------------------------------------

## Diagnose Outliers

There are several numerical variables that have outliers above, let's see what the data look like with and without them

-   Create a table with columns containing outliers

-   Plot outliers in a box plot and histogram

```{python}
# Make a copy of the data 
dataCopy = data.copy()

# Select only numerical columns
dataRed = dataCopy.select_dtypes(include = np.number)

# List of numerical columns
dataRedColsList = dataRed.columns[...]

# For all values in the numerical column list from above
for i_col in dataRedColsList:
  # List of the values in i_col
  dataRed_i = dataRed.loc[:,i_col]
  
  # Define the 25th and 75th percentiles
  q25, q75 = round((dataRed_i.quantile(q = 0.25)), 3), round((dataRed_i.quantile(q = 0.75)), 3)
  
  # Define the interquartile range from the 25th and 75th percentiles defined above
  IQR = round((q75 - q25), 3)
  
  # Calculate the outlier cutoff 
  cut_off = IQR * 1.5
  
  # Define lower and upper cut-offs
  lower, upper = round((q25 - cut_off), 3), round((q75 + cut_off), 3)
  
  # Print the values
  print(' ')
  
  # For each value of i_col, print the 25th and 75th percentiles and IQR
  print(i_col, 'q25=', q25, 'q75=', q75, 'IQR=', IQR)
  
  # Print the lower and upper cut-offs
  print('lower, upper:', lower, upper)

  # Count the number of outliers outside the (lower, upper) limits, print that value
  print('Number of Outliers: ', dataRed_i[(dataRed_i < lower) | (dataRed_i > upper)].count())
```

-   `q25`: 1/4 quartile, 25th percentile

-   `q75`: 3/4 quartile, 75th percentile

-   `IQR`: interquartile range (q75-q25)

-   `lower`: lower limit of $1.5*IQR$ used to calculate outliers

-   

    ## `upper`: upper limit of $1.5*IQR$ used to calculate outliers

## Basic Exploration of Missing Values (NAs)

-   Table showing the extent of NAs in columns containing them

```{python}
dataNA = data

for col in dataNA.columns:
    dataNA.loc[dataNA.sample(frac = 0.1).index, col] = np.nan
    
dataNA.isnull().sum()
```

Bar plot showing all NA values in each column. Since we randomly produced a set amount above the numbers will all be the same.

```{python}
msno.bar(dataNA, figsize = (8, 8), fontsize = 10)
plt.tight_layout()
```

------------------------------------------------------------------------

## Advanced Exploration of Missing Values (NAs)

This matrix shows the number of missing values throughout each column.

-   X-axis is the column names

-   Left Y-axis is the row number

-   Right Y-axis is a line plot that shows each row's completeness, e.g., if there are 11 columns, 4-10 valid values means that there are 1-7 missing values in a row.

```{python}
#| fig-align: center
dataNA1 = dataNA.drop('DiabetesPedigreeFunction', axis = "columns")

# NA matric
msno.matrix(dataNA1, figsize = (8, 8), fontsize = 10)
```

------------------------------------------------------------------------

## Impute Outliers and NAs

Removing outliers and NAs can be tricky, but there are methods to do so. I will go over several, and discuss benefits and costs to each.

The principle goal for all imputation is to find the method that does not change the distribution too much (or oddly).

------------------------------------------------------------------------

**NOTE: imputation should only be used when missing data is unavoidable and probably limited to 10% of your data being outliers / missing data (though some argue imputation is necessary between 30-60%). Ask what the cause is for the outlier and missing data.**

------------------------------------------------------------------------

### Classifying Outliers

Before imputing outliers, you will want to diagnose whether it's they are natural outliers or not. We will be looking at "Insulin" for example across Age_group, because there are several outliers and NAs, which we will impute below.

```{python}
#| fig-align: right

# Increase font size of all seaborn plot elements
sns.set(font_scale = 1.25, rc = {'figure.figsize':(8, 8)})

# Change theme to "white"
sns.set_style("white")

# Box plot
Age_Box = sns.boxplot(data = data, x = "Insulin", y = "Age_group", width = .3)

# Tweak the visual presentation
Age_Box.set(ylabel = "Age group")
```

Now let's say that we want to impute extreme values and remove outliers that don't make sense, such as Insulin levels \> 600 mg/dL: values greater than this induce a diabetic coma.

We remove outliers using `SimpleImputer` from `sklearn` and replace them with values that are estimates based on the existing data

-   mean: arithmetic mean

-   median: median

-   mode: mode

-   capping: Impute the upper outliers with 95 percentile, and impute the bottom outliers with 5 percentile - aka Winsorizing

```{python}
# Select only Insulin
InsMod = data.filter(["Insulin"], axis = "columns")
```

------------------------------------------------------------------------

### Mean Imputation

The mean of the observed values for each variable is computed and the outliers for that variable are imputed by this mean

```{python}
# Python can't impute outliers easily, so we will convert them to NAs and imputate them
InsMod.loc[InsMod.Insulin > 600, 'Insulin'] = np.nan

# Set mean imputation algorithm
Mean_Impute = SimpleImputer(missing_values = np.nan, strategy = 'mean')

# Fit imputation
Mean_Impute = Mean_Impute.fit(InsMod[['Insulin']])

# Transform NAs with the mean imputation
InsMod['Ins_Mean'] = Mean_Impute.transform(InsMod[['Insulin']])
```

```{python}
# Visualization of the mean imputation
# Original data
mean_plot = sns.kdeplot(data = InsMod, x = 'Insulin', linewidth = 2, label = "Original")

# Mean imputation
mean_plot = sns.kdeplot(data = InsMod, x = 'Ins_Mean', linewidth = 2, label = "Mean Imputated")

# Show legend
plt.legend()

# Show plot
plt.show()
```

------------------------------------------------------------------------

### Median Imputation

The median of the observed values for each variable is computed and the outliers for that variable are imputed by this median

```{python}
# Python can't impute outliers easily, so we will convert them to NAs and imputate them
InsMod.loc[InsMod.Insulin > 600, 'Insulin'] = np.nan

# Set median imputation algorithm
Median_Impute = SimpleImputer(missing_values = np.nan, strategy = 'median')

# Fit imputation
Median_Impute = Median_Impute.fit(InsMod[['Insulin']])

# Transform NAs with the median imputation
InsMod['Ins_Median'] = Median_Impute.transform(InsMod[['Insulin']])
```

```{python}
# Visualization of the median imputation
# Original data
median_plot = sns.kdeplot(data = InsMod, x = 'Insulin', linewidth = 2, label = "Original")

# Median imputation
median_plot = sns.kdeplot(data = InsMod, x = 'Ins_Median', linewidth = 2, label = "Median Imputated")

# Show legend
plt.legend()

# Show plot
plt.show()
```

------------------------------------------------------------------------

#### Pros & Cons of Using the Mean or Median Imputation

**Pros**:

-   Easy and fast.
-   Works well with small numerical datasets.

**Cons**:

-   Doesn't factor the correlations between features. It only works on the column level.
-   Will give poor results on encoded categorical features (do **NOT** use it on categorical features).
-   Not very accurate.
-   Doesn't account for the uncertainty in the imputations.

------------------------------------------------------------------------

### Mode Imputation

The mode of the observed values for each variable is computed and the outliers for that variable are imputed by this mode

```{python}
# Python can't impute outliers easily, so we will convert them to NAs and imputate them
InsMod.loc[InsMod.Insulin > 600, 'Insulin'] = np.nan

# Set mode imputation algorithm
Mode_Impute = SimpleImputer(missing_values = np.nan, strategy = 'most_frequent')

# Fit imputation
Mode_Impute = Mode_Impute.fit(InsMod[['Insulin']])

# Transform NAs with the mode imputation
InsMod['Ins_Mode'] = Mode_Impute.transform(InsMod[['Insulin']])
```

```{python}
# Visualization of the mode imputation
# Original data
mode_plot = sns.kdeplot(data = InsMod, x = 'Insulin', linewidth = 2, label = "Original")

# Mode imputation
mode_plot = sns.kdeplot(data = InsMod, x = 'Ins_Mode', linewidth = 2, label = "Mode Imputated")

# Show legend
plt.legend()

# Show plot
plt.show()
```

------------------------------------------------------------------------

#### Pros & Cons of Using the Mode Imputation

**Pros**:

-   Works well with categorical features.

**Cons**:

-   It also doesn't factor the correlations between features.

-   It can introduce bias in the data.

------------------------------------------------------------------------

### Capping Imputation (aka Winsorizing)

The Percentile Capping is a method of Imputing the outlier values by replacing those observations outside the lower limit with the value of 5th percentile and those that lie above the upper limit, with the value of 95th percentile of the same dataset.

```{python}
# Winsorizing deals woth outliers, so we don't have to worry about changing to NAs

# Transform NAs with the median imputation
InsMod['Ins_Cap'] = stats.mstats.winsorize(InsMod['Insulin'], limits = 0.05)
```

```{python}
# Visualization of the capping imputation
# Original data
cap_plot = sns.kdeplot(data = InsMod, x = 'Insulin', linewidth = 2, label = "Original")

# Mode imputation
cap_plot = sns.kdeplot(data = InsMod, x = 'Ins_Cap', linewidth = 2, label = "Capping Imputated")

# Show legend
plt.legend()

# Show plot
plt.show()
```

------------------------------------------------------------------------

#### Pros and Cons of Capping

**Pros**:

-   Not influenced by extreme values

**Cons**:

-   Capping only modifies the smallest and largest values slightly. This is generally not a good idea since it means we're just modifying data values for the sake of modifications.

-   If no extreme outliers are present, Winsorization may be unnecessary.

## Imputing NAs

I will only be addressing a subset of methods for NA imputation, but you can use the mean, median, and mode methods from above as well:

1.  knn: K-nearest neighbors (KNN)
2.  mice: Multivariate Imputation by Chained Equations (MICE)

Since our normal `data` has no NA values, we will add the `Insulin` column from the `dataNA` we created earlier and replace the original with it.

```{python}
# Make a copy of the data 
dataCopy = data.copy()

# Select the Insulin 
InsNA = dataNA.filter(["Insulin"], axis = "columns")

# Add Insulin with NAs to copy of original data
dataCopy['Insulin'] = InsNA
```

------------------------------------------------------------------------

### K-Nearest Neighbor (KNN) Imputation

KNN is a machine learning algorithm that classifies data by similarity. This in effect clusters data into similar groups. The algorithm predicts values of new data to replace NA values based on how closely they resembles training data points, such as by comparing across other columns.

Here's a visual example using the `plot_decision_regions` function from `mlxtend.plotting` library to run a KNN algorithm on our `dataset`, where three clusters are created by the algorithm.

```{python}
# KNN plot function
def knn_comparision(data, k):
  # Define x and y values (your data will need to have these)
    X = data[['x1','x2']].values
    y = data['y'].astype(int).values
    # Knn function, defining the number of neighbors
    clf = neighbors.KNeighborsClassifier(n_neighbors = k)
    # Fit knn algorithm to data
    clf.fit(X, y)

    # Plotting decision regions
    plot_decision_regions(X, y, clf = clf, legend = 2)

    # Adding axes annotations
    plt.xlabel('X1')
    plt.ylabel('X2')
    plt.title('Knn with K='+ str(k))
    plt.legend(loc = 'upper right')
    plt.tight_layout()
    plt.show()
```

```{python}
# Prepare data for the KNN plotting function
data1 = data.loc[:, ['Insulin', 'Glucose', 'Outcome']]

# Drop NAs
data1 = data1.dropna()

# Set the two target x variables and the binary y variable we are clustering the data from
data1 = data1.rename(columns = {'Insulin': 'x1', 'Glucose': 'x2', 'Outcome': 'y'})

# Create KNN plot for 3 nearest neighbors
knn_comparision(data1, 3)
```

You can also loop the KNN plots for i nearest neighbors:

```{python, eval = FALSE}
# Loop to create KNN plots for i number of nearest neighbors
for i in [1, 5, 15]:
  knn_comparision(data1, i)
```

**Note, we have to change a three things to make `KNNImputer` work correctly**:

1.  We need to change any characters, into dummy variables that are numericals, because scalars and imputers do not recognize characters. In this case, `Age_group` is an ordinal category, so we will use `OrdinalEncoder` from `Scikit-learn`, specifically in `preprocessing` which we imported as `pproc`.

```{python}
# Numeric dummy variable from our Age_group ordinal column
# Define the orginal encoder 
enc = pproc.OrdinalEncoder()

# Ordinal variable from Age_group column 
dataCopy[['Age_group']] = enc.fit_transform(dataCopy[['Age_group']])
```

2.  We need to reorder our target column with NAs to the end of the dataframe so that the rest of the dataframe can be called as training data more easily.

```{python}
# Reorder columns
dataCopy = dataCopy[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', "BMI", "DiabetesPedigreeFunction", "Age", "Outcome", "Age_group", "Insulin"]]
```

3.  `KNNImputater` is distance-based so we need to normalize our data. Otherwise `KNNImputer` will create biased replacement. We will use the `pproc.MinMaxScaler` from `Scikit-learn`, which scales our values from 0-1.

```{python}
# Min-max schaler
scaler = pproc.MinMaxScaler()

# Scale columns
dataCopy_Scale = pd.DataFrame(scaler.fit_transform(dataCopy), columns = dataCopy.columns)
```

We are finally ready to for KNN Imputation!

```{python}
# Set KNN imputation function parameters
imputer = KNNImputer(n_neighbors = 3)

# Fit imputation
DataKnn = pd.DataFrame(imputer.fit_transform(dataCopy_Scale),columns = dataCopy_Scale.columns)
```

```{python}
# Add KNN imputated column to original dataCopy
dataCopy_Scale[['InsKnn']] = DataKnn[['Insulin']]

# Visualization of the KNN imputation
# Original data
knn_plot = sns.kdeplot(data = dataCopy_Scale, x = 'Insulin', linewidth = 2, label = "Original")

# KNN imputation
knn_plot = sns.kdeplot(data = dataCopy_Scale, x = 'InsKnn', linewidth = 2, label = "KNN Imputated")

# Show legend
plt.legend()

# Show plot
plt.show()
```

------------------------------------------------------------------------

#### Pros & Cons of Using KNN Imputation

**Pro**:

-   Possibly much more accurate than mean, median, or mode imputation for some data sets.

**Cons**:

-   KNN is computationally expensive because it stores the entire training dataset into computer memory.

-   KNN is very sensitive to outliers, so you would have to imputate these first.

------------------------------------------------------------------------

### Multivariate Imputation by Chained Equations (MICE)

MICE is an algorithm that fills missing values multiple times, hence dealing with uncertainty better than other methods. This approach creates multiple copies of the data that can then be analyzed and then pooled into a single dataset.

![Image Credit: [Will Badr](https://towardsdatascience.com/6-different-ways-to-compensate-for-missing-values-data-imputation-with-examples-6022d9ca0779)](https://miro.medium.com/max/1400/1*cmZFWypJUrFL2QL3KyzXEQ.png)

```{python, output = FALSE}
# Assign a regression model
lm = LinearRegression()

# Set MICE imputation function parameters
imputer = IterativeImputer(estimator = lm, missing_values = np.nan, max_iter = 10, verbose = 2, imputation_order = 'roman', random_state = 0)

# Fit imputation
dataMice = pd.DataFrame(imputer.fit_transform(dataCopy),columns = dataCopy.columns)
```

```{python}
# Add MICE imputated column to original dataCopy
dataCopy[['InsMice']] = dataMice[['Insulin']]

# Visualization of the MICE imputation
# Original data
knn_plot = sns.kdeplot(data = dataCopy, x = 'Insulin', linewidth = 2, label = "Original")

# KNN imputation
knn_plot = sns.kdeplot(data = dataCopy, x = 'InsMice', linewidth = 2, label = "MICE Imputated")

# Show legend
plt.legend()

# Show plot
plt.show()
```

------------------------------------------------------------------------

#### Pros & Cons of MICE Imputation

**Pros**:

-   Multiple imputations are more accurate than a single imputation.

-   The chained equations are very flexible to data types, such as categorical and ordinal.

**Cons**:

-   You have to round the results for ordinal data because resulting data points are too great or too small (floating-points).

------------------------------------------------------------------------

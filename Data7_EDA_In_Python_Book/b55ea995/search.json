[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data 7 Exploratory Data Analysis In Python Book",
    "section": "",
    "text": "Here you will find a collection of workshops prepared by the staff of the Data Science Institute\n\n\n\n\n\nExploring a novel data set and produce an HTML interactive reports\n\n\n\n\n\n\n\nExploring the normality of numerical columns in a novel data set\n\n\n\n\n\n\n\nUsing data transformation to correct non-normality in numerical data\n\n\n\n\n\n\n\nExploring, visualizing, and imputing outliers and missing values (NAs) in a novel data set\n\n\n\n\n\n\n\nAssess relationships within a novel data set\n\nVisit our available Digital Learning Resources Library!\n\nCreated: 09/14/2022 (G. Chism); Last update: 09/14/2022\n CC BY-NC-SA"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Exploratory data analysis is an essential first step towards determining the validity of your data and should be performed throughout the data pipeline. However, EDA is often performed too late or not at all. The Python programming language, is a widely used open source platform for data analysis and data visualization. This is because of the variety of libraries available and attentive community devoted to data analysis.\nHere, we utilize the pandas and pandas-profiling libraries to conduct preliminary exploratory data analysis aimed at diagnosing any major issues with an imported data set. pandas and pandas-profiling offers a clean and straightforward methodology to uncover issues such as data outliers, missing data, as well as summary statistical reports."
  },
  {
    "objectID": "intro.html#what-are-some-important-data-set-characteristics",
    "href": "intro.html#what-are-some-important-data-set-characteristics",
    "title": "Introduction",
    "section": "What are Some Important Data Set Characteristics?",
    "text": "What are Some Important Data Set Characteristics?\nThere are several characteristics that are arguably important, but we will only consider those covered in this workshop series. Let’s start with the fundamentals that will help guide us."
  },
  {
    "objectID": "intro.html#diagnostics",
    "href": "intro.html#diagnostics",
    "title": "Introduction",
    "section": "Diagnostics",
    "text": "Diagnostics\nWhen importing data sets, it is important to consider characteristics about the data columns, rows, and individual cells.\n\n\nVariables\nName of each variable\n\n\n   Pregnancies  Glucose  BloodPressure  ...  Age  Outcome  Age_group\n0            6      148             72  ...   50        1     Middle\n1            1       85             66  ...   31        0     Middle\n2            8      183             64  ...   32        1     Middle\n3            1       89             66  ...   21        0      Young\n4            0      137             40  ...   33        1     Middle\n\n[5 rows x 10 columns]\n\n\n\n\nTypes\nData type of each variable\n\n\nPregnancies                   int64\nGlucose                       int64\nBloodPressure                 int64\nSkinThickness                 int64\nInsulin                       int64\nBMI                         float64\nDiabetesPedigreeFunction    float64\nAge                           int64\nOutcome                       int64\nAge_group                    object\ndtype: object\n\n\n\nNumerical: Continuous\nMeasurable numbers that are fractional or decimal and cannot be counted (e.g., time, height, weight)\n\n\n\n\n\n\n\nNumerical: Discrete\nCountable whole numbers or integers (e.g., number of successes or failures)\n\n\n\n\n\n\n\n\nCategorical: Nominal\nLabeling variables without any order or quantitative value (e.g., hair color, nationality)\n\n\n\n\n\n\n\nCategorical: Ordinal\nWhere there is a hierarchical order along a scale (e.g., ranks, letter grades, age groups)\n\n\n\n\n\n\n\n\nMissing Values (NAs)\nCells, rows, or columns without data\n\nMissing percent: percentage of missing values * Unique count: number of unique values.\nUnique rate: rate of unique value - unique count / total number of observations.\n\n\n\n   Pregnancies  Glucose  BloodPressure  ...  Outcome  Age_group  Outcome1\n0          6.0      NaN           72.0  ...      1.0     Middle       Yes\n1          1.0     85.0           66.0  ...      0.0     Middle       NaN\n2          8.0      NaN           64.0  ...      1.0     Middle       Yes\n3          1.0     89.0           66.0  ...      0.0      Young        No\n4          0.0    137.0           40.0  ...      1.0     Middle       Yes\n\n[5 rows x 11 columns]"
  },
  {
    "objectID": "intro.html#summary-statistics",
    "href": "intro.html#summary-statistics",
    "title": "Introduction",
    "section": "Summary Statistics",
    "text": "Summary Statistics\nAbove we described some properties of data. However, you will need to know some descriptive characteristics of your data before you can move forward. Enter, summary statistics.\nSummary statistics allow you to summarize large amounts of information about your data as quickly as possible.\n\nCentral Tendency\nMeasuring a central property of your data. Some examples you’ve probably heard of are:\n\nMean: Average value\nMedian: Middle value\nMode: Most common value\n\n\n\n\n\n\nNotice that all values of central tendency can be pretty similar in this figure.\n\n\n\n\n\nHowever, in this figure, all measures are different. This will be important when we discuss statistical dispersion in chapter 3.\n\n\n\n\n\nStatistical Dispersion\nMeasure of data variability, scatter, or spread. Some examples you may have heard of:\n\nStandard deviation (SD): The amount of variation that occurs in a set of values.\nInterquartile range (IQR): The difference between the 75th and 25th percentiles\nOutliers: A value outside of \\(1.5 * IQR\\)\n\n\n\n\n\n\n\n\nDistribution Shape\nMeasures of describing the shape of a distribution, usually compared to a normal distribution (bell-curve)\n\nSkewness: The symmetry of the distribution\nKurtosis: The tailedness of the distribution\n\n\n\n\n\n\n\n\nStatistical Dependence (Correlation)\nMeasure of causality between two random variables (statistically). Notably, we approximate causality with correlations (see correlation \\(\\neq\\) causation)\n\nNumerical values, but you can compare numericals across categories (see the first plot above)."
  },
  {
    "objectID": "DiagnosingLikeDataDoctor.html",
    "href": "DiagnosingLikeDataDoctor.html",
    "title": "Exploratory Data Analysis in Python - Diagnosing like a Data Doctor",
    "section": "",
    "text": "Exploring a novel data set and produce an HTML interactive reports"
  },
  {
    "objectID": "DiagnosingLikeDataDoctor.html#objectives",
    "href": "DiagnosingLikeDataDoctor.html#objectives",
    "title": "Exploratory Data Analysis in Python - Diagnosing like a Data Doctor",
    "section": "Objectives",
    "text": "Objectives\n\nLoad and explore a data set with publication quality tables\nDiagnose outliers and missing values in a data set\nPrepare an HTML summary report showcasing properties of a data set"
  },
  {
    "objectID": "DiagnosingLikeDataDoctor.html#required-setup",
    "href": "DiagnosingLikeDataDoctor.html#required-setup",
    "title": "Exploratory Data Analysis in Python - Diagnosing like a Data Doctor",
    "section": "Required Setup",
    "text": "Required Setup\nWe first need to prepare our environment with the necessary libraries and set a global theme for publishable plots in seaborn.\n\n# Import all required libraries\n# Data analysis and manipulation\nimport pandas as pd\n# Working with arrays\nimport numpy as np\n# Statistical visualization\nimport seaborn as sns\n# Matlab plotting for Python\nimport matplotlib.pyplot as plt\n# Data analysis\nimport statistics as stat\n# Predictive data analysis: process data \nfrom sklearn import preprocessing as pproc\nimport scipy.stats as stats\n# Visualizing missing values\nimport missingno as msno\n# Interactive HTML EDA report\nfrom pandas_profiling import ProfileReport\n\n# Increase font size of all Seaborn plot elements\nsns.set(font_scale = 1.25)"
  },
  {
    "objectID": "DiagnosingLikeDataDoctor.html#load-and-examine-a-data-set",
    "href": "DiagnosingLikeDataDoctor.html#load-and-examine-a-data-set",
    "title": "Exploratory Data Analysis in Python - Diagnosing like a Data Doctor",
    "section": "Load and Examine a Data Set",
    "text": "Load and Examine a Data Set\n\nLoad data and view\nExamine columns and data types\nDefine box plots\nDescribe meta data\n\nWe will be using open source data from UArizona researchers for Test, Trace, Treat (T3) efforts offers two clinical diagnostic tests (Antigen, RT-PCR) to determine whether an individual is currently infected with the COVID-19 virus. (Merchant et al. 2022)\n\n# Read csv \ndata = pd.read_csv(\"data/daily_summary.csv\")\n\n# What does the data look like\ndata.head()\n\n  result_date      affil_category  ... test_count          test_source\n0  2020-08-04            Employee  ...          5        Campus Health\n1  2020-08-04            Employee  ...          0        Campus Health\n2  2020-08-04            Employee  ...          1  Test All Test Smart\n3  2020-08-04            Employee  ...          0  Test All Test Smart\n4  2020-08-04  Off-Campus Student  ...          9        Campus Health\n\n[5 rows x 6 columns]"
  },
  {
    "objectID": "DiagnosingLikeDataDoctor.html#diagnose-your-data",
    "href": "DiagnosingLikeDataDoctor.html#diagnose-your-data",
    "title": "Exploratory Data Analysis in Python - Diagnosing like a Data Doctor",
    "section": "Diagnose your Data",
    "text": "Diagnose your Data\n\n# What are the properties of the data\ndiagnose = data.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 9180 entries, 0 to 9179\nData columns (total 6 columns):\n #   Column          Non-Null Count  Dtype \n---  ------          --------------  ----- \n 0   result_date     9180 non-null   object\n 1   affil_category  9180 non-null   object\n 2   test_type       9180 non-null   object\n 3   test_result     9180 non-null   object\n 4   test_count      9180 non-null   int64 \n 5   test_source     9180 non-null   object\ndtypes: int64(1), object(5)\nmemory usage: 430.4+ KB\n\n\n\nColumn: name of each variable\nNon-Null Count: number of missing values\nDType: data type of each variable"
  },
  {
    "objectID": "DiagnosingLikeDataDoctor.html#summary-statistics-of-your-data",
    "href": "DiagnosingLikeDataDoctor.html#summary-statistics-of-your-data",
    "title": "Exploratory Data Analysis in Python - Diagnosing like a Data Doctor",
    "section": "Summary Statistics of your Data",
    "text": "Summary Statistics of your Data\n\nNumerical Variables\n\n# Summary statistics of our numerical columns\ndata.describe()\n\n        test_count\ncount  9180.000000\nmean     46.771024\nstd     129.475844\nmin       0.000000\n25%       0.000000\n50%       2.000000\n75%      16.000000\nmax    1472.000000\n\n\n\ncount: number of observations\nmean: arithmetic mean (average value)\nstd: standard deviation\nmin: minimum value\n25%: 1/4 quartile, 25th percentile\n50%: median, 50th percentile\n75%: 3/4 quartile, 75th percentile\nmax: maximum value\n\n\n\n\nOutliers\nValues outside of \\(1.5 * IQR\\)\n\n\n\nImage Credit: CÉDRIC SCHERER\n\n\n\nThere are several numerical variables that have outliers above, let’s see what the data look like with and without them\n\nCreate a table with columns containing outliers\nPlot outliers in a box plot and histogram\n\n\n# Make a copy of the data \ndataCopy = data.copy()\n\n# Select only numerical columns\ndataRed = dataCopy.select_dtypes(include = np.number)\n\n# List of numerical columns\ndataRedColsList = dataRed.columns[...]\n\n# For all values in the numerical column list from above\nfor i_col in dataRedColsList:\n  # List of the values in i_col\n  dataRed_i = dataRed.loc[:,i_col]\n  \n  # Define the 25th and 75th percentiles\n  q25, q75 = round((dataRed_i.quantile(q=0.25)), 3), round((dataRed_i.quantile(q=0.75)), 3)\n  \n  # Define the interquartile range from the 25th and 75th percentiles defined above\n  IQR = round((q75 - q25), 3)\n  \n  # Calculate the outlier cutoff \n  cut_off = IQR * 1.5\n  \n  # Define lower and upper cut-offs\n  lower, upper = round((q25 - cut_off), 3), round((q75 + cut_off), 3)\n  \n  # Print the values\n  print(' ')\n  \n  # For each value of i_col, print the 25th and 75th percentiles and IQR\n  print(i_col, 'q25=', q25, 'q75=', q75, 'IQR=', IQR)\n  \n  # Print the lower and upper cut-offs\n  print('lower, upper:', lower, upper)\n\n  # Count the number of outliers outside the (lower, upper) limits, print that value\n  print('Number of Outliers: ', dataRed_i[(dataRed_i < lower) | (dataRed_i > upper)].count())\n\n \ntest_count q25= 0.0 q75= 16.0 IQR= 16.0\nlower, upper: -24.0 40.0\nNumber of Outliers:  1721\n\n\n\nq25: 1/4 quartile, 25th percentile\nq75: 3/4 quartile, 75th percentile\nIQR: interquartile range (q75-q25)\nlower: lower limit of \\(1.5*IQR\\) used to calculate outliers\nupper: upper limit of \\(1.5*IQR\\) used to calculate outliers\n\n\n# Change theme to \"white\"\nsns.set_style(\"white\")\n\n# Select only numerical columns\ndataRedColsList = data.select_dtypes(include = np.number)\n\n# Melt data from wide-to-long format\ndata_melted = pd.melt(dataRedColsList)\n\n# Boxplot of all numerical variables\nsns.boxplot(data = data_melted, x = 'variable', y = 'value', hue = 'variable' , width = 0.20)\n\n\n\n\nNote the extreme number of outliers represented in the boxplot\n\n# Find Q1, Q3, and interquartile range (IQR) for each column\nQ1 = dataRedColsList.quantile(q = .25)\nQ3 = dataRedColsList.quantile(q = .75)\nIQR = dataRedColsList.apply(stats.iqr)\n\n# Only keep rows in dataframe that have values within 1.5*IQR of Q1 and Q3\ndata_clean = dataRedColsList[~((dataRedColsList < (Q1 - 1.5 * IQR)) | (dataRedColsList > (Q3 + 1.5 * IQR))).any(axis = 1)]\n\n# Melt data from wide-to-long format\ndata_clean_melted =  pd.melt(data_clean)\n\n# Boxplot of all numerical variables, with outliers removed via the IQR cutoff criteria\nsns.boxplot(data = data_clean_melted, x = 'variable', y = 'value', hue = 'variable' , width = 0.20)\n\n\n\n\nBut the distribution changes dramatically when we remove outliers with the IQR method (see above). Interestingly, there are a new set of “outliers” which results from a new IQR being calculated.\n\n\nMissing Values (NAs)\n\nTable showing the extent of NAs in columns containing them\n\n\n# Copy of the data\ndataNA = data\n\n# Randomly add NAs to all columns replacing 10% of values\nfor col in dataNA.columns:\n    dataNA.loc[dataNA.sample(frac = 0.1).index, col] = np.nan\n\n# Sum of NAs in each column (should be the same, 10% of all)   \ndataNA.isnull().sum()\n\nresult_date       918\naffil_category    918\ntest_type         918\ntest_result       918\ntest_count        918\ntest_source       918\ndtype: int64\n\n\nBar plot showing all NA values in each column. Since we randomly produced a set amount above the numbers will all be the same.\n\n# Bar plot showing the number of NAs in each column\nmsno.bar(dataNA, figsize = (8, 8), fontsize = 10)\n\n\n\n\n\n\n\nCategorical Variables\n\n# Select only categorical columns (objects) and describe\ndata.describe(exclude = [np.number]) \n\n       result_date      affil_category  ... test_result          test_source\ncount         8262                8262  ...        8262                 8262\nunique         541                   4  ...           3                    2\ntop     2020-09-17  Off-Campus Student  ...    Negative  Test All Test Smart\nfreq            26                3033  ...        4157                 4568\n\n[4 rows x 5 columns]\n\n\n\ncount: number of values in the column\nunique: the number of unique categories\ntop: category with the most observations\nfreq: number of observations in the top category"
  },
  {
    "objectID": "DiagnosingLikeDataDoctor.html#produce-an-html-summary-of-a-data-set",
    "href": "DiagnosingLikeDataDoctor.html#produce-an-html-summary-of-a-data-set",
    "title": "Exploratory Data Analysis in Python - Diagnosing like a Data Doctor",
    "section": "Produce an HTML Summary of a Data Set",
    "text": "Produce an HTML Summary of a Data Set\n\n# Producing a pandas-profiling report \n# profile = ProfileReport(data, title = \"Pandas Profiling Report\")\n\n# HTML output\nprofile.to_widgets()\n\n\n\n\n\nMerchant, Nirav C, Jim Davis, George H Franks, Chun Ly, Fernando Rios, Todd Wickizer, Gary D Windham, and Michelle Yung. 2022. “University of Arizona Test-Trace-Treat COVID-19 Testing Results.” University of Arizona Research Data Repository. https://doi.org/10.25422/AZU.DATA.14869740.V3."
  },
  {
    "objectID": "ExploringLikeDataAdventurer.html",
    "href": "ExploringLikeDataAdventurer.html",
    "title": "Exploratory Data Analysis in Python - Exploring like a Data Adventurer",
    "section": "",
    "text": "Exploring the normality of numerical columns in a novel data set"
  },
  {
    "objectID": "ExploringLikeDataAdventurer.html#objectives",
    "href": "ExploringLikeDataAdventurer.html#objectives",
    "title": "Exploratory Data Analysis in Python - Exploring like a Data Adventurer",
    "section": "Objectives",
    "text": "Objectives\n\nUsing summary statistics to better understand individual columns in a data set.\nAssessing data normality in numerical columns.\nAssessing data normality within groups."
  },
  {
    "objectID": "ExploringLikeDataAdventurer.html#required-setup",
    "href": "ExploringLikeDataAdventurer.html#required-setup",
    "title": "Exploratory Data Analysis in Python - Exploring like a Data Adventurer",
    "section": "Required Setup",
    "text": "Required Setup\nWe first need to prepare our environment with the necessary libraries and set a global theme for publishable plots in seaborn.\n\n# Import all required libraries\n# Data analysis and manipulation\nimport pandas as pd\n# Working with arrays\nimport numpy as np\n# Statistical visualization\nimport seaborn as sns\n# Matlab plotting for Python\nimport matplotlib.pyplot as plt\n# Data analysis\nimport statistics as stat\n# Predictive data analysis: process data \nfrom sklearn import preprocessing as pproc\nimport scipy.stats as stats\n# Visualizing missing values\nimport missingno as msno\n# Statistical modeling\nimport statsmodels.api as sm\n\n# increase font size of all seaborn plot elements\nsns.set(font_scale = 1.25)"
  },
  {
    "objectID": "ExploringLikeDataAdventurer.html#load-and-examine-a-data-set",
    "href": "ExploringLikeDataAdventurer.html#load-and-examine-a-data-set",
    "title": "Exploratory Data Analysis in Python - Exploring like a Data Adventurer",
    "section": "Load and Examine a Data Set",
    "text": "Load and Examine a Data Set\nWe will be using open source data from UArizona researchers that investigates the effects of climate change on canopy trees. (Meredith, Ladd, and Werner 2021)\n\n# Read csv \ndata = pd.read_csv(\"data/Data_Fig2_Repo.csv\")\n\n# What does the data look like\ndata.head()\n\n      Date                Group    Sap_Flow  TWaterFlux      pLWP      mLWP\n0  10/4/19  Drought-sens-canopy  184.040975   82.243292 -0.263378 -0.679769\n1  10/4/19   Drought-sens-under    2.475989    1.258050 -0.299669 -0.761326\n2  10/4/19   Drought-tol-canopy   10.598949    4.405479 -0.437556 -0.722557\n3  10/4/19    Drought-tol-under    4.399854    2.055276 -0.205224 -0.702858\n4  10/5/19  Drought-sens-canopy  182.905444   95.865255 -0.276928 -0.708261"
  },
  {
    "objectID": "ExploringLikeDataAdventurer.html#diagnose-your-data",
    "href": "ExploringLikeDataAdventurer.html#diagnose-your-data",
    "title": "Exploratory Data Analysis in Python - Exploring like a Data Adventurer",
    "section": "Diagnose your Data",
    "text": "Diagnose your Data\n\n# What are the properties of the data\ndiagnose = data.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 588 entries, 0 to 587\nData columns (total 6 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   Date        588 non-null    object \n 1   Group       588 non-null    object \n 2   Sap_Flow    480 non-null    float64\n 3   TWaterFlux  588 non-null    float64\n 4   pLWP        276 non-null    float64\n 5   mLWP        308 non-null    float64\ndtypes: float64(4), object(2)\nmemory usage: 27.7+ KB\n\n\n\nColumn: name of each variable\nNon-Null Count: number of missing values\nDType: data type of each variable\n\n\n\nBox Plot\n\n\n\nImage Credit: CÉDRIC SCHERER\n\n\n\n\n\nSkewness\n\n\n\n(c) Andrey Akinshin\n\n\n\n\nNOTE\n\n“Skewness” has multiple definitions. Several underlying equations mey be at play\nSkewness is “designed” for distributions with one peak (unimodal); it’s meaningless for distributions with multiple peaks (multimodal).\nMost default skewness definitions are not robust: a single outlier could completely distort the skewness value.\nWe can’t make conclusions about the locations of the mean and the median based on the skewness sign.\n\n\n\n\n\nKurtosis\n\n\n\n(c) Andrey Akinshin\n\n\n\nNOTE\n\nThere are multiple definitions of kurtosis - i.e., “kurtosis” and “excess kurtosis,” but there are other definitions of this measure.\nKurtosis may work fine for distributions with one peak (unimodal); it’s meaningless for distributions with multiple peaks (multimodal).\nThe classic definition of kurtosis is not robust: it could be easily spoiled by extreme outliers."
  },
  {
    "objectID": "ExploringLikeDataAdventurer.html#describe-your-continuous-data",
    "href": "ExploringLikeDataAdventurer.html#describe-your-continuous-data",
    "title": "Exploratory Data Analysis in Python - Exploring like a Data Adventurer",
    "section": "Describe your Continuous Data",
    "text": "Describe your Continuous Data\n\n# Summary statistics of our numerical columns\ndata.describe()\n\n         Sap_Flow  TWaterFlux        pLWP        mLWP\ncount  480.000000  588.000000  276.000000  308.000000\nmean    25.091576   11.925722   -0.609055   -1.029703\nstd     40.520386   19.048809    0.227151    0.295834\nmin      0.172630    0.101381   -1.433333   -1.812151\n25%      2.454843    1.293764   -0.714008   -1.227326\n50%      5.815661    2.995357   -0.586201   -0.946656\n75%     16.371703    7.577102   -0.450000   -0.808571\nmax    184.040975   96.012719   -0.205224   -0.545165\n\n\n\ncount: number of observations\nmean: arithmetic mean (average value)\nstd: standard deviation\nmin: minimum value\n25%: 1/4 quartile, 25th percentile\n50%: median, 50th percentile\n75%: 3/4 quartile, 75th percentile\nmax: maximum value\n\n\n# Make a copy of the data \ndataCopy = data.copy()\n\n# Select only numerical columns\ndataRed = dataCopy.select_dtypes(include = np.number)\n\n# List of numerical columns\ndataRedColsList = dataRed.columns[...]\n\n# For all values in the numerical column list from above\nfor i_col in dataRedColsList:\n  # List of the values in i_col\n  dataRed_i = dataRed.loc[:,i_col]\n  \n  # Define the 25th and 75th percentiles\n  q25, q75 = round((dataRed_i.quantile(q = 0.25)), 3), round((dataRed_i.quantile(q = 0.75)), 3)\n  \n  # Define the interquartile range from the 25th and 75th percentiles defined above\n  IQR = round((q75 - q25), 3)\n  \n  # Calculate the outlier cutoff \n  cut_off = IQR * 1.5\n  \n  # Define lower and upper cut-offs\n  lower, upper = round((q25 - cut_off), 3), round((q75 + cut_off), 3)\n  \n  # Skewness\n  skewness = round((dataRed_i.skew()), 3) \n  \n  # Kurtosis\n  kurtosis = round((dataRed_i.kurt()), 3)\n  \n  # Number of outliers\n  outliers = dataRed_i[(dataRed_i < lower) | (dataRed_i > upper)].count()\n  \n  # Print a blank row\n  print('')\n  \n  # Print the column name\n  print(i_col)\n  \n  # For each value of i_col, print the 25th and 75th percentiles and IQR\n  print('q25 =', q25, 'q75 =', q75, 'IQR =', IQR)\n  \n  # Print the lower and upper cut-offs\n  print('lower, upper:', lower, upper)\n  \n  # Print skewness and kurtosis\n  print('skewness =', skewness, 'kurtosis =', kurtosis)\n  \n  # Count the number of outliers outside the (lower, upper) limits, print that value\n  print('Number of Outliers: ', outliers)\n\n\nSap_Flow\nq25 = 2.455 q75 = 16.372 IQR = 13.917\nlower, upper: -18.42 37.248\nskewness = 2.153 kurtosis = 4.197\nNumber of Outliers:  116\n\nTWaterFlux\nq25 = 1.294 q75 = 7.577 IQR = 6.283\nlower, upper: -8.13 17.002\nskewness = 2.081 kurtosis = 3.884\nNumber of Outliers:  139\n\npLWP\nq25 = -0.714 q75 = -0.45 IQR = 0.264\nlower, upper: -1.11 -0.054\nskewness = -1.105 kurtosis = 1.767\nNumber of Outliers:  12\n\nmLWP\nq25 = -1.227 q75 = -0.809 IQR = 0.418\nlower, upper: -1.854 -0.182\nskewness = -0.797 kurtosis = -0.181\nNumber of Outliers:  0\n\n\n\nq25: 1/4 quartile, 25th percentile\nq75: 3/4 quartile, 75th percentile\nIQR: interquartile range (q75-q25)\nlower: lower limit of \\(1.5*IQR\\) used to calculate outliers\nupper: upper limit of \\(1.5*IQR\\) used to calculate outliers\nskewness: skewness\nkurtosis: kurtosis"
  },
  {
    "objectID": "ExploringLikeDataAdventurer.html#describe-categorical-variables",
    "href": "ExploringLikeDataAdventurer.html#describe-categorical-variables",
    "title": "Exploratory Data Analysis in Python - Exploring like a Data Adventurer",
    "section": "Describe Categorical Variables",
    "text": "Describe Categorical Variables\n\n# Select only categorical columns (objects) \ndata.describe(exclude=[np.number]) \n\n           Date                Group\ncount       588                  588\nunique      147                    4\ntop     10/4/19  Drought-sens-canopy\nfreq          4                  147\n\n\n\n\nGroup Descriptive Statistics\n\n# Grouped describe by one column, stacked \nGroups = data.groupby('Group').describe().unstack(1)\n\n# Print all rows\nprint(Groups.to_string())\n\n                   Group              \nSap_Flow    count  Drought-sens-canopy    120.000000\n                   Drought-sens-under     120.000000\n                   Drought-tol-canopy     120.000000\n                   Drought-tol-under      120.000000\n            mean   Drought-sens-canopy     85.269653\n                   Drought-sens-under       1.448825\n                   Drought-tol-canopy       9.074309\n                   Drought-tol-under        4.573516\n            std    Drought-sens-canopy     41.313962\n                   Drought-sens-under       0.803858\n                   Drought-tol-canopy       1.395670\n                   Drought-tol-under        0.902430\n            min    Drought-sens-canopy     33.370450\n                   Drought-sens-under       0.172630\n                   Drought-tol-canopy       5.904610\n                   Drought-tol-under        2.171780\n            25%    Drought-sens-canopy     53.975162\n                   Drought-sens-under       0.534165\n                   Drought-tol-canopy       8.119410\n                   Drought-tol-under        4.053346\n            50%    Drought-sens-canopy     76.717782\n                   Drought-sens-under       1.665492\n                   Drought-tol-canopy       9.286552\n                   Drought-tol-under        4.944842\n            75%    Drought-sens-canopy     94.068107\n                   Drought-sens-under       2.194299\n                   Drought-tol-canopy      10.404117\n                   Drought-tol-under        5.139685\n            max    Drought-sens-canopy    184.040975\n                   Drought-sens-under       2.475989\n                   Drought-tol-canopy      10.705455\n                   Drought-tol-under        5.726712\nTWaterFlux  count  Drought-sens-canopy    147.000000\n                   Drought-sens-under     147.000000\n                   Drought-tol-canopy     147.000000\n                   Drought-tol-under      147.000000\n            mean   Drought-sens-canopy     40.404061\n                   Drought-sens-under       0.751770\n                   Drought-tol-canopy       4.357234\n                   Drought-tol-under        2.189824\n            std    Drought-sens-canopy     19.027997\n                   Drought-sens-under       0.429073\n                   Drought-tol-canopy       0.940353\n                   Drought-tol-under        0.597511\n            min    Drought-sens-canopy     12.377738\n                   Drought-sens-under       0.101381\n                   Drought-tol-canopy       2.036843\n                   Drought-tol-under        0.953906\n            25%    Drought-sens-canopy     25.220908\n                   Drought-sens-under       0.274190\n                   Drought-tol-canopy       3.601341\n                   Drought-tol-under        1.735003\n            50%    Drought-sens-canopy     38.630891\n                   Drought-sens-under       0.824875\n                   Drought-tol-canopy       4.460778\n                   Drought-tol-under        2.198131\n            75%    Drought-sens-canopy     50.096197\n                   Drought-sens-under       1.112890\n                   Drought-tol-canopy       5.112844\n                   Drought-tol-under        2.686605\n            max    Drought-sens-canopy     96.012719\n                   Drought-sens-under       1.801823\n                   Drought-tol-canopy       5.976890\n                   Drought-tol-under        3.654336\npLWP        count  Drought-sens-canopy     69.000000\n                   Drought-sens-under      69.000000\n                   Drought-tol-canopy      69.000000\n                   Drought-tol-under       69.000000\n            mean   Drought-sens-canopy     -0.669932\n                   Drought-sens-under      -0.696138\n                   Drought-tol-canopy      -0.629909\n                   Drought-tol-under       -0.440243\n            std    Drought-sens-canopy      0.246390\n                   Drought-sens-under       0.283935\n                   Drought-tol-canopy       0.095571\n                   Drought-tol-under        0.131879\n            min    Drought-sens-canopy     -1.299263\n                   Drought-sens-under      -1.433333\n                   Drought-tol-canopy      -0.863656\n                   Drought-tol-under       -0.746667\n            25%    Drought-sens-canopy     -0.790573\n                   Drought-sens-under      -0.800000\n                   Drought-tol-canopy      -0.706479\n                   Drought-tol-under       -0.520487\n            50%    Drought-sens-canopy     -0.705942\n                   Drought-sens-under      -0.592118\n                   Drought-tol-canopy      -0.602841\n                   Drought-tol-under       -0.406439\n            75%    Drought-sens-canopy     -0.473290\n                   Drought-sens-under      -0.521217\n                   Drought-tol-canopy      -0.571356\n                   Drought-tol-under       -0.360789\n            max    Drought-sens-canopy     -0.263378\n                   Drought-sens-under      -0.299669\n                   Drought-tol-canopy      -0.437556\n                   Drought-tol-under       -0.205224\nmLWP        count  Drought-sens-canopy     77.000000\n                   Drought-sens-under      77.000000\n                   Drought-tol-canopy      77.000000\n                   Drought-tol-under       77.000000\n            mean   Drought-sens-canopy     -1.319148\n                   Drought-sens-under      -1.097537\n                   Drought-tol-canopy      -0.892554\n                   Drought-tol-under       -0.809572\n            std    Drought-sens-canopy      0.298107\n                   Drought-sens-under       0.263522\n                   Drought-tol-canopy       0.091729\n                   Drought-tol-under        0.170603\n            min    Drought-sens-canopy     -1.812151\n                   Drought-sens-under      -1.808333\n                   Drought-tol-canopy      -1.073619\n                   Drought-tol-under       -1.168716\n            25%    Drought-sens-canopy     -1.525563\n                   Drought-sens-under      -1.335521\n                   Drought-tol-canopy      -0.945841\n                   Drought-tol-under       -0.907041\n            50%    Drought-sens-canopy     -1.354771\n                   Drought-sens-under      -1.054159\n                   Drought-tol-canopy      -0.890061\n                   Drought-tol-under       -0.735647\n            75%    Drought-sens-canopy     -1.111942\n                   Drought-sens-under      -0.907564\n                   Drought-tol-canopy      -0.828777\n                   Drought-tol-under       -0.699087\n            max    Drought-sens-canopy     -0.679769\n                   Drought-sens-under      -0.546152\n                   Drought-tol-canopy      -0.707789\n                   Drought-tol-under       -0.545165"
  },
  {
    "objectID": "ExploringLikeDataAdventurer.html#sec-testing-normality",
    "href": "ExploringLikeDataAdventurer.html#sec-testing-normality",
    "title": "Exploratory Data Analysis in Python - Exploring like a Data Adventurer",
    "section": "Testing Normality",
    "text": "Testing Normality\n\nShapiro-Wilk test & Q-Q plots\nTesting overall normality of two columns\nTesting normality of groups\n\n\n\nNormality of Columns\n\n\nShapiro-Wilk Test\nShapiro-Wilk test looks at whether a target distribution is sample form a normal distribution\n\n# Make a copy of the data \ndataCopy = data.copy()\n\n# Remove NAs\ndataCopyFin = dataCopy.dropna()\n\n# Specify desired column\ni_col = dataCopyFin.Sap_Flow\n\n# Normality test\nstat, p = stats.shapiro(i_col)\n\nprint('')\nprint('Shapiro-Wilk Test for Normality')\n\nShapiro-Wilk Test for Normality\n\nprint('')\nprint('Sap_Flow')\n\nSap_Flow\n\nprint('Statistic = %.3f, p = %.3f' % (stat, p))\n  \n# Interpret\n\nStatistic = 0.603, p = 0.000\n\nalpha = 0.05\n  \nif p > alpha:\n  print('Sample looks Gaussian (fail to reject H0)')\nelse:\n  print('Sample does not look Gaussian (reject H0)')\n\nSample does not look Gaussian (reject H0)\n\n\nYou can also run the Shapiro-Wilk test on all numerical columns with a for-loop\n\n# Make a copy of the data \ndataCopy = data.copy()\n\n# Remove NAs\ndataCopyFin = dataCopy.dropna()\n\n# Select only numerical columns\ndataRed = dataCopyFin.select_dtypes(include = np.number)\n\n# List of numerical columns\ndataRedColsList = dataRed.columns[...]\n\n# For all values in the numerical column list from above\nfor i_col in dataRedColsList:\n  # List of the values in i_col\n  dataRed_i = dataRed.loc[:,i_col]\n  \n  # Normality test\n  stat, p = stats.shapiro(dataRed_i)\n  \n  # Print a blank, the column name, the statistic and p-value\n  print('')\n  print(i_col)\n  print('Statistic = %.3f, p = %.3f' % (stat, p))\n  \n  # Interpret\n  alpha = 0.05\n  \n  # Print the interpretation\n  if p > alpha:\n    print('Sample looks Gaussian (fail to reject H0)')\n  else:\n      print('Sample does not look Gaussian (reject H0)')\n\n\nSap_Flow\nStatistic = 0.603, p = 0.000\nSample does not look Gaussian (reject H0)\n\nTWaterFlux\nStatistic = 0.600, p = 0.000\nSample does not look Gaussian (reject H0)\n\npLWP\nStatistic = 0.929, p = 0.000\nSample does not look Gaussian (reject H0)\n\nmLWP\nStatistic = 0.940, p = 0.000\nSample does not look Gaussian (reject H0)\n\n\n\n\n\nQ-Q Plots\nPlots of the quartiles of a target data set and plot it against predicted quartiles from a normal distribution.\n\n# Change theme to \"white\"\nsns.set_style(\"white\")\n\n# Make a copy of the data \ndataCopy = data.copy()\n\n# Remove NAs\ndataCopyFin = dataCopy.dropna()\n\n# Specify desired column\ni_col = dataCopyFin.Sap_Flow\n\n# Subplots\nfig, (ax1, ax2) = plt.subplots(ncols = 2, nrows = 1)\n\n# Density plot\nsns.kdeplot(i_col, linewidth = 5, ax = ax1)\nax1.set_title('Sap_Flow Density plot')\n\n# Q-Q plot\nsm.qqplot(i_col, line='s', ax = ax2)\nax2.set_title('Sap_Flow Q-Q plot')\nplt.tight_layout()\nplt.show()\n\n\n\n\nYou can also produce these plots for all numerical columns with a for-loop.\n\n# Change theme to \"white\"\nsns.set_style(\"white\")\n\n# Make a copy of the data \ndataCopy = data.copy()\n\n# Remove NAs\ndataCopyFin = dataCopy.dropna()\n\n# Select only numerical columns\ndataRed = dataCopyFin.select_dtypes(include = np.number)\n\n# Combine multiple plots, the number of columns and rows is derived from the number of numerical columns from above. \n\n# Overall figure that subplots fill\nfig, axes = plt.subplots(ncols = 2, nrows = 4, sharex = True, figsize = (4, 4))\n\n# Fill the subplots\nfor k, ax in zip(dataRed.columns, np.ravel(axes)):\n    # Subplots\n    fig, (ax1, ax2) = plt.subplots(ncols = 2, nrows = 1)\n    \n    # Density plot\n    sns.kdeplot(dataRed[k], linewidth = 5, ax = ax1)\n    ax1.set_title(f'{k} Density Plot')\n    \n    # Q-Q plot\n    sm.qqplot(dataRed[k], line='s', ax = ax2)\n    ax2.set_title(f'{k} QQ Plot')\n    plt.tight_layout()\n    plt.show()\n\n\n\n\n\nNormality within Groups\nLooking within Age_group at the subgroup normality.\n\nShapiro-Wilk Test\n\n# Make a copy of the data \ndataCopy = data.copy()\n\n# Remove NAs\ndataCopyFin = dataCopy.dropna()\n\n# Pivot the data from long-to-wide with pivot, using Date as the index, so that a column is created for each Group and numerical column subset\ndataPivot = dataCopyFin.pivot(index = 'Date', columns = 'Group', values = ['Sap_Flow', 'TWaterFlux', 'pLWP', 'mLWP'])\n\n# Select only numerical columns\ndataRed = dataPivot.select_dtypes(include = np.number)\n\n# List of numerical columns\ndataRedColsList = dataRed.columns[...]\n\n# For all values in the numerical column list from above\nfor i_col in dataRedColsList:\n  # List of the values in i_col\n  dataRed_i = dataRed.loc[:,i_col]\n  \n  # normality test\n  stat, p = stats.shapiro(dataRed_i)\n  \n  print('')\n  print(i_col)\n  print('Statistics = %.3f, p = %.3f' % (stat, p))\n  \n  # interpret\n  alpha = 0.05\n  \n  if p > alpha:\n    print('Sample looks Gaussian (fail to reject H0)')\n  else:\n      print('Sample does not look Gaussian (reject H0)')\n\n\n('Sap_Flow', 'Drought-sens-canopy')\nStatistics = 0.869, p = 0.000\nSample does not look Gaussian (reject H0)\n\n('Sap_Flow', 'Drought-sens-under')\nStatistics = 0.889, p = 0.000\nSample does not look Gaussian (reject H0)\n\n('Sap_Flow', 'Drought-tol-canopy')\nStatistics = 0.950, p = 0.008\nSample does not look Gaussian (reject H0)\n\n('Sap_Flow', 'Drought-tol-under')\nStatistics = 0.908, p = 0.000\nSample does not look Gaussian (reject H0)\n\n('TWaterFlux', 'Drought-sens-canopy')\nStatistics = 0.885, p = 0.000\nSample does not look Gaussian (reject H0)\n\n('TWaterFlux', 'Drought-sens-under')\nStatistics = 0.856, p = 0.000\nSample does not look Gaussian (reject H0)\n\n('TWaterFlux', 'Drought-tol-canopy')\nStatistics = 0.973, p = 0.147\nSample looks Gaussian (fail to reject H0)\n\n('TWaterFlux', 'Drought-tol-under')\nStatistics = 0.977, p = 0.233\nSample looks Gaussian (fail to reject H0)\n\n('pLWP', 'Drought-sens-canopy')\nStatistics = 0.969, p = 0.086\nSample looks Gaussian (fail to reject H0)\n\n('pLWP', 'Drought-sens-under')\nStatistics = 0.867, p = 0.000\nSample does not look Gaussian (reject H0)\n\n('pLWP', 'Drought-tol-canopy')\nStatistics = 0.952, p = 0.010\nSample does not look Gaussian (reject H0)\n\n('pLWP', 'Drought-tol-under')\nStatistics = 0.964, p = 0.044\nSample does not look Gaussian (reject H0)\n\n('mLWP', 'Drought-sens-canopy')\nStatistics = 0.962, p = 0.034\nSample does not look Gaussian (reject H0)\n\n('mLWP', 'Drought-sens-under')\nStatistics = 0.956, p = 0.016\nSample does not look Gaussian (reject H0)\n\n('mLWP', 'Drought-tol-canopy')\nStatistics = 0.962, p = 0.034\nSample does not look Gaussian (reject H0)\n\n('mLWP', 'Drought-tol-under')\nStatistics = 0.852, p = 0.000\nSample does not look Gaussian (reject H0)\n\n\n\n\n\nQ-Q Plots\n\n# Make a copy of the data \ndataCopy = data.copy()\n\n# Remove NAs\ndataCopyFin = dataCopy.dropna()\n\n# Pivot the data from long-to-wide with pivot, using Date as the index, so that a column is created for each Group and numerical column subset\ndataPivot = dataCopyFin.pivot(index = 'Date', columns = 'Group', values = ['Sap_Flow', 'TWaterFlux', 'pLWP', 'mLWP'])\n\n# Select only numerical columns\ndataRed = dataPivot.select_dtypes(include = np.number)\n\n# Combine multiple plots, the number of columns and rows is derived from the number of numerical columns from above. \nfig, axes = plt.subplots(ncols = 2, nrows = 8, sharex = True, figsize = (2 * 4, 8 * 4))\n\n# Generate figures for all numerical grouped data subsets\nfor k, ax in zip(dataRed.columns, np.ravel(axes)):\n    sm.qqplot(dataRed[k], line = 's', ax = ax)\n    ax.set_title(f'{k}\\n QQ Plot')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nMeredith, Laura, S. Nemiah Ladd, and Christiane Werner. 2021. “Data for \"Ecosystem Fluxes During Drought and Recovery in an Experimental Forest\".” University of Arizona Research Data Repository. https://doi.org/10.25422/AZU.DATA.14632593.V1."
  },
  {
    "objectID": "TransformingLikeDataTrans.html",
    "href": "TransformingLikeDataTrans.html",
    "title": "Exploratory Data Analysis in Python - Transforming like a Data… Transformer",
    "section": "",
    "text": "Using data transformation to correct non-normality in numerical data"
  },
  {
    "objectID": "TransformingLikeDataTrans.html#objectives",
    "href": "TransformingLikeDataTrans.html#objectives",
    "title": "Exploratory Data Analysis in Python - Transforming like a Data… Transformer",
    "section": "Objectives",
    "text": "Objectives\n\nLoad and explore a data set with publication quality tables\nQuickly diagnose non-normality in data\nData transformation"
  },
  {
    "objectID": "TransformingLikeDataTrans.html#required-setup",
    "href": "TransformingLikeDataTrans.html#required-setup",
    "title": "Exploratory Data Analysis in Python - Transforming like a Data… Transformer",
    "section": "Required Setup",
    "text": "Required Setup\nWe first need to prepare our environment with the necessary libraries\n\n# Import all required libraries\n# Data analysis and manipulation\nimport pandas as pd\n# Working with arrays\nimport numpy as np\n# Statistical visualization\nimport seaborn as sns\n# Matlab plotting for Python\nimport matplotlib.pyplot as plt\n# Data analysis\nimport statistics as stat\n# Predictive data analysis: process data \nfrom sklearn import preprocessing as pproc\nimport scipy.stats as stats\n# Visualizing missing values\nimport missingno as msno\n# Statistical modeling\nimport statsmodels.api as sm\n\n# Increase font size of all seaborn plot elements\nsns.set(font_scale = 1.5, rc = {'figure.figsize':(8, 8)})\n\n# Change theme to \"white\"\nsns.set_style(\"white\")"
  },
  {
    "objectID": "TransformingLikeDataTrans.html#load-and-examine-a-data-set",
    "href": "TransformingLikeDataTrans.html#load-and-examine-a-data-set",
    "title": "Exploratory Data Analysis in Python - Transforming like a Data… Transformer",
    "section": "Load and Examine a Data Set",
    "text": "Load and Examine a Data Set\n\nLoad data and view\nExamine columns and data types\nExamine data normality\nDescribe properties of data\n\n\n# Read csv \ndata = pd.read_csv(\"data/diabetes.csv\")\n\n# Create Age_group from the age column\ndef Age_group_data(data): \n  if data.Age >= 21 and data.Age <= 30: return \"Young\"\n  elif data.Age > 30 and data.Age <= 50: return \"Middle\" \n  else: return \"Elderly\"\n\n# Apply the function to data\ndata['Age_group'] = data.apply(Age_group_data, axis = 1)\n\n# What does the data look like\ndata.head()\n\n   Pregnancies  Glucose  BloodPressure  ...  Age  Outcome  Age_group\n0            6      148             72  ...   50        1     Middle\n1            1       85             66  ...   31        0     Middle\n2            8      183             64  ...   32        1     Middle\n3            1       89             66  ...   21        0      Young\n4            0      137             40  ...   33        1     Middle\n\n[5 rows x 10 columns]\n\n\n\n\nData Normality\nNormal distributions (bell curves) are a common data assumptions for many hypothesis testing statistics, in particular parametric statistics. Deviations from normality can either strongly skew the results or reduce the power to detect a significant statistical difference.\nHere are the distribution properties to know and consider:\n\nThe mean, median, and mode are the same value.\nDistribution symmetry at the mean.\nNormal distributions can be described by the mean and standard deviation.\n\nHere’s an example using the Glucose column in our dataset\n\n\n\n\n\n\n\nDescribing Properties of our Data (Refined)\n\nSkewness\nThe symmetry of the distribution\nSee Section 3.3 for more information about these values\n\n# Make a copy of the data \ndataCopy = data.copy()\n\n# Select only numerical columns\ndataRed = dataCopy.select_dtypes(include = np.number)\n\n# List of numerical columns\ndataRedColsList = dataRed.columns[...]\n\n# For all values in the numerical column list from above\nfor i_col in dataRedColsList:\n  # List of the values in i_col\n  dataRed_i = dataRed.loc[:,i_col]\n  \n  # Skewness\n  skewness = round((dataRed_i.skew()), 3) \n  \n  # Kurtosis\n  kurtosis = round((dataRed_i.kurt()), 3)\n  \n  # Print a blank row\n  print('')\n  \n  # Print the column name\n  print(i_col)\n  \n  # Print skewness and kurtosis\n  print('skewness =', skewness, 'kurtosis =', kurtosis)\n\n\nPregnancies\nskewness = 0.902 kurtosis = 0.159\n\nGlucose\nskewness = 0.174 kurtosis = 0.641\n\nBloodPressure\nskewness = -1.844 kurtosis = 5.18\n\nSkinThickness\nskewness = 0.109 kurtosis = -0.52\n\nInsulin\nskewness = 2.272 kurtosis = 7.214\n\nBMI\nskewness = -0.429 kurtosis = 3.29\n\nDiabetesPedigreeFunction\nskewness = 1.92 kurtosis = 5.595\n\nAge\nskewness = 1.13 kurtosis = 0.643\n\nOutcome\nskewness = 0.635 kurtosis = -1.601\n\n\n\nskewness: skewness\nkurtosis: kurtosis"
  },
  {
    "objectID": "TransformingLikeDataTrans.html#testing-normality-accelerated",
    "href": "TransformingLikeDataTrans.html#testing-normality-accelerated",
    "title": "Exploratory Data Analysis in Python - Transforming like a Data… Transformer",
    "section": "Testing Normality (Accelerated)",
    "text": "Testing Normality (Accelerated)\n\nQ-Q plots\nTesting overall normality of numerical columns\nTesting normality of groups\n\nNote that you can also run Shapiro-Wilk tests (see -Section 8), but since this test is not viable at N < 20, I recommend just skipping to Q-Q plots.\n\n\nQ-Q Plots\nPlots of the quartiles of a target data set and plot it against predicted quartiles from a normal distribution (see -Section 8.2 for density and Q-Q plots)\n\n# Make a copy of the data \ndataCopy = data.copy()\n\n# Remove NAs\ndataCopyFin = dataCopy.dropna()\n\n# Drop Outcome, binary columns are never normally distributed \ndataCopyFin1 = dataCopyFin.drop('Outcome', axis = \"columns\")\n\n# Select only numerical columns\ndataRed = dataCopyFin1.select_dtypes(include = np.number)\n\n# Combine multiple plots, the number of columns and rows is derived from the number of numerical columns from above. \nfig, axes = plt.subplots(ncols = 2, nrows = 4, sharex = True, figsize = (2 * 4, 4 * 4))\n\n# Generate figures for all numerical grouped data subsets\nfor k, ax in zip(dataRed.columns, np.ravel(axes)):\n    sm.qqplot(dataRed[k], line = 's', ax = ax)\n    ax.set_title(f'{k}\\n QQ Plot')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "TransformingLikeDataTrans.html#normality-within-groups",
    "href": "TransformingLikeDataTrans.html#normality-within-groups",
    "title": "Exploratory Data Analysis in Python - Transforming like a Data… Transformer",
    "section": "Normality within Groups",
    "text": "Normality within Groups\nLooking within Age_group at the subgroup normality\n\nQ-Q Plots\n\n# Make a copy of the data \ndataCopy = data.copy()\n\n# Remove NAs\ndataCopyFin = dataCopy.dropna()\n\n# Create a new column named in x, which is filled with the dataset rownames\ndataCopyFin.index.name = 'Index'\n\n# Reset the rownames index (not a column)\ndataCopyFin.reset_index(inplace = True)\n\n# Pivot the data from long-to-wide with pivot, using Date as the index, so that a column is created for each Group and numerical column subset\ndataPivot = dataCopyFin.pivot(index = 'Index', columns = 'Age_group', values = ['Insulin', 'Glucose', 'SkinThickness', 'BloodPressure'])\n\n# Select only numerical columns\ndataRed = dataPivot.select_dtypes(include = np.number)\n\n# Combine multiple plots, the number of columns and rows is derived from the number of numerical columns from above. \nfig, axes = plt.subplots(ncols = 2, nrows = 6, sharex = True, figsize = (2 * 4, 6 * 4))\n\n# Generate figures for all numerical grouped data subsets\nfor k, ax in zip(dataRed.columns, np.ravel(axes)):\n    sm.qqplot(dataRed[k], line = 's', ax = ax)\n    ax.set_title(f'{k}\\n QQ Plot')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "TransformingLikeDataTrans.html#transforming-data",
    "href": "TransformingLikeDataTrans.html#transforming-data",
    "title": "Exploratory Data Analysis in Python - Transforming like a Data… Transformer",
    "section": "Transforming Data",
    "text": "Transforming Data\nYour data could be more easily interpreted with a transformation, since not all relationships in nature follow a linear relationship - i.e., many biological phenomena follow a power law (or logarithmic curve), where they do not scale linearly.\nWe will try to transform the Insulin column with through several approaches and discuss the pros and cons of each. First however, we will remove 0 values, because Insulin values are impossible…\n\n# Filter insulin greater than 0\nIns = data[data.Insulin > 0]\n\n# Select only Insulin\nInsMod = Ins.filter([\"Insulin\"], axis = \"columns\")\n\n\n\nSquare-root, Cube-root, and Logarithmic Transformations\nResolving Skewness using the following data transformations:\nSquare-root transformation. \\(\\sqrt x\\) (moderate skew)\nLog transformation. \\(log(x)\\) (greater skew)\nLog + constant transformation. \\(log(x + 1)\\). Used for values that contain 0.\nInverse transformation. \\(1/x\\) (severe skew)\nSquared transformation. \\(x^2\\)\nCubed transformation. \\(x^3\\)\nWe will compare sqrt, log+1, and 1/x (inverse) transformations. Note that you would have to add a constant to use the log transformation, so it is easier to use the log+1 instead. You however need to add a constant to both the sqrt and 1/x transformations because they don’t include zeros and will otherwise skew the results.\n\n\nSquare-root Transformation\n\n# Square-root transform the data in a new column\nInsMod['Ins_Sqrt'] = np.sqrt(InsMod['Insulin'])\n\n# Specify desired column\ncol = InsMod.Insulin\n\n# Specify desired column\ni_col = InsMod.Ins_Sqrt\n\n# ORIGINAL\n# Subplots\nfig, (ax1, ax2) = plt.subplots(ncols = 2, nrows = 1)\n\n# Density plot\nsns.kdeplot(col, linewidth = 5, ax = ax1)\nax1.set_title('Insulin Density plot')    \n\n# Q-Q plot\nsm.qqplot(col, line='s', ax = ax2)\nax2.set_title('Insulin Q-Q plot')    \nplt.tight_layout()\nplt.show()\n\n# TRANSFORMED\n# Subplots\n\n\n\nfig, (ax1, ax2) = plt.subplots(ncols = 2, nrows = 1)\n\n# Density plot\nsns.kdeplot(i_col, linewidth = 5, ax = ax1)\nax1.set_title('Ins_Sqrt Density plot')   \n\n# Q-Q plot\nsm.qqplot(i_col, line='s', ax = ax2)\nax2.set_title('Ins_Sqrt Q-Q plot') \nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nLogarithmic (+1) Transformation\n\n# Logarithmic transform the data in a new column\nInsMod['Ins_Log'] = np.log(InsMod['Insulin'] + 1)\n\n# Specify desired column\ncol = InsMod.Insulin\n\n# Specify desired column\ni_col = InsMod.Ins_Log\n\n# ORIGINAL\n# Subplots\nfig, (ax1, ax2) = plt.subplots(ncols = 2, nrows = 1)\n\n# Density plot\nsns.kdeplot(col, linewidth = 5, ax = ax1)\nax1.set_title('Insulin Density plot')    \n\n# Q-Q plot\nsm.qqplot(col, line='s', ax = ax2)\nax2.set_title('Insulin Q-Q plot')    \nplt.tight_layout()\nplt.show()\n\n# TRANSFORMED\n# Subplots\n\n\n\nfig, (ax1, ax2) = plt.subplots(ncols = 2, nrows = 1)\n\n# Density plot\nsns.kdeplot(i_col, linewidth = 5, ax = ax1)\nax1.set_title('Ins_Log Density plot')   \n\n# Q-Q plot\nsm.qqplot(i_col, line='s', ax = ax2)\nax2.set_title('Ins_Log Q-Q plot') \nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nInverse Transformation\n\n# Inverse transform the data in a new column\nInsMod['Ins_Inv'] = 1/InsMod.Insulin\n\n# Specify desired column\ncol = InsMod.Insulin\n\n# Specify desired column\ni_col = InsMod.Ins_Inv\n\n# ORIGINAL\n# Subplots\nfig, (ax1, ax2) = plt.subplots(ncols = 2, nrows = 1)\n\n# Density plot\nsns.kdeplot(col, linewidth = 5, ax = ax1)\nax1.set_title('Insulin Density plot')    \n\n# Q-Q plot\nsm.qqplot(col, line='s', ax = ax2)\nax2.set_title('Insulin Q-Q plot')    \nplt.tight_layout()\nplt.show()\n\n# TRANSFORMED\n# Subplots\n\n\n\nfig, (ax1, ax2) = plt.subplots(ncols = 2, nrows = 1)\n\n# Density plot\nsns.kdeplot(i_col, linewidth = 5, ax = ax1)\nax1.set_title('Ins_Inv Density plot')   \n\n# Q-Q plot\nsm.qqplot(i_col, line='s', ax = ax2)\nax2.set_title('Ins_Inv Q-Q plot') \nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nBox-cox Transformation\nThere are several transformations, each with it’s own “criteria”, and they don’t always fix extremely skewed data. Instead, you can just choose the Box-Cox transformation which searches for the the best lambda value that maximizes the log-likelihood (basically, what power transformation is best). The benefit is that you should have normally distributed data after, but the power relationship might be pretty abstract (i.e., what would a transformation of x^0.12 be interpreted as in your system?..)\n\n# Box-cox transform the data in a new column\nInsMod['Ins_Boxcox'], parameters = stats.boxcox(InsMod['Insulin'])\n\n# Specify desired column\ncol = InsMod.Insulin\n\n# Specify desired column\ni_col = InsMod.Ins_Boxcox\n\n# ORIGINAL\n# Subplots\nfig, (ax1, ax2) = plt.subplots(ncols = 2, nrows = 1)\n\n# Density plot\nsns.kdeplot(col, linewidth = 5, ax = ax1)\nax1.set_title('Insulin Density plot')    \n\n# Q-Q plot\nsm.qqplot(col, line='s', ax = ax2)\nax2.set_title('Insulin Q-Q plot')    \nplt.tight_layout()\nplt.show()\n\n# TRANSFORMED\n# Subplots\n\n\n\nfig, (ax1, ax2) = plt.subplots(ncols = 2, nrows = 1)\n\n# Density plot\nsns.kdeplot(i_col, linewidth = 5, ax = ax1)\nax1.set_title('Ins_Boxcox Density plot')   \n\n# Q-Q plot\nsm.qqplot(i_col, line='s', ax = ax2)\nax2.set_title('Ins_Boxcox Q-Q plot') \nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "ImputingLikeDataScientist.html",
    "href": "ImputingLikeDataScientist.html",
    "title": "Exploratory Data Analysis in Python - Imputing like a Data Scientist",
    "section": "",
    "text": "Exploring, visualizing, and imputing outliers and missing values (NAs) in a novel data set"
  },
  {
    "objectID": "ImputingLikeDataScientist.html#objectives",
    "href": "ImputingLikeDataScientist.html#objectives",
    "title": "Exploratory Data Analysis in Python - Imputing like a Data Scientist",
    "section": "Objectives",
    "text": "Objectives\n\nLoad and explore a data set with publication quality tables\nThoroughly diagnose outliers and missing values\nImpute outliers and missing values"
  },
  {
    "objectID": "ImputingLikeDataScientist.html#required-setup",
    "href": "ImputingLikeDataScientist.html#required-setup",
    "title": "Exploratory Data Analysis in Python - Imputing like a Data Scientist",
    "section": "Required Setup",
    "text": "Required Setup\nWe first need to prepare our environment with the necessary libraries and set a global theme for publishable plots in seaborn.\n\n# Import all required libraries\n# Data analysis and manipulation\nimport pandas as pd\n# Working with arrays\nimport numpy as np\n# Statistical visualization\nimport seaborn as sns\n# Matlab plotting for Python\nimport matplotlib.pyplot as plt\n# Data analysis\nimport statistics as stat\nimport scipy.stats as stats\n# Visualizing missing values\nimport missingno as msno\n# Statistical modeling\nimport statsmodels.api as smx\n# Predictive data analysis: process data \nfrom sklearn import preprocessing as pproc\n# Predictive data analysis: outlier imputation\nfrom sklearn.impute import SimpleImputer\n# Predictive data analysis: KNN NA imputation\nfrom sklearn.impute import KNNImputer\n# Predictive data analysis: experimental iterative NA imputer (MICE)\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n# Predictive data analysis: linear models\nfrom sklearn.linear_model import LinearRegression\n# Predictive data analysis: Classifying nearest neighbors\nfrom sklearn import neighbors\n# Predictive data analysis: Plotting decision regions\nfrom mlxtend.plotting import plot_decision_regions\n\n\n# Increase font size of all seaborn plot elements\nsns.set(font_scale = 1.5, rc = {'figure.figsize':(8, 8)})\n\n# Change theme to \"white\"\nsns.set_style(\"white\")"
  },
  {
    "objectID": "ImputingLikeDataScientist.html#load-and-examine-a-data-set",
    "href": "ImputingLikeDataScientist.html#load-and-examine-a-data-set",
    "title": "Exploratory Data Analysis in Python - Imputing like a Data Scientist",
    "section": "Load and Examine a Data Set",
    "text": "Load and Examine a Data Set\n\n# Read csv \ndata = pd.read_csv(\"data/diabetes.csv\")\n\n# Create Age_group from the age column\ndef Age_group_data(data): \n  if data.Age >= 21 and data.Age <= 30: return \"Young\"\n  elif data.Age > 30 and data.Age <= 50: return \"Middle\" \n  else: return \"Elderly\"\n\n# Apply the function to data\ndata['Age_group'] = data.apply(Age_group_data, axis = 1)\n\n# What does the data look like\ndata.head()\n\n   Pregnancies  Glucose  BloodPressure  ...  Age  Outcome  Age_group\n0            6      148             72  ...   50        1     Middle\n1            1       85             66  ...   31        0     Middle\n2            8      183             64  ...   32        1     Middle\n3            1       89             66  ...   21        0      Young\n4            0      137             40  ...   33        1     Middle\n\n[5 rows x 10 columns]"
  },
  {
    "objectID": "ImputingLikeDataScientist.html#diagnose-your-data",
    "href": "ImputingLikeDataScientist.html#diagnose-your-data",
    "title": "Exploratory Data Analysis in Python - Imputing like a Data Scientist",
    "section": "Diagnose your Data",
    "text": "Diagnose your Data\n\n# What are the properties of the data\ndiagnose = data.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 768 entries, 0 to 767\nData columns (total 10 columns):\n #   Column                    Non-Null Count  Dtype  \n---  ------                    --------------  -----  \n 0   Pregnancies               768 non-null    int64  \n 1   Glucose                   768 non-null    int64  \n 2   BloodPressure             768 non-null    int64  \n 3   SkinThickness             768 non-null    int64  \n 4   Insulin                   768 non-null    int64  \n 5   BMI                       768 non-null    float64\n 6   DiabetesPedigreeFunction  768 non-null    float64\n 7   Age                       768 non-null    int64  \n 8   Outcome                   768 non-null    int64  \n 9   Age_group                 768 non-null    object \ndtypes: float64(2), int64(7), object(1)\nmemory usage: 60.1+ KB\n\n\n\nColumn: name of each variable\nNon-Null Count: number of missing values\nDType: data type of each variable"
  },
  {
    "objectID": "ImputingLikeDataScientist.html#diagnose-outliers",
    "href": "ImputingLikeDataScientist.html#diagnose-outliers",
    "title": "Exploratory Data Analysis in Python - Imputing like a Data Scientist",
    "section": "Diagnose Outliers",
    "text": "Diagnose Outliers\nThere are several numerical variables that have outliers above, let’s see what the data look like with and without them\n\nCreate a table with columns containing outliers\nPlot outliers in a box plot and histogram\n\n\n# Make a copy of the data \ndataCopy = data.copy()\n\n# Select only numerical columns\ndataRed = dataCopy.select_dtypes(include = np.number)\n\n# List of numerical columns\ndataRedColsList = dataRed.columns[...]\n\n# For all values in the numerical column list from above\nfor i_col in dataRedColsList:\n  # List of the values in i_col\n  dataRed_i = dataRed.loc[:,i_col]\n  \n  # Define the 25th and 75th percentiles\n  q25, q75 = round((dataRed_i.quantile(q = 0.25)), 3), round((dataRed_i.quantile(q = 0.75)), 3)\n  \n  # Define the interquartile range from the 25th and 75th percentiles defined above\n  IQR = round((q75 - q25), 3)\n  \n  # Calculate the outlier cutoff \n  cut_off = IQR * 1.5\n  \n  # Define lower and upper cut-offs\n  lower, upper = round((q25 - cut_off), 3), round((q75 + cut_off), 3)\n  \n  # Print the values\n  print(' ')\n  \n  # For each value of i_col, print the 25th and 75th percentiles and IQR\n  print(i_col, 'q25=', q25, 'q75=', q75, 'IQR=', IQR)\n  \n  # Print the lower and upper cut-offs\n  print('lower, upper:', lower, upper)\n\n  # Count the number of outliers outside the (lower, upper) limits, print that value\n  print('Number of Outliers: ', dataRed_i[(dataRed_i < lower) | (dataRed_i > upper)].count())\n\n \nPregnancies q25= 1.0 q75= 6.0 IQR= 5.0\nlower, upper: -6.5 13.5\nNumber of Outliers:  4\n \nGlucose q25= 99.0 q75= 140.25 IQR= 41.25\nlower, upper: 37.125 202.125\nNumber of Outliers:  5\n \nBloodPressure q25= 62.0 q75= 80.0 IQR= 18.0\nlower, upper: 35.0 107.0\nNumber of Outliers:  45\n \nSkinThickness q25= 0.0 q75= 32.0 IQR= 32.0\nlower, upper: -48.0 80.0\nNumber of Outliers:  1\n \nInsulin q25= 0.0 q75= 127.25 IQR= 127.25\nlower, upper: -190.875 318.125\nNumber of Outliers:  34\n \nBMI q25= 27.3 q75= 36.6 IQR= 9.3\nlower, upper: 13.35 50.55\nNumber of Outliers:  19\n \nDiabetesPedigreeFunction q25= 0.244 q75= 0.626 IQR= 0.382\nlower, upper: -0.329 1.199\nNumber of Outliers:  29\n \nAge q25= 24.0 q75= 41.0 IQR= 17.0\nlower, upper: -1.5 66.5\nNumber of Outliers:  9\n \nOutcome q25= 0.0 q75= 1.0 IQR= 1.0\nlower, upper: -1.5 2.5\nNumber of Outliers:  0\n\n\n\nq25: 1/4 quartile, 25th percentile\nq75: 3/4 quartile, 75th percentile\nIQR: interquartile range (q75-q25)\nlower: lower limit of \\(1.5*IQR\\) used to calculate outliers\nupper: upper limit of \\(1.5*IQR\\) used to calculate outliers"
  },
  {
    "objectID": "ImputingLikeDataScientist.html#basic-exploration-of-missing-values-nas",
    "href": "ImputingLikeDataScientist.html#basic-exploration-of-missing-values-nas",
    "title": "Exploratory Data Analysis in Python - Imputing like a Data Scientist",
    "section": "Basic Exploration of Missing Values (NAs)",
    "text": "Basic Exploration of Missing Values (NAs)\n\nTable showing the extent of NAs in columns containing them\n\n\ndataNA = data\n\nfor col in dataNA.columns:\n    dataNA.loc[dataNA.sample(frac = 0.1).index, col] = np.nan\n    \ndataNA.isnull().sum()\n\nPregnancies                 77\nGlucose                     77\nBloodPressure               77\nSkinThickness               77\nInsulin                     77\nBMI                         77\nDiabetesPedigreeFunction    77\nAge                         77\nOutcome                     77\nAge_group                   77\ndtype: int64\n\n\nBar plot showing all NA values in each column. Since we randomly produced a set amount above the numbers will all be the same.\n\nmsno.bar(dataNA, figsize = (8, 8), fontsize = 10)\nplt.tight_layout()"
  },
  {
    "objectID": "ImputingLikeDataScientist.html#advanced-exploration-of-missing-values-nas",
    "href": "ImputingLikeDataScientist.html#advanced-exploration-of-missing-values-nas",
    "title": "Exploratory Data Analysis in Python - Imputing like a Data Scientist",
    "section": "Advanced Exploration of Missing Values (NAs)",
    "text": "Advanced Exploration of Missing Values (NAs)\nThis matrix shows the number of missing values throughout each column.\n\nX-axis is the column names\nLeft Y-axis is the row number\nRight Y-axis is a line plot that shows each row’s completeness, e.g., if there are 11 columns, 4-10 valid values means that there are 1-7 missing values in a row.\n\n\ndataNA1 = dataNA.drop('DiabetesPedigreeFunction', axis = \"columns\")\n\n# NA matric\nmsno.matrix(dataNA1, figsize = (8, 8), fontsize = 10)"
  },
  {
    "objectID": "ImputingLikeDataScientist.html#impute-outliers-and-nas",
    "href": "ImputingLikeDataScientist.html#impute-outliers-and-nas",
    "title": "Exploratory Data Analysis in Python - Imputing like a Data Scientist",
    "section": "Impute Outliers and NAs",
    "text": "Impute Outliers and NAs\nRemoving outliers and NAs can be tricky, but there are methods to do so. I will go over several, and discuss benefits and costs to each.\nThe principle goal for all imputation is to find the method that does not change the distribution too much (or oddly).\n\nNOTE: imputation should only be used when missing data is unavoidable and probably limited to 10% of your data being outliers / missing data (though some argue imputation is necessary between 30-60%). Ask what the cause is for the outlier and missing data.\n\n\nClassifying Outliers\nBefore imputing outliers, you will want to diagnose whether it’s they are natural outliers or not. We will be looking at “Insulin” for example across Age_group, because there are several outliers and NAs, which we will impute below.\n\n# Increase font size of all seaborn plot elements\nsns.set(font_scale = 1.25, rc = {'figure.figsize':(8, 8)})\n\n# Change theme to \"white\"\nsns.set_style(\"white\")\n\n# Box plot\nAge_Box = sns.boxplot(data = data, x = \"Insulin\", y = \"Age_group\", width = .3)\n\n# Tweak the visual presentation\nAge_Box.set(ylabel = \"Age group\")\n\n\n\n\n\n\n\n\nNow let’s say that we want to impute extreme values and remove outliers that don’t make sense, such as Insulin levels > 600 mg/dL: values greater than this induce a diabetic coma.\nWe remove outliers using SimpleImputer from sklearn and replace them with values that are estimates based on the existing data\n\nmean: arithmetic mean\nmedian: median\nmode: mode\ncapping: Impute the upper outliers with 95 percentile, and impute the bottom outliers with 5 percentile - aka Winsorizing\n\n\n# Select only Insulin\nInsMod = data.filter([\"Insulin\"], axis = \"columns\")\n\n\n\n\nMean Imputation\nThe mean of the observed values for each variable is computed and the outliers for that variable are imputed by this mean\n\n# Python can't impute outliers easily, so we will convert them to NAs and imputate them\nInsMod.loc[InsMod.Insulin > 600, 'Insulin'] = np.nan\n\n# Set mean imputation algorithm\nMean_Impute = SimpleImputer(missing_values = np.nan, strategy = 'mean')\n\n# Fit imputation\nMean_Impute = Mean_Impute.fit(InsMod[['Insulin']])\n\n# Transform NAs with the mean imputation\nInsMod['Ins_Mean'] = Mean_Impute.transform(InsMod[['Insulin']])\n\n\n# Visualization of the mean imputation\n# Original data\nmean_plot = sns.kdeplot(data = InsMod, x = 'Insulin', linewidth = 2, label = \"Original\")\n\n# Mean imputation\nmean_plot = sns.kdeplot(data = InsMod, x = 'Ins_Mean', linewidth = 2, label = \"Mean Imputated\")\n\n# Show legend\nplt.legend()\n\n# Show plot\nplt.show()\n\n\n\n\n\n\n\nMedian Imputation\nThe median of the observed values for each variable is computed and the outliers for that variable are imputed by this median\n\n# Python can't impute outliers easily, so we will convert them to NAs and imputate them\nInsMod.loc[InsMod.Insulin > 600, 'Insulin'] = np.nan\n\n# Set median imputation algorithm\nMedian_Impute = SimpleImputer(missing_values = np.nan, strategy = 'median')\n\n# Fit imputation\nMedian_Impute = Median_Impute.fit(InsMod[['Insulin']])\n\n# Transform NAs with the median imputation\nInsMod['Ins_Median'] = Median_Impute.transform(InsMod[['Insulin']])\n\n\n# Visualization of the median imputation\n# Original data\nmedian_plot = sns.kdeplot(data = InsMod, x = 'Insulin', linewidth = 2, label = \"Original\")\n\n# Median imputation\nmedian_plot = sns.kdeplot(data = InsMod, x = 'Ins_Median', linewidth = 2, label = \"Median Imputated\")\n\n# Show legend\nplt.legend()\n\n# Show plot\nplt.show()\n\n\n\n\n\n\nPros & Cons of Using the Mean or Median Imputation\nPros:\n\nEasy and fast.\nWorks well with small numerical datasets.\n\nCons:\n\nDoesn’t factor the correlations between features. It only works on the column level.\nWill give poor results on encoded categorical features (do NOT use it on categorical features).\nNot very accurate.\nDoesn’t account for the uncertainty in the imputations.\n\n\n\n\n\nMode Imputation\nThe mode of the observed values for each variable is computed and the outliers for that variable are imputed by this mode\n\n# Python can't impute outliers easily, so we will convert them to NAs and imputate them\nInsMod.loc[InsMod.Insulin > 600, 'Insulin'] = np.nan\n\n# Set mode imputation algorithm\nMode_Impute = SimpleImputer(missing_values = np.nan, strategy = 'most_frequent')\n\n# Fit imputation\nMode_Impute = Mode_Impute.fit(InsMod[['Insulin']])\n\n# Transform NAs with the mode imputation\nInsMod['Ins_Mode'] = Mode_Impute.transform(InsMod[['Insulin']])\n\n\n# Visualization of the mode imputation\n# Original data\nmode_plot = sns.kdeplot(data = InsMod, x = 'Insulin', linewidth = 2, label = \"Original\")\n\n# Mode imputation\nmode_plot = sns.kdeplot(data = InsMod, x = 'Ins_Mode', linewidth = 2, label = \"Mode Imputated\")\n\n# Show legend\nplt.legend()\n\n# Show plot\nplt.show()\n\n\n\n\n\n\nPros & Cons of Using the Mode Imputation\nPros:\n\nWorks well with categorical features.\n\nCons:\n\nIt also doesn’t factor the correlations between features.\nIt can introduce bias in the data.\n\n\n\n\n\nCapping Imputation (aka Winsorizing)\nThe Percentile Capping is a method of Imputing the outlier values by replacing those observations outside the lower limit with the value of 5th percentile and those that lie above the upper limit, with the value of 95th percentile of the same dataset.\n\n# Winsorizing deals woth outliers, so we don't have to worry about changing to NAs\n\n# Transform NAs with the median imputation\nInsMod['Ins_Cap'] = stats.mstats.winsorize(InsMod['Insulin'], limits = 0.05)\n\n\n# Visualization of the capping imputation\n# Original data\ncap_plot = sns.kdeplot(data = InsMod, x = 'Insulin', linewidth = 2, label = \"Original\")\n\n# Mode imputation\ncap_plot = sns.kdeplot(data = InsMod, x = 'Ins_Cap', linewidth = 2, label = \"Capping Imputated\")\n\n# Show legend\nplt.legend()\n\n# Show plot\nplt.show()\n\n\n\n\n\n\nPros and Cons of Capping\nPros:\n\nNot influenced by extreme values\n\nCons:\n\nCapping only modifies the smallest and largest values slightly. This is generally not a good idea since it means we’re just modifying data values for the sake of modifications.\nIf no extreme outliers are present, Winsorization may be unnecessary."
  },
  {
    "objectID": "ImputingLikeDataScientist.html#imputing-nas",
    "href": "ImputingLikeDataScientist.html#imputing-nas",
    "title": "Exploratory Data Analysis in Python - Imputing like a Data Scientist",
    "section": "Imputing NAs",
    "text": "Imputing NAs\nI will only be addressing a subset of methods for NA imputation, but you can use the mean, median, and mode methods from above as well:\n\nknn: K-nearest neighbors (KNN)\nmice: Multivariate Imputation by Chained Equations (MICE)\n\nSince our normal data has no NA values, we will add the Insulin column from the dataNA we created earlier and replace the original with it.\n\n# Make a copy of the data \ndataCopy = data.copy()\n\n# Select the Insulin \nInsNA = dataNA.filter([\"Insulin\"], axis = \"columns\")\n\n# Add Insulin with NAs to copy of original data\ndataCopy['Insulin'] = InsNA\n\n\n\nK-Nearest Neighbor (KNN) Imputation\nKNN is a machine learning algorithm that classifies data by similarity. This in effect clusters data into similar groups. The algorithm predicts values of new data to replace NA values based on how closely they resembles training data points, such as by comparing across other columns.\nHere’s a visual example using the plot_decision_regions function from mlxtend.plotting library to run a KNN algorithm on our dataset, where three clusters are created by the algorithm.\n\n# KNN plot function\ndef knn_comparision(data, k):\n  # Define x and y values (your data will need to have these)\n    X = data[['x1','x2']].values\n    y = data['y'].astype(int).values\n    # Knn function, defining the number of neighbors\n    clf = neighbors.KNeighborsClassifier(n_neighbors = k)\n    # Fit knn algorithm to data\n    clf.fit(X, y)\n\n    # Plotting decision regions\n    plot_decision_regions(X, y, clf = clf, legend = 2)\n\n    # Adding axes annotations\n    plt.xlabel('X1')\n    plt.ylabel('X2')\n    plt.title('Knn with K='+ str(k))\n    plt.legend(loc = 'upper right')\n    plt.tight_layout()\n    plt.show()\n\n\n# Prepare data for the KNN plotting function\ndata1 = data.loc[:, ['Insulin', 'Glucose', 'Outcome']]\n\n# Drop NAs\ndata1 = data1.dropna()\n\n# Set the two target x variables and the binary y variable we are clustering the data from\ndata1 = data1.rename(columns = {'Insulin': 'x1', 'Glucose': 'x2', 'Outcome': 'y'})\n\n# Create KNN plot for 3 nearest neighbors\nknn_comparision(data1, 3)\n\n\n\n\nYou can also loop the KNN plots for i nearest neighbors:\n\n# Loop to create KNN plots for i number of nearest neighbors\nfor i in [1, 5, 15]:\n  knn_comparision(data1, i)\n\nNote, we have to change a three things to make KNNImputer work correctly:\n\nWe need to change any characters, into dummy variables that are numerics, because scalers and imputaters do not recognize characters. In this case, Age_group is an ordinal category, so we will use OrdinalEncoder from Scikit-learn, specifically in preprocessing which we imported as pproc.\n\n\n# Numeric dummy variable from our Age_group ordinal column\n# Define the orginal encoder \nenc = pproc.OrdinalEncoder()\n\n# Ordinal variable from Age_group column \ndataCopy[['Age_group']] = enc.fit_transform(dataCopy[['Age_group']])\n\n\nWe need to reorder our target column with NAs to the end of the dataframe so that the rest of the dataframe can be called as training data more easily.\n\n\n# Reorder columns\ndataCopy = dataCopy[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', \"BMI\", \"DiabetesPedigreeFunction\", \"Age\", \"Outcome\", \"Age_group\", \"Insulin\"]]\n\n\nKNNImputater is distance-based so we need to normalize our data. Otherwise KNNImputer will create biased replacement. We will use the pproc.MinMaxScaler from Scikit-learn, which scales our values from 0-1.\n\n\n# Min-max schaler\nscaler = pproc.MinMaxScaler()\n\n# Scale columns\ndataCopy_Scale = pd.DataFrame(scaler.fit_transform(dataCopy), columns = dataCopy.columns)\n\nWe are finally ready to for KNN Imputation!\n\n# Set KNN imputation function parameters\nimputer = KNNImputer(n_neighbors = 3)\n\n# Fit imputation\nDataKnn = pd.DataFrame(imputer.fit_transform(dataCopy_Scale),columns = dataCopy_Scale.columns)\n\n\n# Add KNN imputated column to original dataCopy\ndataCopy_Scale[['InsKnn']] = DataKnn[['Insulin']]\n\n# Visualization of the KNN imputation\n# Original data\nknn_plot = sns.kdeplot(data = dataCopy_Scale, x = 'Insulin', linewidth = 2, label = \"Original\")\n\n# KNN imputation\nknn_plot = sns.kdeplot(data = dataCopy_Scale, x = 'InsKnn', linewidth = 2, label = \"KNN Imputated\")\n\n# Show legend\nplt.legend()\n\n# Show plot\nplt.show()\n\n\n\n\n\n\nPros & Cons of Using KNN Imputation\nPro:\n\nPossibly much more accurate than mean, median, or mode imputation for some data sets.\n\nCons:\n\nKNN is computationally expensive because it stores the entire training dataset into computer memory.\nKNN is very sensitive to outliers, so you would have to imputate these first.\n\n\n\n\n\nMultivariate Imputation by Chained Equations (MICE)\nMICE is an algorithm that fills missing values multiple times, hence dealing with uncertainty better than other methods. This approach creates multiple copies of the data that can then be analyzed and then pooled into a single dataset.\n\n\n\nImage Credit: Will Badr\n\n\n\n# Assign a regression model\nlm = LinearRegression()\n\n# Set MICE imputation function parameters\nimputer = IterativeImputer(estimator = lm, missing_values = np.nan, max_iter = 10, verbose = 2, imputation_order = 'roman', random_state = 0)\n\n# Fit imputation\ndataMice = pd.DataFrame(imputer.fit_transform(dataCopy),columns = dataCopy.columns)\n\n\n# Add MICE imputated column to original dataCopy\ndataCopy[['InsMice']] = dataMice[['Insulin']]\n\n# Visualization of the MICE imputation\n# Original data\nknn_plot = sns.kdeplot(data = dataCopy, x = 'Insulin', linewidth = 2, label = \"Original\")\n\n# KNN imputation\nknn_plot = sns.kdeplot(data = dataCopy, x = 'InsMice', linewidth = 2, label = \"MICE Imputated\")\n\n# Show legend\nplt.legend()\n\n# Show plot\nplt.show()\n\n\n\n\n\n\nPros & Cons of MICE Imputation\nPros:\n\nMultiple imputations are more accurate than a single imputation.\nThe chained equations are very flexible to data types, such as categorical and ordinal.\n\nCons:\n\nYou have to round the results for ordinal data because resulting data points are too great or too small (floating-points)."
  }
]
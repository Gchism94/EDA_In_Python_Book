---
bibliography: references.bib
---

# Correlating Like a Data Master {.unnumbered}

```{r setup, include = FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 

# Sets the repository to download packages from
options(repos = list(CRAN = "http://cran.rstudio.com/"))

library(reticulate)

py_install("pandas")

py_install("seaborn")

py_install("scikit-learn")

py_install("tabulate")

py_install("missingno")

py_install("statsmodels")
```

## Purpose of this chapter

**Assess relationships within a novel data set**

------------------------------------------------------------------------

## Take-aways

1.  Describe and visualize correlations between numerical variables
2.  Visualize correlations of all numerical variables within groups
3.  Describe and visualize relationships based on target variables

------------------------------------------------------------------------

### Required setup

We first need to prepare our environment with the necessary libraries and set a global theme for publishable plots in `seaborn`.

```{python req-setup, echo = TRUE}
# Import all required libraries
# Data analysis and manipulation
import pandas as pd
# Working with arrays
import numpy as np
# Statistical visualization
import seaborn as sns
# Matlab plotting for Python
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
# Data analysis
import statistics as stat
import scipy.stats as stats
# Two-sample Chi-Square test
from scipy.stats import chi2_contingency
# Predictive data analysis: process data 
from sklearn import preprocessing as pproc
# Predictive data analysis: linear models
from sklearn.model_selection import cross_val_predict
# Predictive data analysis: linear models
from sklearn.linear_model import LinearRegression
# Visualizing missing values
import missingno as msno
# Statistical modeling
import statsmodels.api as sm
# Statistical modeling: ANOVA
from statsmodels.formula.api import ols
# Mosaic plot
from statsmodels.graphics.mosaicplot import mosaic
from itertools import product

# Increase font and figure size of all seaborn plot elements
sns.set(font_scale = 1.5, rc = {'figure.figsize':(8, 8)})

# Change theme to "white"
sns.set_style("white")
```

------------------------------------------------------------------------

## Load the Examine a Data Set

We will be using open source data from UArizona researchers that investigates the effects of climate change on canopy trees. [@meredith2021]

```{python load-data, warning = FALSE}
# Read csv 
data = pd.read_csv("data/Data_Fig2_Repo.csv")

# Convert 'Date' column to datetime
data['Date'] = pd.to_datetime(data['Date'])

# What does the data look like
data.head()
```

------------------------------------------------------------------------

## Describe and Visualize Correlations

[Correlations](https://en.wikipedia.org/wiki/Correlation) are a statistical relationship between two numerical variables, may or may not be causal. Exploring correlations in your data allows you determine data independence, a major [assumption of parametric statistics](https://www.statology.org/parametric-tests-assumptions/), which means your variables are both randomly collected.

#### If you're interested in some underlying statistics...

Note that the we will use the [Pearson's $r$ coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficienthttps://en.wikipedia.org/wiki/Pearson_correlation_coefficient) in `corr()` function from the `pandas` library, but you can specify any method you would like: `corr(method = "")`, where the method can be `"pearson"` for Pearson's $r$, `"spearman"` for [Spearman's](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient) $\rho$, or `"kendall"` for [Kendall's](https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient) $\tau$. The main differences are that Pearson's $r$ assumes a normal distribution for ALL numerical variables, whereas Spearman's $\rho$ and Kendall's $\tau$ do not, but Spearman's $\rho$ requires $N > 10$, and Kendall's $\tau$ does not. Notably, Kendall's $\tau$ performs as well as Spearman's $\rho$ when $N > 10$, so its best to just use Kendall's $\tau$ when data are not normally distributed.

```{python corr-matrix}
# subset dataframe to include only numeric columns
numData = data.select_dtypes(include='number')

# Table of correlations between numerical variables (we are sticking to the default Pearson's r coefficient)
numData.corr()
```

```{python corr-plot}
# Heatmap correlation matrix of numerical variables
# Correlation matrix
corr = numData.corr()

# Generate a mask for the upper triangle
mask = np.triu(np.ones_like(corr, dtype = bool))

# Generate a custom diverging colormap
cmap = sns.diverging_palette(230, 20, as_cmap = True)

# Heatmap of the correlation matrix
sns.heatmap(corr, cmap = cmap, mask = mask, vmax = 0.3, center = 0,
            square = True, linewidths = 0.5, cbar_kws = {"shrink": .5})
            
# Tight margins for plot
plt.tight_layout()

# Show plot
plt.show()
```

------------------------------------------------------------------------

## Visualize Correlations within Groups

If we have groups that we will compare later on, it is a good idea to see how each numerical variable correlates within these groups.

```{python group-corr-plot}
# Increase font and figure size of all seaborn plot elements
sns.set(font_scale = 1.5, rc = {'figure.figsize':(10, 10)})

# Change theme to "white"
sns.set_style("white")

# Heatmap correlation matrix of numerical variables
# Correlation matrix
corr = data.groupby('Group').corr()

# Generate a mask for the upper triangle
mask = np.triu(np.ones_like(corr, dtype = bool))

# Generate a custom diverging colormap
cmap = sns.diverging_palette(230, 20, as_cmap = True)

# Heatmap of the correlation matrix
ax = sns.heatmap(corr, cmap = cmap, mask = mask, vmax = 0.3, center = 0,
            square = True, linewidths = 0.5, cbar_kws = {"shrink": .5})
            
# Change y-axis label
ax.set(ylabel = 'Group')

# Tight margins for plot
plt.tight_layout()

# Show plot
plt.show()
```

This is great, we have our correlations within groups! However, the correlation matrices aren't always the most intuitive, so let's plot!

Specifically, we are looking at the correlations between predawn leaf water potential `pLWP` and midday leaf water potential `mLWP`. Leaf water potential is a key indicator for how stressed plants are in droughts.

```{python grid-plot}
dataplot = data[["Group", "pLWP", "mLWP"]]

# Increase font and figure size of all seaborn plot elements
sns.set(font_scale = 2.5, rc = {'figure.figsize':(10, 10)})

# Change seaborn plot theme to white
sns.set_style("white")

# Empty subplot grid for pairwise relationships
g = sns.PairGrid(dataplot, hue = "Group", height = 5)

# Add scatterplots to the upper portion of the grid
g1 = g.map_upper(sns.scatterplot, alpha = 0.5, s = 100)

# Add a kernal density plot to the diagonal of the grid
g2 = g1.map_diag(sns.kdeplot, fill = True, linewidth = 3)

# Add a kernal density plot to the lower portion of the grid
g3 = g2.map_lower(sns.kdeplot, levels = 5, alpha = 0.75)

# Remove legend title
g4 = g3.add_legend(title = "", adjust_subtitles = True)

# Show plot
plt.show()
```

------------------------------------------------------------------------

## Describe and Visualize Relationships Based on Target Variables

### Target Variables

`Target variables` are essentially numerical or categorical variables that you want to relate others to in a data frame.

The relationships below will have the formula relationship `target ~ predictor`.

------------------------------------------------------------------------

### Numerical Target Variables: Numerical Variable of Interest

`Formula: pLWP (numerical)  ~ mLWP (numerical)`

```{python num-num}
# The numerical predictor variable 
X = data[["mLWP"]]

# The numerical target variable
Y = data[["pLWP"]]

# Define the linear model, drop NAs
model = sm.OLS(Y, X, missing = 'drop')

# Fit the model
model_result = model.fit()

# Summary of the linear model
model_result.summary()
```

```{python plot-num-num, message = FALSE, warning = FALSE}
# Plotting the linear relationship

# Increase font and figure size of all seaborn plot elements
sns.set(font_scale = 1.25, rc = {'figure.figsize':(8, 8)})

# Change seaborn plot theme to white
sns.set_style("white")

# Subplots
fig, (ax1, ax2) = plt.subplots(ncols = 2, nrows = 1)

# Regression plot between mLWP and pLWP
sns.regplot(data = data, x = "mLWP", y = "pLWP", ax = ax1)

# Set regression plot title
ax1.set_title("Linear regression")

# Regression plot between mLWP and pLWP
sns.residplot(data = data, x = "mLWP",
              y = "pLWP", ax = ax2)
              
# Set residual plot title
ax2.set_title("Residuals")

# Tight margins
plt.tight_layout()

# Show plot
plt.show()
```

------------------------------------------------------------------------

### Numerical Target Variables: Categorical Variable of Interest

Formula: `pLWP (numerical) ~ Group (categorical)`

```{python num-cat}
model = ols('pLWP ~ C(Group)', data = data).fit()

sm.stats.anova_lm(model, typ = 2)
```

```{python plot-num-cat}
#| fig-align: right

# Increase font and figure size of all seaborn plot elements
sns.set(font_scale = 1.25, rc = {'figure.figsize':(4, 4)})

# Change seaborn plot theme to white
sns.set_style("white")

# Box plot
Group_Box = sns.boxplot(data = data, x = "pLWP", y = "Group", width = 0.3)

# Tweak the visual presentation
Group_Box.set(ylabel = "Group")

# Tight margins
plt.tight_layout()

# Show plot
plt.show()
```

------------------------------------------------------------------------

### Categorical Target Variables: Numerical Variable of Interest

Formula: `Group (categorical) ~ pLWP (numerical)`

```{python group-stats-cat-num}
# Grouped describe by one column, stacked 
Groups = data.groupby('Group').describe().unstack(1)

# Print all rows
print(Groups.to_string())
```

------------------------------------------------------------------------

### Categorical Target Variables: Categorical Variable of Interest

Notably, there is only one categorical variable... Let's make another:

If $mLWP > mean(mLWP) + sd(mLWP)$ then `Yes`, else `No`.

```{python new-cat}
data1 = data.dropna()
Qual = stat.mean(data1.pLWP + stat.stdev(data1.pLWP))

# Create HighLWP from the age column
def HighLWP_data(data): 
  if data.pLWP >= Qual: return "Yes"
  else: return "No"

# Apply the function to data and create a dataframe
HighLWP = pd.DataFrame(data1.apply(HighLWP_data, axis = 1))

# Name new column
HighLWP.columns = ['HighLWP']

# Concatenate the two dataframes
data1 = pd.concat([data1, HighLWP], axis = 1)

# First six rows of new dataset
data1.head()
```

Now we have two categories!

Formula = `Group (categorical) ~ HighLWP (categorical)`

```{python cat-cat}
obs = pd.crosstab(data1.Group, data1.HighLWP)
print(obs)

# Chi-square test
chi2, p, dof, ex = chi2_contingency(obs, correction = False)
  
# Interpret
alpha = 0.05
  
# Print the interpretation
print('Statistic = %.3f, p = %.3f' % (chi2, p))
if p > alpha:
  print('Chi-square value is not greater than critical value (fail to reject H0)')
else:
	print('Chi-square value is greater than critical value (reject H0)')
```

```{python plot-cat-cat}
#| fig-align: center

# Increase font and figure size of all seaborn plot elements
sns.set(font_scale = 1.25)

# Change seaborn plot theme to white
sns.set_style("white")

# Count plot of HighLWP grouped by Group 
counts = sns.countplot(data = data1, x = "HighLWP", hue = "Group")

# Tweak the visual presentation
counts.set(ylabel = "Count")

# Tight margins
plt.tight_layout()

# Show plot
plt.show()
```
